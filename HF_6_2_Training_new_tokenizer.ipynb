{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnVnm42JTtd1TImN8l1CID",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d8dd116e1d344efbad454047c54424f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c86520eeff954d0b919ac826b440e4d4",
              "IPY_MODEL_3cc53cd551884afdae0e3fc7c4355c98",
              "IPY_MODEL_bb8f22e73c60447e83d1a7b324a6ded8"
            ],
            "layout": "IPY_MODEL_c458cb773b95421d91d9bd2cec56ea71"
          }
        },
        "c86520eeff954d0b919ac826b440e4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a060f431658474b91d80d6c3f915a2e",
            "placeholder": "​",
            "style": "IPY_MODEL_618fbee68eb04f72a779b29a724e1258",
            "value": "Downloading builder script: 100%"
          }
        },
        "3cc53cd551884afdae0e3fc7c4355c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6594a3cfae064f0b80c75684de04a643",
            "max": 8440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd651e720d524b7daaa67820f1e56e4f",
            "value": 8440
          }
        },
        "bb8f22e73c60447e83d1a7b324a6ded8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16503d511cfd4648babd91668ec8ff5e",
            "placeholder": "​",
            "style": "IPY_MODEL_f68700ccf93948749d9dd24eb4bea4aa",
            "value": " 8.44k/8.44k [00:00&lt;00:00, 120kB/s]"
          }
        },
        "c458cb773b95421d91d9bd2cec56ea71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a060f431658474b91d80d6c3f915a2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "618fbee68eb04f72a779b29a724e1258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6594a3cfae064f0b80c75684de04a643": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd651e720d524b7daaa67820f1e56e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16503d511cfd4648babd91668ec8ff5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f68700ccf93948749d9dd24eb4bea4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c1c31095c4d48d7aa2d2c06d37fab99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54a6ee59dc3248d09007f16024c30caf",
              "IPY_MODEL_622911412f2f45cc9e8e0e02f9e6d1e2",
              "IPY_MODEL_a295c67eb7aa4654ad25ed22d9af3c66"
            ],
            "layout": "IPY_MODEL_a14643588d634e2fbdb1088daff5da19"
          }
        },
        "54a6ee59dc3248d09007f16024c30caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c59f3f139bdd41f59fa6fbd2d7a528d3",
            "placeholder": "​",
            "style": "IPY_MODEL_868bc00a7c0e4c92950925b73c98c4c9",
            "value": "Downloading metadata: 100%"
          }
        },
        "622911412f2f45cc9e8e0e02f9e6d1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43a48ab54b7d46f0a8dac5795fa26246",
            "max": 18481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cd72b4d778948109924a15ffc38e373",
            "value": 18481
          }
        },
        "a295c67eb7aa4654ad25ed22d9af3c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5897189d30c1456ca8d3f9d40b9b0b59",
            "placeholder": "​",
            "style": "IPY_MODEL_a15f2dce3baf4407a2fc322b020ff6da",
            "value": " 18.5k/18.5k [00:00&lt;00:00, 286kB/s]"
          }
        },
        "a14643588d634e2fbdb1088daff5da19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c59f3f139bdd41f59fa6fbd2d7a528d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "868bc00a7c0e4c92950925b73c98c4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43a48ab54b7d46f0a8dac5795fa26246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cd72b4d778948109924a15ffc38e373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5897189d30c1456ca8d3f9d40b9b0b59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a15f2dce3baf4407a2fc322b020ff6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "014f93625a144da9940f6d8538415374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60babe503491439a8cacc1444264150f",
              "IPY_MODEL_48ac04ab79f84accbf7ed5373b0e715f",
              "IPY_MODEL_a477b607b04846a781de3f9bbbf1e449"
            ],
            "layout": "IPY_MODEL_66e95c7b1e8b4f32bb74efcf9b17da8d"
          }
        },
        "60babe503491439a8cacc1444264150f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7505d9c06094f668c7e9f911f0681a5",
            "placeholder": "​",
            "style": "IPY_MODEL_654e58d8121c4fef853d38047a8656bd",
            "value": "Downloading readme: 100%"
          }
        },
        "48ac04ab79f84accbf7ed5373b0e715f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f411ef5b9d2f47179cbf198debaa8e3e",
            "max": 12909,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db8d4153c1e54f7e8f23375505de8bff",
            "value": 12909
          }
        },
        "a477b607b04846a781de3f9bbbf1e449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63c8ba8605e94508a79edcf6dd16363c",
            "placeholder": "​",
            "style": "IPY_MODEL_14c03cfc24f14d8c99b865fd81dcef68",
            "value": " 12.9k/12.9k [00:00&lt;00:00, 221kB/s]"
          }
        },
        "66e95c7b1e8b4f32bb74efcf9b17da8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7505d9c06094f668c7e9f911f0681a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "654e58d8121c4fef853d38047a8656bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f411ef5b9d2f47179cbf198debaa8e3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db8d4153c1e54f7e8f23375505de8bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63c8ba8605e94508a79edcf6dd16363c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14c03cfc24f14d8c99b865fd81dcef68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b599cea45a4140749dca570786742296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdf6c66f4a974bfeba4b482666796e94",
              "IPY_MODEL_9d7b5d0d320145fda09a1b49b3a34faf",
              "IPY_MODEL_f9215705ba564ec0bd25aad26b6ca32b"
            ],
            "layout": "IPY_MODEL_d05349616d1247dc800da279f40ed65e"
          }
        },
        "fdf6c66f4a974bfeba4b482666796e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f72a522f98d438fb965a2f0d636a87c",
            "placeholder": "​",
            "style": "IPY_MODEL_539c9942a2fc4673abeb119957a93c47",
            "value": "Downloading data files: 100%"
          }
        },
        "9d7b5d0d320145fda09a1b49b3a34faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05f9a112088c46649ea6b96a786d2483",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e21e85754ca74276a6a5409d39274548",
            "value": 1
          }
        },
        "f9215705ba564ec0bd25aad26b6ca32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66c54dfe08ed4fe7a1fcc01762a62be6",
            "placeholder": "​",
            "style": "IPY_MODEL_c0f8ab81aee04db3a53152672beb08da",
            "value": " 1/1 [00:14&lt;00:00, 14.52s/it]"
          }
        },
        "d05349616d1247dc800da279f40ed65e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f72a522f98d438fb965a2f0d636a87c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "539c9942a2fc4673abeb119957a93c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05f9a112088c46649ea6b96a786d2483": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e21e85754ca74276a6a5409d39274548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66c54dfe08ed4fe7a1fcc01762a62be6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0f8ab81aee04db3a53152672beb08da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9028a74178124e2785717cb9a2effe44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eff6029ab9b402eab7e7d26de3032bc",
              "IPY_MODEL_512d7367a85a42b0a98a0a17be2a4329",
              "IPY_MODEL_d62af8c1ec8c40b694f5a29474fbe9ee"
            ],
            "layout": "IPY_MODEL_9ff5137d7a6e425fa4daa85d42ce9713"
          }
        },
        "6eff6029ab9b402eab7e7d26de3032bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9aa72e536ab4d04b463af3eb7231023",
            "placeholder": "​",
            "style": "IPY_MODEL_e6b8ff8ac2814d938d1421a84841fda9",
            "value": "Downloading data: 100%"
          }
        },
        "512d7367a85a42b0a98a0a17be2a4329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15d73077d922465f93cb4084557b11d7",
            "max": 940909997,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a7896d6502c4e5cb8ec00e7d7121282",
            "value": 940909997
          }
        },
        "d62af8c1ec8c40b694f5a29474fbe9ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_581b8ccc790644888d4f18e81bafed35",
            "placeholder": "​",
            "style": "IPY_MODEL_cd506db1610849a29b2955b18c09b143",
            "value": " 941M/941M [00:13&lt;00:00, 39.5MB/s]"
          }
        },
        "9ff5137d7a6e425fa4daa85d42ce9713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9aa72e536ab4d04b463af3eb7231023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6b8ff8ac2814d938d1421a84841fda9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15d73077d922465f93cb4084557b11d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a7896d6502c4e5cb8ec00e7d7121282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "581b8ccc790644888d4f18e81bafed35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd506db1610849a29b2955b18c09b143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd1215e88a0b4343a2cf74a6d7be666a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea09e82bf1504f698360ca12333edee1",
              "IPY_MODEL_11e1165f5429472dbb8d69486751c4e4",
              "IPY_MODEL_cfe5dd0a30d140e1a217d00147cd5dc6"
            ],
            "layout": "IPY_MODEL_9e7c43e0d33c48cda8271d8d8c52ebe2"
          }
        },
        "ea09e82bf1504f698360ca12333edee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cb63f9546a34c548da6ae858ab29aa9",
            "placeholder": "​",
            "style": "IPY_MODEL_423c6ffffc8f4f2d8403563978c75231",
            "value": "Extracting data files: 100%"
          }
        },
        "11e1165f5429472dbb8d69486751c4e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05140144cd5e4ec3a410ca172132a75b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1b3d9b7454e4a1885fa176a2da195e8",
            "value": 1
          }
        },
        "cfe5dd0a30d140e1a217d00147cd5dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e439369309334285b57c4cd27df363b2",
            "placeholder": "​",
            "style": "IPY_MODEL_0be7d742aa3a43398ca3659913ec5a47",
            "value": " 1/1 [00:34&lt;00:00, 34.55s/it]"
          }
        },
        "9e7c43e0d33c48cda8271d8d8c52ebe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cb63f9546a34c548da6ae858ab29aa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "423c6ffffc8f4f2d8403563978c75231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05140144cd5e4ec3a410ca172132a75b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b3d9b7454e4a1885fa176a2da195e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e439369309334285b57c4cd27df363b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0be7d742aa3a43398ca3659913ec5a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06ffe4155eeb4c53a07c2e54d3953ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67ac937c121e4278b2afeabf4386e8d5",
              "IPY_MODEL_c873fa6cbf95472082a4d96220f009ea",
              "IPY_MODEL_1f54f64236a941609eafa2b4298a2e5d"
            ],
            "layout": "IPY_MODEL_c9675e0ffd1542798df0fb18c0210a5e"
          }
        },
        "67ac937c121e4278b2afeabf4386e8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_349ce0fb29fd43508766f6dc6abecf5e",
            "placeholder": "​",
            "style": "IPY_MODEL_179de4abbda84121873a083bd0108c85",
            "value": "Extracting data files: 100%"
          }
        },
        "c873fa6cbf95472082a4d96220f009ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef28c35615f74f1ba7d6a6ea90f000d7",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cc86baf0cc64d52bf79c9ebdfca4d27",
            "value": 3
          }
        },
        "1f54f64236a941609eafa2b4298a2e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1513f60f4a0d4b1da3f527b8fe382539",
            "placeholder": "​",
            "style": "IPY_MODEL_9a66e770f7b84b11a42a32c928168803",
            "value": " 3/3 [00:14&lt;00:00,  3.37s/it]"
          }
        },
        "c9675e0ffd1542798df0fb18c0210a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "349ce0fb29fd43508766f6dc6abecf5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "179de4abbda84121873a083bd0108c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef28c35615f74f1ba7d6a6ea90f000d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cc86baf0cc64d52bf79c9ebdfca4d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1513f60f4a0d4b1da3f527b8fe382539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a66e770f7b84b11a42a32c928168803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e53d634bbc64da08127ba8039b34c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ae02b2680714e08b7edd74868f5b9d1",
              "IPY_MODEL_6cf29870a92649d89ef5eda735e51d7f",
              "IPY_MODEL_bae97e05fc1b470db3c0ba7070276353"
            ],
            "layout": "IPY_MODEL_5cbb757ff8754403b7172fc365f4a793"
          }
        },
        "1ae02b2680714e08b7edd74868f5b9d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3d8ac4d367848a8bd61edd1764cfe1c",
            "placeholder": "​",
            "style": "IPY_MODEL_1e5ef2e02951428190b01128a7be5aaa",
            "value": "Generating train split: 100%"
          }
        },
        "6cf29870a92649d89ef5eda735e51d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b862ceeb864faab14185c66387967b",
            "max": 412178,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a74d7caa36d94b2f970c410ed133bb0a",
            "value": 412178
          }
        },
        "bae97e05fc1b470db3c0ba7070276353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83ed352331ac4a2fa7a000c33c3b3ada",
            "placeholder": "​",
            "style": "IPY_MODEL_2391d900a9374dfb8966085954738cfb",
            "value": " 412178/412178 [04:09&lt;00:00, 772.60 examples/s]"
          }
        },
        "5cbb757ff8754403b7172fc365f4a793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3d8ac4d367848a8bd61edd1764cfe1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e5ef2e02951428190b01128a7be5aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87b862ceeb864faab14185c66387967b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a74d7caa36d94b2f970c410ed133bb0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83ed352331ac4a2fa7a000c33c3b3ada": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2391d900a9374dfb8966085954738cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87ba064c74234c2d8430b3489152f7a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c41974b64e945a09c76bcee0d8e08ae",
              "IPY_MODEL_07799bd7661a4fd5aba3f45b9ccd2aec",
              "IPY_MODEL_0bac8bb0814f4ae897e0fa6420047087"
            ],
            "layout": "IPY_MODEL_1f22d586cdc04c0f926451942ba85997"
          }
        },
        "6c41974b64e945a09c76bcee0d8e08ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_174f7546034548e89a5eb2fa7d126fb8",
            "placeholder": "​",
            "style": "IPY_MODEL_9ecb39fd6ff44e808429e5be9f894719",
            "value": "Generating test split: 100%"
          }
        },
        "07799bd7661a4fd5aba3f45b9ccd2aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c8f8a52fb7140e1b12cc6e2d81fa9ec",
            "max": 22176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cc5c0f761c84c9ab40641be0c8377b1",
            "value": 22176
          }
        },
        "0bac8bb0814f4ae897e0fa6420047087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37a24537cf2e480e95fc1e1207efff2a",
            "placeholder": "​",
            "style": "IPY_MODEL_87faefa2d07f46769276c969604902c3",
            "value": " 22176/22176 [00:12&lt;00:00, 1842.79 examples/s]"
          }
        },
        "1f22d586cdc04c0f926451942ba85997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "174f7546034548e89a5eb2fa7d126fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ecb39fd6ff44e808429e5be9f894719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c8f8a52fb7140e1b12cc6e2d81fa9ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc5c0f761c84c9ab40641be0c8377b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37a24537cf2e480e95fc1e1207efff2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87faefa2d07f46769276c969604902c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "963bec99ebce47b1bad0e4701efa91ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62cbc04f6d894ddfa07e1d6e10102a04",
              "IPY_MODEL_16b8fbb3f6204361915df1564586d369",
              "IPY_MODEL_139dd58634004ccaa0d143c65579ec69"
            ],
            "layout": "IPY_MODEL_dc86747532e84583af80259c3ffd0934"
          }
        },
        "62cbc04f6d894ddfa07e1d6e10102a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16cf5f44338942cbac78bee9cb2baac9",
            "placeholder": "​",
            "style": "IPY_MODEL_f7bda62911f64176992a43976a77b76c",
            "value": "Generating validation split: 100%"
          }
        },
        "16b8fbb3f6204361915df1564586d369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb7250e678294efd9db24212eddafeea",
            "max": 23107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_909ff87c46ed436688823625d0b91222",
            "value": 23107
          }
        },
        "139dd58634004ccaa0d143c65579ec69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d7112effab94357b9299c4e628f4824",
            "placeholder": "​",
            "style": "IPY_MODEL_f028e383b52f4966ba26f6baec6f1bf2",
            "value": " 23107/23107 [00:14&lt;00:00, 1969.07 examples/s]"
          }
        },
        "dc86747532e84583af80259c3ffd0934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16cf5f44338942cbac78bee9cb2baac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7bda62911f64176992a43976a77b76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb7250e678294efd9db24212eddafeea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "909ff87c46ed436688823625d0b91222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d7112effab94357b9299c4e628f4824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f028e383b52f4966ba26f6baec6f1bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b1740bbab024b718cacf9a195dcd9e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa22e48e39044584a08ef9d8182609e5",
              "IPY_MODEL_60052a02267e4f48a947a07c4ef0f049",
              "IPY_MODEL_557f1c0a6fca4696b19ebaf28e54db96"
            ],
            "layout": "IPY_MODEL_f5e4712a8c9646e8bdfe29af87232cd5"
          }
        },
        "aa22e48e39044584a08ef9d8182609e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17d169b13aa64e60a1affa8b8f8813a1",
            "placeholder": "​",
            "style": "IPY_MODEL_4f84d6353cdd46f48123efb032c889bb",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "60052a02267e4f48a947a07c4ef0f049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00e89577491644d88e5cc442a6188a61",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b181fcd791764e7a823e0678d7121e51",
            "value": 665
          }
        },
        "557f1c0a6fca4696b19ebaf28e54db96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41785660ae16498cbbe758274413a4a5",
            "placeholder": "​",
            "style": "IPY_MODEL_f310603ab9de4d06834bf852fac70aac",
            "value": " 665/665 [00:00&lt;00:00, 10.8kB/s]"
          }
        },
        "f5e4712a8c9646e8bdfe29af87232cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17d169b13aa64e60a1affa8b8f8813a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f84d6353cdd46f48123efb032c889bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00e89577491644d88e5cc442a6188a61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b181fcd791764e7a823e0678d7121e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41785660ae16498cbbe758274413a4a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f310603ab9de4d06834bf852fac70aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dc91e7890194b708eb37b27ef086366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5e2dda2cb934a65a747656fc67cb932",
              "IPY_MODEL_5cdc416b5f78452095848a934d81d480",
              "IPY_MODEL_f2d0c05e89384ee89a64bb57cc8c2e18"
            ],
            "layout": "IPY_MODEL_d59f806edc264c83a15b0a20e1fc8d64"
          }
        },
        "a5e2dda2cb934a65a747656fc67cb932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_953186ea08e544e888a306614beef57e",
            "placeholder": "​",
            "style": "IPY_MODEL_77d0ec0f721743f3989efd24beaa3387",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "5cdc416b5f78452095848a934d81d480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bf43bfd7ad040829b88ff667da051ad",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29052b2eae1e475db1660ebe3e216583",
            "value": 1042301
          }
        },
        "f2d0c05e89384ee89a64bb57cc8c2e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_242c0dc30fe94aed9b83d60c5f406ba6",
            "placeholder": "​",
            "style": "IPY_MODEL_b325ddf694284236b881cd296f82b5e1",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 6.55MB/s]"
          }
        },
        "d59f806edc264c83a15b0a20e1fc8d64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "953186ea08e544e888a306614beef57e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d0ec0f721743f3989efd24beaa3387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bf43bfd7ad040829b88ff667da051ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29052b2eae1e475db1660ebe3e216583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "242c0dc30fe94aed9b83d60c5f406ba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b325ddf694284236b881cd296f82b5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35ff95da9cb043659899a0c09caef6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8382e4dc1534c039bdc1f794f2c771e",
              "IPY_MODEL_f50827fd80324a3eaf6a3239acd3bd51",
              "IPY_MODEL_9f011410971c47fdac1bb947ede59d92"
            ],
            "layout": "IPY_MODEL_53c3979c27ee4d908cb85c211227b965"
          }
        },
        "f8382e4dc1534c039bdc1f794f2c771e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_953d15d7a7ed4ab78ce8530ff850e6ce",
            "placeholder": "​",
            "style": "IPY_MODEL_1a54daae914a42afa9faaa3553228809",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "f50827fd80324a3eaf6a3239acd3bd51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc07c176512c4cc4ba73558d496d6dc4",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c9d28764e9d4835ac21a671393ad9ec",
            "value": 456318
          }
        },
        "9f011410971c47fdac1bb947ede59d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4208a81c11894825833e54d924c5e977",
            "placeholder": "​",
            "style": "IPY_MODEL_ea8a138743134f24840a78c35bed3e7a",
            "value": " 456k/456k [00:00&lt;00:00, 4.56MB/s]"
          }
        },
        "53c3979c27ee4d908cb85c211227b965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "953d15d7a7ed4ab78ce8530ff850e6ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a54daae914a42afa9faaa3553228809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc07c176512c4cc4ba73558d496d6dc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c9d28764e9d4835ac21a671393ad9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4208a81c11894825833e54d924c5e977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea8a138743134f24840a78c35bed3e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6713958c31534656a83292c87b0f08e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64b84be1349a42d38af972a8a7e05dc5",
              "IPY_MODEL_e5e62130741d48f09e727ffba57226df",
              "IPY_MODEL_12e565bd71694aa0873c8bb11b28f4ab"
            ],
            "layout": "IPY_MODEL_39e9d1e31e384fa2b75438a917cb808c"
          }
        },
        "64b84be1349a42d38af972a8a7e05dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25c721e3dcfb4055a71a92b170081b1c",
            "placeholder": "​",
            "style": "IPY_MODEL_fa3d8c86ebf44a4fa0a0b184c9ae7c12",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "e5e62130741d48f09e727ffba57226df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8692e7465e144c6db89aa23b7409ace3",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ec9bdb247a14a6f814e09b5a28b6540",
            "value": 1355256
          }
        },
        "12e565bd71694aa0873c8bb11b28f4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aa3bdd9846143b5baae3ee9624ca5b3",
            "placeholder": "​",
            "style": "IPY_MODEL_23dd1946423a4eba90e4402bb0a964a2",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 15.7MB/s]"
          }
        },
        "39e9d1e31e384fa2b75438a917cb808c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25c721e3dcfb4055a71a92b170081b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa3d8c86ebf44a4fa0a0b184c9ae7c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8692e7465e144c6db89aa23b7409ace3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ec9bdb247a14a6f814e09b5a28b6540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1aa3bdd9846143b5baae3ee9624ca5b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23dd1946423a4eba90e4402bb0a964a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c5924e0ab2347e0adc1adb207b64c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_478f9ca8526c48268fdabb1964308249",
              "IPY_MODEL_e24239df5a9d4cb4b5a2957846b59e5f",
              "IPY_MODEL_3c80dd3a1d4e4c45a34e4e3fab3b1c00"
            ],
            "layout": "IPY_MODEL_04f7a51e5d40468fb114e06c02bd817d"
          }
        },
        "478f9ca8526c48268fdabb1964308249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f64b6c884d5d41de9b165b204861ab11",
            "placeholder": "​",
            "style": "IPY_MODEL_dda1777b75d041bd9ff70acf107df218",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "e24239df5a9d4cb4b5a2957846b59e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c4d62dd41ba4510bf6e2b18086bbce6",
            "max": 234,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b4681d6d0de4ee1a8b84cd2edf7c714",
            "value": 234
          }
        },
        "3c80dd3a1d4e4c45a34e4e3fab3b1c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e999ca429472470bb412ef616502d5a9",
            "placeholder": "​",
            "style": "IPY_MODEL_ddadad3b6b234d8abf10e68d30ac1e11",
            "value": " 234/234 [00:00&lt;00:00, 6.52kB/s]"
          }
        },
        "04f7a51e5d40468fb114e06c02bd817d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f64b6c884d5d41de9b165b204861ab11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dda1777b75d041bd9ff70acf107df218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c4d62dd41ba4510bf6e2b18086bbce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b4681d6d0de4ee1a8b84cd2edf7c714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e999ca429472470bb412ef616502d5a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddadad3b6b234d8abf10e68d30ac1e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "395ba15e7fbe4e6289250605b6d45bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_759f0ce6e1864124b9fb9dc420adb4d0",
              "IPY_MODEL_30967782aec54e70b47d6a03cd2626a0",
              "IPY_MODEL_64739ff5991f4170a1c7fa646af3920d"
            ],
            "layout": "IPY_MODEL_4668fa7d2e6e4b538e273bdbf1e206b0"
          }
        },
        "759f0ce6e1864124b9fb9dc420adb4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5339dc30359e4b3f89cde766f12879e4",
            "placeholder": "​",
            "style": "IPY_MODEL_7a10b0ecf45b4e6bb40f96f04537963f",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "30967782aec54e70b47d6a03cd2626a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc23fee0122748288ae79b210c262a85",
            "max": 822294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37a6b3e3cfea49feac889c6446d7c51f",
            "value": 822294
          }
        },
        "64739ff5991f4170a1c7fa646af3920d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac540d6aa41c43b997be0d31406accb8",
            "placeholder": "​",
            "style": "IPY_MODEL_7dd3b8cfc5844f6c996c11ccab5d2d78",
            "value": " 822k/822k [00:00&lt;00:00, 8.68MB/s]"
          }
        },
        "4668fa7d2e6e4b538e273bdbf1e206b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5339dc30359e4b3f89cde766f12879e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a10b0ecf45b4e6bb40f96f04537963f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc23fee0122748288ae79b210c262a85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37a6b3e3cfea49feac889c6446d7c51f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac540d6aa41c43b997be0d31406accb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dd3b8cfc5844f6c996c11ccab5d2d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e8a360ad74f476f9e7cab3e7131eb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9db7d56c4b7d48888d65b52fa3915182",
              "IPY_MODEL_e5ee6fa5f6704b23b7eedef4ff1daa2e",
              "IPY_MODEL_6c913ec3585f4581a922647df4a5fbcf"
            ],
            "layout": "IPY_MODEL_f3e38243ff07499c8412c33791fa6d95"
          }
        },
        "9db7d56c4b7d48888d65b52fa3915182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e55448193daa45ad868ea9bc3e155926",
            "placeholder": "​",
            "style": "IPY_MODEL_7c09301b856647ea98eb2cd0e16b9ccd",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "e5ee6fa5f6704b23b7eedef4ff1daa2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b41852d5edb456b87d63124520fcaa2",
            "max": 467148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a6d1adb4dd44eb48e281bb194b8cdbd",
            "value": 467148
          }
        },
        "6c913ec3585f4581a922647df4a5fbcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f645e70514c4a61be8fe4972a5ed9ed",
            "placeholder": "​",
            "style": "IPY_MODEL_0b3c0c9381c34c2fbd39a35a0deda32d",
            "value": " 467k/467k [00:00&lt;00:00, 7.23MB/s]"
          }
        },
        "f3e38243ff07499c8412c33791fa6d95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e55448193daa45ad868ea9bc3e155926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c09301b856647ea98eb2cd0e16b9ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b41852d5edb456b87d63124520fcaa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a6d1adb4dd44eb48e281bb194b8cdbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f645e70514c4a61be8fe4972a5ed9ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b3c0c9381c34c2fbd39a35a0deda32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c28222fa9cfa4b13bd06344fc5b38972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d2472b80cd0445c846e0e70eaa3a497",
              "IPY_MODEL_1396431d25cd40788adc58c3b6e22840",
              "IPY_MODEL_f8e5057dc1e9463eaee1100a4edc5d46"
            ],
            "layout": "IPY_MODEL_bdbc78a9cc334e0c9bd8e558da2b3151"
          }
        },
        "0d2472b80cd0445c846e0e70eaa3a497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d1cc36db4d34daa916d7d0f6184121d",
            "placeholder": "​",
            "style": "IPY_MODEL_a804139fbd7a4b23b3e32636d17110c1",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "1396431d25cd40788adc58c3b6e22840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2f020f0b69243cfba8c36dc6dec42a6",
            "max": 2173355,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c62e5b18190c40899d0513f8ea05fabe",
            "value": 2173355
          }
        },
        "f8e5057dc1e9463eaee1100a4edc5d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1acf3667e1d042239b96a8c46ea8751b",
            "placeholder": "​",
            "style": "IPY_MODEL_0b9466e761164eb9a38d19b23ceed9bb",
            "value": " 2.17M/2.17M [00:00&lt;00:00, 21.1MB/s]"
          }
        },
        "bdbc78a9cc334e0c9bd8e558da2b3151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d1cc36db4d34daa916d7d0f6184121d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a804139fbd7a4b23b3e32636d17110c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2f020f0b69243cfba8c36dc6dec42a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62e5b18190c40899d0513f8ea05fabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1acf3667e1d042239b96a8c46ea8751b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b9466e761164eb9a38d19b23ceed9bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f7c911fdf12463683ef6d5efa2a2198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec4d3ea73f224d7b8eb20fdb624e3e7a",
              "IPY_MODEL_14e4e824690942818126998f44acb9d6",
              "IPY_MODEL_7c6dbec1477f49e8bd32ca0c93498e8c"
            ],
            "layout": "IPY_MODEL_c3673866b33a4b74b71ec139391bf403"
          }
        },
        "ec4d3ea73f224d7b8eb20fdb624e3e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b222a85b807e451c9daeb73701903ebe",
            "placeholder": "​",
            "style": "IPY_MODEL_cb406d7ea393418c9252259cf0271778",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "14e4e824690942818126998f44acb9d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92cf911c1e1a4e2eaeac420f71dd9235",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f227c9879bb450593e6e81b552b588e",
            "value": 99
          }
        },
        "7c6dbec1477f49e8bd32ca0c93498e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d40858103e84f7e904cf6cbaad561df",
            "placeholder": "​",
            "style": "IPY_MODEL_979b236c720d445d991c046e4a9faf24",
            "value": " 99.0/99.0 [00:00&lt;00:00, 1.50kB/s]"
          }
        },
        "c3673866b33a4b74b71ec139391bf403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b222a85b807e451c9daeb73701903ebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb406d7ea393418c9252259cf0271778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92cf911c1e1a4e2eaeac420f71dd9235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f227c9879bb450593e6e81b552b588e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d40858103e84f7e904cf6cbaad561df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "979b236c720d445d991c046e4a9faf24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamiti-ruet-ece/Huggingface-Tutorial/blob/main/HF_6_2_Training_new_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "3d8dd116e1d344efbad454047c54424f",
            "c86520eeff954d0b919ac826b440e4d4",
            "3cc53cd551884afdae0e3fc7c4355c98",
            "bb8f22e73c60447e83d1a7b324a6ded8",
            "c458cb773b95421d91d9bd2cec56ea71",
            "9a060f431658474b91d80d6c3f915a2e",
            "618fbee68eb04f72a779b29a724e1258",
            "6594a3cfae064f0b80c75684de04a643",
            "cd651e720d524b7daaa67820f1e56e4f",
            "16503d511cfd4648babd91668ec8ff5e",
            "f68700ccf93948749d9dd24eb4bea4aa",
            "3c1c31095c4d48d7aa2d2c06d37fab99",
            "54a6ee59dc3248d09007f16024c30caf",
            "622911412f2f45cc9e8e0e02f9e6d1e2",
            "a295c67eb7aa4654ad25ed22d9af3c66",
            "a14643588d634e2fbdb1088daff5da19",
            "c59f3f139bdd41f59fa6fbd2d7a528d3",
            "868bc00a7c0e4c92950925b73c98c4c9",
            "43a48ab54b7d46f0a8dac5795fa26246",
            "2cd72b4d778948109924a15ffc38e373",
            "5897189d30c1456ca8d3f9d40b9b0b59",
            "a15f2dce3baf4407a2fc322b020ff6da",
            "014f93625a144da9940f6d8538415374",
            "60babe503491439a8cacc1444264150f",
            "48ac04ab79f84accbf7ed5373b0e715f",
            "a477b607b04846a781de3f9bbbf1e449",
            "66e95c7b1e8b4f32bb74efcf9b17da8d",
            "a7505d9c06094f668c7e9f911f0681a5",
            "654e58d8121c4fef853d38047a8656bd",
            "f411ef5b9d2f47179cbf198debaa8e3e",
            "db8d4153c1e54f7e8f23375505de8bff",
            "63c8ba8605e94508a79edcf6dd16363c",
            "14c03cfc24f14d8c99b865fd81dcef68",
            "b599cea45a4140749dca570786742296",
            "fdf6c66f4a974bfeba4b482666796e94",
            "9d7b5d0d320145fda09a1b49b3a34faf",
            "f9215705ba564ec0bd25aad26b6ca32b",
            "d05349616d1247dc800da279f40ed65e",
            "2f72a522f98d438fb965a2f0d636a87c",
            "539c9942a2fc4673abeb119957a93c47",
            "05f9a112088c46649ea6b96a786d2483",
            "e21e85754ca74276a6a5409d39274548",
            "66c54dfe08ed4fe7a1fcc01762a62be6",
            "c0f8ab81aee04db3a53152672beb08da",
            "9028a74178124e2785717cb9a2effe44",
            "6eff6029ab9b402eab7e7d26de3032bc",
            "512d7367a85a42b0a98a0a17be2a4329",
            "d62af8c1ec8c40b694f5a29474fbe9ee",
            "9ff5137d7a6e425fa4daa85d42ce9713",
            "f9aa72e536ab4d04b463af3eb7231023",
            "e6b8ff8ac2814d938d1421a84841fda9",
            "15d73077d922465f93cb4084557b11d7",
            "7a7896d6502c4e5cb8ec00e7d7121282",
            "581b8ccc790644888d4f18e81bafed35",
            "cd506db1610849a29b2955b18c09b143",
            "cd1215e88a0b4343a2cf74a6d7be666a",
            "ea09e82bf1504f698360ca12333edee1",
            "11e1165f5429472dbb8d69486751c4e4",
            "cfe5dd0a30d140e1a217d00147cd5dc6",
            "9e7c43e0d33c48cda8271d8d8c52ebe2",
            "8cb63f9546a34c548da6ae858ab29aa9",
            "423c6ffffc8f4f2d8403563978c75231",
            "05140144cd5e4ec3a410ca172132a75b",
            "d1b3d9b7454e4a1885fa176a2da195e8",
            "e439369309334285b57c4cd27df363b2",
            "0be7d742aa3a43398ca3659913ec5a47",
            "06ffe4155eeb4c53a07c2e54d3953ce3",
            "67ac937c121e4278b2afeabf4386e8d5",
            "c873fa6cbf95472082a4d96220f009ea",
            "1f54f64236a941609eafa2b4298a2e5d",
            "c9675e0ffd1542798df0fb18c0210a5e",
            "349ce0fb29fd43508766f6dc6abecf5e",
            "179de4abbda84121873a083bd0108c85",
            "ef28c35615f74f1ba7d6a6ea90f000d7",
            "9cc86baf0cc64d52bf79c9ebdfca4d27",
            "1513f60f4a0d4b1da3f527b8fe382539",
            "9a66e770f7b84b11a42a32c928168803",
            "8e53d634bbc64da08127ba8039b34c93",
            "1ae02b2680714e08b7edd74868f5b9d1",
            "6cf29870a92649d89ef5eda735e51d7f",
            "bae97e05fc1b470db3c0ba7070276353",
            "5cbb757ff8754403b7172fc365f4a793",
            "f3d8ac4d367848a8bd61edd1764cfe1c",
            "1e5ef2e02951428190b01128a7be5aaa",
            "87b862ceeb864faab14185c66387967b",
            "a74d7caa36d94b2f970c410ed133bb0a",
            "83ed352331ac4a2fa7a000c33c3b3ada",
            "2391d900a9374dfb8966085954738cfb",
            "87ba064c74234c2d8430b3489152f7a7",
            "6c41974b64e945a09c76bcee0d8e08ae",
            "07799bd7661a4fd5aba3f45b9ccd2aec",
            "0bac8bb0814f4ae897e0fa6420047087",
            "1f22d586cdc04c0f926451942ba85997",
            "174f7546034548e89a5eb2fa7d126fb8",
            "9ecb39fd6ff44e808429e5be9f894719",
            "2c8f8a52fb7140e1b12cc6e2d81fa9ec",
            "1cc5c0f761c84c9ab40641be0c8377b1",
            "37a24537cf2e480e95fc1e1207efff2a",
            "87faefa2d07f46769276c969604902c3",
            "963bec99ebce47b1bad0e4701efa91ef",
            "62cbc04f6d894ddfa07e1d6e10102a04",
            "16b8fbb3f6204361915df1564586d369",
            "139dd58634004ccaa0d143c65579ec69",
            "dc86747532e84583af80259c3ffd0934",
            "16cf5f44338942cbac78bee9cb2baac9",
            "f7bda62911f64176992a43976a77b76c",
            "eb7250e678294efd9db24212eddafeea",
            "909ff87c46ed436688823625d0b91222",
            "4d7112effab94357b9299c4e628f4824",
            "f028e383b52f4966ba26f6baec6f1bf2"
          ]
        },
        "id": "TXuHWD8m4zps",
        "outputId": "1ab311b7-a284-417b-a356-ecd297b59274"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.44k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d8dd116e1d344efbad454047c54424f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/18.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c1c31095c4d48d7aa2d2c06d37fab99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "014f93625a144da9940f6d8538415374"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b599cea45a4140749dca570786742296"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9028a74178124e2785717cb9a2effe44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd1215e88a0b4343a2cf74a6d7be666a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06ffe4155eeb4c53a07c2e54d3953ce3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e53d634bbc64da08127ba8039b34c93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87ba064c74234c2d8430b3489152f7a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "963bec99ebce47b1bad0e4701efa91ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ-D1LCD4qc4",
        "outputId": "708a798b-324c-4e35-90ac-8e5c43ae3d8a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
              "    num_rows: 412178\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwK4j1LF-CJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16qPcQaI6ytS",
        "outputId": "0d198ad2-9ec3-416d-d0dc-5d92153daec8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def _compress_for_distribute(max_vol, plan, **kwargs):\n",
            "    \"\"\"\n",
            "    Combines as many dispenses as can fit within the maximum volume\n",
            "    \"\"\"\n",
            "    source = None\n",
            "    new_source = None\n",
            "    a_vol = 0\n",
            "    temp_dispenses = []\n",
            "    new_transfer_plan = []\n",
            "    disposal_vol = kwargs.get('disposal_vol', 0)\n",
            "    max_vol = max_vol - disposal_vol\n",
            "\n",
            "    def _append_dispenses():\n",
            "        nonlocal a_vol, temp_dispenses, new_transfer_plan, source\n",
            "        if not temp_dispenses:\n",
            "            return\n",
            "        added_volume = 0\n",
            "        if len(temp_dispenses) > 1:\n",
            "            added_volume = disposal_vol\n",
            "        new_transfer_plan.append({\n",
            "            'aspirate': {\n",
            "                'location': source,\n",
            "                'volume': a_vol + added_volume\n",
            "            }\n",
            "        })\n",
            "        for d in temp_dispenses:\n",
            "            new_transfer_plan.append({\n",
            "                'dispense': {\n",
            "                    'location': d['location'],\n",
            "                    'volume': d['volume']\n",
            "                }\n",
            "            })\n",
            "        a_vol = 0\n",
            "        temp_dispenses = []\n",
            "\n",
            "    for p in plan:\n",
            "        this_vol = p['aspirate']['volume']\n",
            "        new_source = p['aspirate']['location']\n",
            "        if (new_source is not source) or (this_vol + a_vol > max_vol):\n",
            "            _append_dispenses()\n",
            "        source = new_source\n",
            "        a_vol += this_vol\n",
            "        temp_dispenses.append(p['dispense'])\n",
            "    _append_dispenses()\n",
            "    return new_transfer_plan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_corpus = (\n",
        "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        ")"
      ],
      "metadata": {
        "id": "swT4BThw-DqK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]"
      ],
      "metadata": {
        "id": "6LnQ_Ca1_5kV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_corpus = (\n",
        "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        ")"
      ],
      "metadata": {
        "id": "KtkyDdeR_6U6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = (i for i in range(10))\n",
        "print(list(gen))\n",
        "print(list(gen))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I38fBg5H_8AZ",
        "outputId": "98e8948d-0378-4b30-b5f9-ceb3c408383c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "    return (\n",
        "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        "    )\n",
        "\n",
        "\n",
        "training_corpus = get_training_corpus()"
      ],
      "metadata": {
        "id": "oZTpfwHe__yq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "    dataset = raw_datasets[\"train\"]\n",
        "    for start_idx in range(0, len(dataset), 1000):\n",
        "        samples = dataset[start_idx : start_idx + 1000]\n",
        "        yield samples[\"whole_func_string\"]"
      ],
      "metadata": {
        "id": "-YcUI8rXABmi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El00l-H1ADpK",
        "outputId": "a3f6c5ae-c17b-422c-c20f-86531729c319"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object get_training_corpus.<locals>.<genexpr> at 0x78f10af2d230>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(next(training_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvKXc1tzBVxl",
        "outputId": "d39a09fd-6f46-4ad4-99d5-47f171dd018e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['def fnmatch(name, pat):\\n    \"\"\"Test whether FILENAME matches PATTERN.\\n\\n    Patterns are Unix shell style:\\n\\n    *       matches everything\\n    ?       matches any single character\\n    [seq]   matches any character in seq\\n    [!seq]  matches any char not in seq\\n\\n    An initial period in FILENAME is not special.\\n    Both FILENAME and PATTERN are first case-normalized\\n    if the operating system requires it.\\n    If you don\\'t want this, use fnmatchcase(FILENAME, PATTERN).\\n    \"\"\"\\n\\n    name = name.lower()\\n    pat = pat.lower()\\n    return fnmatchcase(name, pat)', 'def get(self, config_id):\\n        \"\"\"\\n        Get a config.\\n\\n        Args:\\n            config_id (str): Config ID.\\n\\n        Returns:\\n            (:py:class:`Config`): The config.\\n\\n        Raises:\\n            :py:class:`docker.errors.NotFound`\\n                If the config does not exist.\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.prepare_model(self.client.api.inspect_config(config_id))', 'def get_connection(self, *args, **kwargs):\\n        \"\"\"\\n        Ensure assert_hostname is set correctly on our pool\\n\\n        We already take care of a normal poolmanager via init_poolmanager\\n\\n        But we still need to take care of when there is a proxy poolmanager\\n        \"\"\"\\n        conn = super(SSLHTTPAdapter, self).get_connection(*args, **kwargs)\\n        if conn.assert_hostname != self.assert_hostname:\\n            conn.assert_hostname = self.assert_hostname\\n        return conn', 'def exclude_paths(root, patterns, dockerfile=None):\\n    \"\"\"\\n    Given a root directory path and a list of .dockerignore patterns, return\\n    an iterator of all paths (both regular files and directories) in the root\\n    directory that do *not* match any of the patterns.\\n\\n    All paths returned are relative to the root.\\n    \"\"\"\\n\\n    if dockerfile is None:\\n        dockerfile = \\'Dockerfile\\'\\n\\n    patterns.append(\\'!\\' + dockerfile)\\n    pm = PatternMatcher(patterns)\\n    return set(pm.walk(root))', 'def get_image(self, image, chunk_size=DEFAULT_DATA_CHUNK_SIZE):\\n        \"\"\"\\n        Get a tarball of an image. Similar to the ``docker save`` command.\\n\\n        Args:\\n            image (str): Image name to get\\n            chunk_size (int): The number of bytes returned by each iteration\\n                of the generator. If ``None``, data will be streamed as it is\\n                received. Default: 2 MB\\n\\n        Returns:\\n            (generator): A stream of raw archive data.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Example:\\n\\n            >>> image = cli.get_image(\"busybox:latest\")\\n            >>> f = open(\\'/tmp/busybox-latest.tar\\', \\'wb\\')\\n            >>> for chunk in image:\\n            >>>   f.write(chunk)\\n            >>> f.close()\\n        \"\"\"\\n        res = self._get(self._url(\"/images/{0}/get\", image), stream=True)\\n        return self._stream_raw_result(res, chunk_size, False)', 'def history(self, image):\\n        \"\"\"\\n        Show the history of an image.\\n\\n        Args:\\n            image (str): The image to show history for\\n\\n        Returns:\\n            (str): The history of the image\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        res = self._get(self._url(\"/images/{0}/history\", image))\\n        return self._result(res, True)', 'def images(self, name=None, quiet=False, all=False, filters=None):\\n        \"\"\"\\n        List images. Similar to the ``docker images`` command.\\n\\n        Args:\\n            name (str): Only show images belonging to the repository ``name``\\n            quiet (bool): Only return numeric IDs as a list.\\n            all (bool): Show intermediate image layers. By default, these are\\n                filtered out.\\n            filters (dict): Filters to be processed on the image list.\\n                Available filters:\\n                - ``dangling`` (bool)\\n                - ``label`` (str): format either ``key`` or ``key=value``\\n\\n        Returns:\\n            (dict or list): A list if ``quiet=True``, otherwise a dict.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        params = {\\n            \\'filter\\': name,\\n            \\'only_ids\\': 1 if quiet else 0,\\n            \\'all\\': 1 if all else 0,\\n        }\\n        if filters:\\n            params[\\'filters\\'] = utils.convert_filters(filters)\\n        res = self._result(self._get(self._url(\"/images/json\"), params=params),\\n                           True)\\n        if quiet:\\n            return [x[\\'Id\\'] for x in res]\\n        return res', 'def import_image(self, src=None, repository=None, tag=None, image=None,\\n                     changes=None, stream_src=False):\\n        \"\"\"\\n        Import an image. Similar to the ``docker import`` command.\\n\\n        If ``src`` is a string or unicode string, it will first be treated as a\\n        path to a tarball on the local system. If there is an error reading\\n        from that file, ``src`` will be treated as a URL instead to fetch the\\n        image from. You can also pass an open file handle as ``src``, in which\\n        case the data will be read from that file.\\n\\n        If ``src`` is unset but ``image`` is set, the ``image`` parameter will\\n        be taken as the name of an existing image to import from.\\n\\n        Args:\\n            src (str or file): Path to tarfile, URL, or file-like object\\n            repository (str): The repository to create\\n            tag (str): The tag to apply\\n            image (str): Use another image like the ``FROM`` Dockerfile\\n                parameter\\n        \"\"\"\\n        if not (src or image):\\n            raise errors.DockerException(\\n                \\'Must specify src or image to import from\\'\\n            )\\n        u = self._url(\\'/images/create\\')\\n\\n        params = _import_image_params(\\n            repository, tag, image,\\n            src=(src if isinstance(src, six.string_types) else None),\\n            changes=changes\\n        )\\n        headers = {\\'Content-Type\\': \\'application/tar\\'}\\n\\n        if image or params.get(\\'fromSrc\\') != \\'-\\':  # from image or URL\\n            return self._result(\\n                self._post(u, data=None, params=params)\\n            )\\n        elif isinstance(src, six.string_types):  # from file path\\n            with open(src, \\'rb\\') as f:\\n                return self._result(\\n                    self._post(\\n                        u, data=f, params=params, headers=headers, timeout=None\\n                    )\\n                )\\n        else:  # from raw data\\n            if stream_src:\\n                headers[\\'Transfer-Encoding\\'] = \\'chunked\\'\\n            return self._result(\\n                self._post(u, data=src, params=params, headers=headers)\\n            )', 'def import_image_from_data(self, data, repository=None, tag=None,\\n                               changes=None):\\n        \"\"\"\\n        Like :py:meth:`~docker.api.image.ImageApiMixin.import_image`, but\\n        allows importing in-memory bytes data.\\n\\n        Args:\\n            data (bytes collection): Bytes collection containing valid tar data\\n            repository (str): The repository to create\\n            tag (str): The tag to apply\\n        \"\"\"\\n\\n        u = self._url(\\'/images/create\\')\\n        params = _import_image_params(\\n            repository, tag, src=\\'-\\', changes=changes\\n        )\\n        headers = {\\'Content-Type\\': \\'application/tar\\'}\\n        return self._result(\\n            self._post(\\n                u, data=data, params=params, headers=headers, timeout=None\\n            )\\n        )', 'def import_image_from_file(self, filename, repository=None, tag=None,\\n                               changes=None):\\n        \"\"\"\\n        Like :py:meth:`~docker.api.image.ImageApiMixin.import_image`, but only\\n        supports importing from a tar file on disk.\\n\\n        Args:\\n            filename (str): Full path to a tar file.\\n            repository (str): The repository to create\\n            tag (str): The tag to apply\\n\\n        Raises:\\n            IOError: File does not exist.\\n        \"\"\"\\n\\n        return self.import_image(\\n            src=filename, repository=repository, tag=tag, changes=changes\\n        )', 'def import_image_from_url(self, url, repository=None, tag=None,\\n                              changes=None):\\n        \"\"\"\\n        Like :py:meth:`~docker.api.image.ImageApiMixin.import_image`, but only\\n        supports importing from a URL.\\n\\n        Args:\\n            url (str): A URL pointing to a tar file.\\n            repository (str): The repository to create\\n            tag (str): The tag to apply\\n        \"\"\"\\n        return self.import_image(\\n            src=url, repository=repository, tag=tag, changes=changes\\n        )', 'def import_image_from_image(self, image, repository=None, tag=None,\\n                                changes=None):\\n        \"\"\"\\n        Like :py:meth:`~docker.api.image.ImageApiMixin.import_image`, but only\\n        supports importing from another image, like the ``FROM`` Dockerfile\\n        parameter.\\n\\n        Args:\\n            image (str): Image name to import from\\n            repository (str): The repository to create\\n            tag (str): The tag to apply\\n        \"\"\"\\n        return self.import_image(\\n            image=image, repository=repository, tag=tag, changes=changes\\n        )', 'def inspect_image(self, image):\\n        \"\"\"\\n        Get detailed information about an image. Similar to the ``docker\\n        inspect`` command, but only for images.\\n\\n        Args:\\n            image (str): The image to inspect\\n\\n        Returns:\\n            (dict): Similar to the output of ``docker inspect``, but as a\\n        single dict\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self._result(\\n            self._get(self._url(\"/images/{0}/json\", image)), True\\n        )', 'def inspect_distribution(self, image, auth_config=None):\\n        \"\"\"\\n        Get image digest and platform information by contacting the registry.\\n\\n        Args:\\n            image (str): The image name to inspect\\n            auth_config (dict): Override the credentials that are found in the\\n                config for this request.  ``auth_config`` should contain the\\n                ``username`` and ``password`` keys to be valid.\\n\\n        Returns:\\n            (dict): A dict containing distribution data\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        registry, _ = auth.resolve_repository_name(image)\\n\\n        headers = {}\\n        if auth_config is None:\\n            header = auth.get_config_header(self, registry)\\n            if header:\\n                headers[\\'X-Registry-Auth\\'] = header\\n        else:\\n            log.debug(\\'Sending supplied auth config\\')\\n            headers[\\'X-Registry-Auth\\'] = auth.encode_header(auth_config)\\n\\n        url = self._url(\"/distribution/{0}/json\", image)\\n\\n        return self._result(\\n            self._get(url, headers=headers), True\\n        )', 'def load_image(self, data, quiet=None):\\n        \"\"\"\\n        Load an image that was previously saved using\\n        :py:meth:`~docker.api.image.ImageApiMixin.get_image` (or ``docker\\n        save``). Similar to ``docker load``.\\n\\n        Args:\\n            data (binary): Image data to be loaded.\\n            quiet (boolean): Suppress progress details in response.\\n\\n        Returns:\\n            (generator): Progress output as JSON objects. Only available for\\n                         API version >= 1.23\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        params = {}\\n\\n        if quiet is not None:\\n            if utils.version_lt(self._version, \\'1.23\\'):\\n                raise errors.InvalidVersion(\\n                    \\'quiet is not supported in API version < 1.23\\'\\n                )\\n            params[\\'quiet\\'] = quiet\\n\\n        res = self._post(\\n            self._url(\"/images/load\"), data=data, params=params, stream=True\\n        )\\n        if utils.version_gte(self._version, \\'1.23\\'):\\n            return self._stream_helper(res, decode=True)\\n\\n        self._raise_for_status(res)', 'def pull(self, repository, tag=None, stream=False, auth_config=None,\\n             decode=False, platform=None):\\n        \"\"\"\\n        Pulls an image. Similar to the ``docker pull`` command.\\n\\n        Args:\\n            repository (str): The repository to pull\\n            tag (str): The tag to pull\\n            stream (bool): Stream the output as a generator. Make sure to\\n                consume the generator, otherwise pull might get cancelled.\\n            auth_config (dict): Override the credentials that are found in the\\n                config for this request.  ``auth_config`` should contain the\\n                ``username`` and ``password`` keys to be valid.\\n            decode (bool): Decode the JSON data from the server into dicts.\\n                Only applies with ``stream=True``\\n            platform (str): Platform in the format ``os[/arch[/variant]]``\\n\\n        Returns:\\n            (generator or str): The output\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Example:\\n\\n            >>> for line in cli.pull(\\'busybox\\', stream=True, decode=True):\\n            ...     print(json.dumps(line, indent=4))\\n            {\\n                \"status\": \"Pulling image (latest) from busybox\",\\n                \"progressDetail\": {},\\n                \"id\": \"e72ac664f4f0\"\\n            }\\n            {\\n                \"status\": \"Pulling image (latest) from busybox, endpoint: ...\",\\n                \"progressDetail\": {},\\n                \"id\": \"e72ac664f4f0\"\\n            }\\n\\n        \"\"\"\\n        if not tag:\\n            repository, tag = utils.parse_repository_tag(repository)\\n        registry, repo_name = auth.resolve_repository_name(repository)\\n\\n        params = {\\n            \\'tag\\': tag,\\n            \\'fromImage\\': repository\\n        }\\n        headers = {}\\n\\n        if auth_config is None:\\n            header = auth.get_config_header(self, registry)\\n            if header:\\n                headers[\\'X-Registry-Auth\\'] = header\\n        else:\\n            log.debug(\\'Sending supplied auth config\\')\\n            headers[\\'X-Registry-Auth\\'] = auth.encode_header(auth_config)\\n\\n        if platform is not None:\\n            if utils.version_lt(self._version, \\'1.32\\'):\\n                raise errors.InvalidVersion(\\n                    \\'platform was only introduced in API version 1.32\\'\\n                )\\n            params[\\'platform\\'] = platform\\n\\n        response = self._post(\\n            self._url(\\'/images/create\\'), params=params, headers=headers,\\n            stream=stream, timeout=None\\n        )\\n\\n        self._raise_for_status(response)\\n\\n        if stream:\\n            return self._stream_helper(response, decode=decode)\\n\\n        return self._result(response)', 'def push(self, repository, tag=None, stream=False, auth_config=None,\\n             decode=False):\\n        \"\"\"\\n        Push an image or a repository to the registry. Similar to the ``docker\\n        push`` command.\\n\\n        Args:\\n            repository (str): The repository to push to\\n            tag (str): An optional tag to push\\n            stream (bool): Stream the output as a blocking generator\\n            auth_config (dict): Override the credentials that are found in the\\n                config for this request.  ``auth_config`` should contain the\\n                ``username`` and ``password`` keys to be valid.\\n            decode (bool): Decode the JSON data from the server into dicts.\\n                Only applies with ``stream=True``\\n\\n        Returns:\\n            (generator or str): The output from the server.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Example:\\n            >>> for line in cli.push(\\'yourname/app\\', stream=True, decode=True):\\n            ...   print(line)\\n            {\\'status\\': \\'Pushing repository yourname/app (1 tags)\\'}\\n            {\\'status\\': \\'Pushing\\',\\'progressDetail\\': {}, \\'id\\': \\'511136ea3c5a\\'}\\n            {\\'status\\': \\'Image already pushed, skipping\\', \\'progressDetail\\':{},\\n             \\'id\\': \\'511136ea3c5a\\'}\\n            ...\\n\\n        \"\"\"\\n        if not tag:\\n            repository, tag = utils.parse_repository_tag(repository)\\n        registry, repo_name = auth.resolve_repository_name(repository)\\n        u = self._url(\"/images/{0}/push\", repository)\\n        params = {\\n            \\'tag\\': tag\\n        }\\n        headers = {}\\n\\n        if auth_config is None:\\n            header = auth.get_config_header(self, registry)\\n            if header:\\n                headers[\\'X-Registry-Auth\\'] = header\\n        else:\\n            log.debug(\\'Sending supplied auth config\\')\\n            headers[\\'X-Registry-Auth\\'] = auth.encode_header(auth_config)\\n\\n        response = self._post_json(\\n            u, None, headers=headers, stream=stream, params=params\\n        )\\n\\n        self._raise_for_status(response)\\n\\n        if stream:\\n            return self._stream_helper(response, decode=decode)\\n\\n        return self._result(response)', 'def remove_image(self, image, force=False, noprune=False):\\n        \"\"\"\\n        Remove an image. Similar to the ``docker rmi`` command.\\n\\n        Args:\\n            image (str): The image to remove\\n            force (bool): Force removal of the image\\n            noprune (bool): Do not delete untagged parents\\n        \"\"\"\\n        params = {\\'force\\': force, \\'noprune\\': noprune}\\n        res = self._delete(self._url(\"/images/{0}\", image), params=params)\\n        return self._result(res, True)', 'def search(self, term):\\n        \"\"\"\\n        Search for images on Docker Hub. Similar to the ``docker search``\\n        command.\\n\\n        Args:\\n            term (str): A term to search for.\\n\\n        Returns:\\n            (list of dicts): The response of the search.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self._result(\\n            self._get(self._url(\"/images/search\"), params={\\'term\\': term}),\\n            True\\n        )', 'def tag(self, image, repository, tag=None, force=False):\\n        \"\"\"\\n        Tag an image into a repository. Similar to the ``docker tag`` command.\\n\\n        Args:\\n            image (str): The image to tag\\n            repository (str): The repository to set for the tag\\n            tag (str): The tag name\\n            force (bool): Force\\n\\n        Returns:\\n            (bool): ``True`` if successful\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Example:\\n\\n            >>> client.tag(\\'ubuntu\\', \\'localhost:5000/ubuntu\\', \\'latest\\',\\n                           force=True)\\n        \"\"\"\\n        params = {\\n            \\'tag\\': tag,\\n            \\'repo\\': repository,\\n            \\'force\\': 1 if force else 0\\n        }\\n        url = self._url(\"/images/{0}/tag\", image)\\n        res = self._post(url, params=params)\\n        self._raise_for_status(res)\\n        return res.status_code == 201', 'def attach(self, container, stdout=True, stderr=True,\\n               stream=False, logs=False, demux=False):\\n        \"\"\"\\n        Attach to a container.\\n\\n        The ``.logs()`` function is a wrapper around this method, which you can\\n        use instead if you want to fetch/stream container output without first\\n        retrieving the entire backlog.\\n\\n        Args:\\n            container (str): The container to attach to.\\n            stdout (bool): Include stdout.\\n            stderr (bool): Include stderr.\\n            stream (bool): Return container output progressively as an iterator\\n                of strings, rather than a single string.\\n            logs (bool): Include the container\\'s previous output.\\n            demux (bool): Keep stdout and stderr separate.\\n\\n        Returns:\\n            By default, the container\\'s output as a single string (two if\\n            ``demux=True``: one for stdout and one for stderr).\\n\\n            If ``stream=True``, an iterator of output strings. If\\n            ``demux=True``, two iterators are returned: one for stdout and one\\n            for stderr.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        params = {\\n            \\'logs\\': logs and 1 or 0,\\n            \\'stdout\\': stdout and 1 or 0,\\n            \\'stderr\\': stderr and 1 or 0,\\n            \\'stream\\': stream and 1 or 0\\n        }\\n\\n        headers = {\\n            \\'Connection\\': \\'Upgrade\\',\\n            \\'Upgrade\\': \\'tcp\\'\\n        }\\n\\n        u = self._url(\"/containers/{0}/attach\", container)\\n        response = self._post(u, headers=headers, params=params, stream=True)\\n\\n        output = self._read_from_socket(\\n            response, stream, self._check_is_tty(container), demux=demux)\\n\\n        if stream:\\n            return CancellableStream(output, response)\\n        else:\\n            return output', 'def attach_socket(self, container, params=None, ws=False):\\n        \"\"\"\\n        Like ``attach``, but returns the underlying socket-like object for the\\n        HTTP request.\\n\\n        Args:\\n            container (str): The container to attach to.\\n            params (dict): Dictionary of request parameters (e.g. ``stdout``,\\n                ``stderr``, ``stream``).\\n                For ``detachKeys``, ~/.docker/config.json is used by default.\\n            ws (bool): Use websockets instead of raw HTTP.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        if params is None:\\n            params = {\\n                \\'stdout\\': 1,\\n                \\'stderr\\': 1,\\n                \\'stream\\': 1\\n            }\\n\\n        if \\'detachKeys\\' not in params \\\\\\n                and \\'detachKeys\\' in self._general_configs:\\n\\n            params[\\'detachKeys\\'] = self._general_configs[\\'detachKeys\\']\\n\\n        if ws:\\n            return self._attach_websocket(container, params)\\n\\n        headers = {\\n            \\'Connection\\': \\'Upgrade\\',\\n            \\'Upgrade\\': \\'tcp\\'\\n        }\\n\\n        u = self._url(\"/containers/{0}/attach\", container)\\n        return self._get_raw_response_socket(\\n            self.post(\\n                u, None, params=self._attach_params(params), stream=True,\\n                headers=headers\\n            )\\n        )', 'def commit(self, container, repository=None, tag=None, message=None,\\n               author=None, changes=None, conf=None):\\n        \"\"\"\\n        Commit a container to an image. Similar to the ``docker commit``\\n        command.\\n\\n        Args:\\n            container (str): The image hash of the container\\n            repository (str): The repository to push the image to\\n            tag (str): The tag to push\\n            message (str): A commit message\\n            author (str): The name of the author\\n            changes (str): Dockerfile instructions to apply while committing\\n            conf (dict): The configuration for the container. See the\\n                `Engine API documentation\\n                <https://docs.docker.com/reference/api/docker_remote_api/>`_\\n                for full details.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        params = {\\n            \\'container\\': container,\\n            \\'repo\\': repository,\\n            \\'tag\\': tag,\\n            \\'comment\\': message,\\n            \\'author\\': author,\\n            \\'changes\\': changes\\n        }\\n        u = self._url(\"/commit\")\\n        return self._result(\\n            self._post_json(u, data=conf, params=params), json=True\\n        )', 'def containers(self, quiet=False, all=False, trunc=False, latest=False,\\n                   since=None, before=None, limit=-1, size=False,\\n                   filters=None):\\n        \"\"\"\\n        List containers. Similar to the ``docker ps`` command.\\n\\n        Args:\\n            quiet (bool): Only display numeric Ids\\n            all (bool): Show all containers. Only running containers are shown\\n                by default\\n            trunc (bool): Truncate output\\n            latest (bool): Show only the latest created container, include\\n                non-running ones.\\n            since (str): Show only containers created since Id or Name, include\\n                non-running ones\\n            before (str): Show only container created before Id or Name,\\n                include non-running ones\\n            limit (int): Show `limit` last created containers, include\\n                non-running ones\\n            size (bool): Display sizes\\n            filters (dict): Filters to be processed on the image list.\\n                Available filters:\\n\\n                - `exited` (int): Only containers with specified exit code\\n                - `status` (str): One of ``restarting``, ``running``,\\n                    ``paused``, ``exited``\\n                - `label` (str): format either ``\"key\"`` or ``\"key=value\"``\\n                - `id` (str): The id of the container.\\n                - `name` (str): The name of the container.\\n                - `ancestor` (str): Filter by container ancestor. Format of\\n                    ``<image-name>[:tag]``, ``<image-id>``, or\\n                    ``<image@digest>``.\\n                - `before` (str): Only containers created before a particular\\n                    container. Give the container name or id.\\n                - `since` (str): Only containers created after a particular\\n                    container. Give container name or id.\\n\\n                A comprehensive list can be found in the documentation for\\n                `docker ps\\n                <https://docs.docker.com/engine/reference/commandline/ps>`_.\\n\\n        Returns:\\n            A list of dicts, one per container\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        params = {\\n            \\'limit\\': 1 if latest else limit,\\n            \\'all\\': 1 if all else 0,\\n            \\'size\\': 1 if size else 0,\\n            \\'trunc_cmd\\': 1 if trunc else 0,\\n            \\'since\\': since,\\n            \\'before\\': before\\n        }\\n        if filters:\\n            params[\\'filters\\'] = utils.convert_filters(filters)\\n        u = self._url(\"/containers/json\")\\n        res = self._result(self._get(u, params=params), True)\\n\\n        if quiet:\\n            return [{\\'Id\\': x[\\'Id\\']} for x in res]\\n        if trunc:\\n            for x in res:\\n                x[\\'Id\\'] = x[\\'Id\\'][:12]\\n        return res', 'def create_container(self, image, command=None, hostname=None, user=None,\\n                         detach=False, stdin_open=False, tty=False, ports=None,\\n                         environment=None, volumes=None,\\n                         network_disabled=False, name=None, entrypoint=None,\\n                         working_dir=None, domainname=None, host_config=None,\\n                         mac_address=None, labels=None, stop_signal=None,\\n                         networking_config=None, healthcheck=None,\\n                         stop_timeout=None, runtime=None,\\n                         use_config_proxy=False):\\n        \"\"\"\\n        Creates a container. Parameters are similar to those for the ``docker\\n        run`` command except it doesn\\'t support the attach options (``-a``).\\n\\n        The arguments that are passed directly to this function are\\n        host-independent configuration options. Host-specific configuration\\n        is passed with the `host_config` argument. You\\'ll normally want to\\n        use this method in combination with the :py:meth:`create_host_config`\\n        method to generate ``host_config``.\\n\\n        **Port bindings**\\n\\n        Port binding is done in two parts: first, provide a list of ports to\\n        open inside the container with the ``ports`` parameter, then declare\\n        bindings with the ``host_config`` parameter. For example:\\n\\n        .. code-block:: python\\n\\n            container_id = cli.create_container(\\n                \\'busybox\\', \\'ls\\', ports=[1111, 2222],\\n                host_config=cli.create_host_config(port_bindings={\\n                    1111: 4567,\\n                    2222: None\\n                })\\n            )\\n\\n\\n        You can limit the host address on which the port will be exposed like\\n        such:\\n\\n        .. code-block:: python\\n\\n            cli.create_host_config(port_bindings={1111: (\\'127.0.0.1\\', 4567)})\\n\\n        Or without host port assignment:\\n\\n        .. code-block:: python\\n\\n            cli.create_host_config(port_bindings={1111: (\\'127.0.0.1\\',)})\\n\\n        If you wish to use UDP instead of TCP (default), you need to declare\\n        ports as such in both the config and host config:\\n\\n        .. code-block:: python\\n\\n            container_id = cli.create_container(\\n                \\'busybox\\', \\'ls\\', ports=[(1111, \\'udp\\'), 2222],\\n                host_config=cli.create_host_config(port_bindings={\\n                    \\'1111/udp\\': 4567, 2222: None\\n                })\\n            )\\n\\n        To bind multiple host ports to a single container port, use the\\n        following syntax:\\n\\n        .. code-block:: python\\n\\n            cli.create_host_config(port_bindings={\\n                1111: [1234, 4567]\\n            })\\n\\n        You can also bind multiple IPs to a single container port:\\n\\n        .. code-block:: python\\n\\n            cli.create_host_config(port_bindings={\\n                1111: [\\n                    (\\'192.168.0.100\\', 1234),\\n                    (\\'192.168.0.101\\', 1234)\\n                ]\\n            })\\n\\n        **Using volumes**\\n\\n        Volume declaration is done in two parts. Provide a list of\\n        paths to use as mountpoints inside the container with the\\n        ``volumes`` parameter, and declare mappings from paths on the host\\n        in the ``host_config`` section.\\n\\n        .. code-block:: python\\n\\n            container_id = cli.create_container(\\n                \\'busybox\\', \\'ls\\', volumes=[\\'/mnt/vol1\\', \\'/mnt/vol2\\'],\\n                host_config=cli.create_host_config(binds={\\n                    \\'/home/user1/\\': {\\n                        \\'bind\\': \\'/mnt/vol2\\',\\n                        \\'mode\\': \\'rw\\',\\n                    },\\n                    \\'/var/www\\': {\\n                        \\'bind\\': \\'/mnt/vol1\\',\\n                        \\'mode\\': \\'ro\\',\\n                    }\\n                })\\n            )\\n\\n        You can alternatively specify binds as a list. This code is equivalent\\n        to the example above:\\n\\n        .. code-block:: python\\n\\n            container_id = cli.create_container(\\n                \\'busybox\\', \\'ls\\', volumes=[\\'/mnt/vol1\\', \\'/mnt/vol2\\'],\\n                host_config=cli.create_host_config(binds=[\\n                    \\'/home/user1/:/mnt/vol2\\',\\n                    \\'/var/www:/mnt/vol1:ro\\',\\n                ])\\n            )\\n\\n        **Networking**\\n\\n        You can specify networks to connect the container to by using the\\n        ``networking_config`` parameter. At the time of creation, you can\\n        only connect a container to a single networking, but you\\n        can create more connections by using\\n        :py:meth:`~connect_container_to_network`.\\n\\n        For example:\\n\\n        .. code-block:: python\\n\\n            networking_config = docker_client.create_networking_config({\\n                \\'network1\\': docker_client.create_endpoint_config(\\n                    ipv4_address=\\'172.28.0.124\\',\\n                    aliases=[\\'foo\\', \\'bar\\'],\\n                    links=[\\'container2\\']\\n                )\\n            })\\n\\n            ctnr = docker_client.create_container(\\n                img, command, networking_config=networking_config\\n            )\\n\\n        Args:\\n            image (str): The image to run\\n            command (str or list): The command to be run in the container\\n            hostname (str): Optional hostname for the container\\n            user (str or int): Username or UID\\n            detach (bool): Detached mode: run container in the background and\\n                return container ID\\n            stdin_open (bool): Keep STDIN open even if not attached\\n            tty (bool): Allocate a pseudo-TTY\\n            ports (list of ints): A list of port numbers\\n            environment (dict or list): A dictionary or a list of strings in\\n                the following format ``[\"PASSWORD=xxx\"]`` or\\n                ``{\"PASSWORD\": \"xxx\"}``.\\n            volumes (str or list): List of paths inside the container to use\\n                as volumes.\\n            network_disabled (bool): Disable networking\\n            name (str): A name for the container\\n            entrypoint (str or list): An entrypoint\\n            working_dir (str): Path to the working directory\\n            domainname (str): The domain name to use for the container\\n            host_config (dict): A dictionary created with\\n                :py:meth:`create_host_config`.\\n            mac_address (str): The Mac Address to assign the container\\n            labels (dict or list): A dictionary of name-value labels (e.g.\\n                ``{\"label1\": \"value1\", \"label2\": \"value2\"}``) or a list of\\n                names of labels to set with empty values (e.g.\\n                ``[\"label1\", \"label2\"]``)\\n            stop_signal (str): The stop signal to use to stop the container\\n                (e.g. ``SIGINT``).\\n            stop_timeout (int): Timeout to stop the container, in seconds.\\n                Default: 10\\n            networking_config (dict): A networking configuration generated\\n                by :py:meth:`create_networking_config`.\\n            runtime (str): Runtime to use with this container.\\n            healthcheck (dict): Specify a test to perform to check that the\\n                container is healthy.\\n            use_config_proxy (bool): If ``True``, and if the docker client\\n                configuration file (``~/.docker/config.json`` by default)\\n                contains a proxy configuration, the corresponding environment\\n                variables will be set in the container being created.\\n\\n        Returns:\\n            A dictionary with an image \\'Id\\' key and a \\'Warnings\\' key.\\n\\n        Raises:\\n            :py:class:`docker.errors.ImageNotFound`\\n                If the specified image does not exist.\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        if isinstance(volumes, six.string_types):\\n            volumes = [volumes, ]\\n\\n        if isinstance(environment, dict):\\n            environment = utils.utils.format_environment(environment)\\n\\n        if use_config_proxy:\\n            environment = self._proxy_configs.inject_proxy_environment(\\n                environment\\n            )\\n\\n        config = self.create_container_config(\\n            image, command, hostname, user, detach, stdin_open, tty,\\n            ports, environment, volumes,\\n            network_disabled, entrypoint, working_dir, domainname,\\n            host_config, mac_address, labels,\\n            stop_signal, networking_config, healthcheck,\\n            stop_timeout, runtime\\n        )\\n        return self.create_container_from_config(config, name)', 'def create_host_config(self, *args, **kwargs):\\n        \"\"\"\\n        Create a dictionary for the ``host_config`` argument to\\n        :py:meth:`create_container`.\\n\\n        Args:\\n            auto_remove (bool): enable auto-removal of the container on daemon\\n                side when the container\\'s process exits.\\n            binds (dict): Volumes to bind. See :py:meth:`create_container`\\n                    for more information.\\n            blkio_weight_device: Block IO weight (relative device weight) in\\n                the form of: ``[{\"Path\": \"device_path\", \"Weight\": weight}]``.\\n            blkio_weight: Block IO weight (relative weight), accepts a weight\\n                value between 10 and 1000.\\n            cap_add (list of str): Add kernel capabilities. For example,\\n                ``[\"SYS_ADMIN\", \"MKNOD\"]``.\\n            cap_drop (list of str): Drop kernel capabilities.\\n            cpu_period (int): The length of a CPU period in microseconds.\\n            cpu_quota (int): Microseconds of CPU time that the container can\\n                get in a CPU period.\\n            cpu_shares (int): CPU shares (relative weight).\\n            cpuset_cpus (str): CPUs in which to allow execution (``0-3``,\\n                ``0,1``).\\n            cpuset_mems (str): Memory nodes (MEMs) in which to allow execution\\n                (``0-3``, ``0,1``). Only effective on NUMA systems.\\n            device_cgroup_rules (:py:class:`list`): A list of cgroup rules to\\n                apply to the container.\\n            device_read_bps: Limit read rate (bytes per second) from a device\\n                in the form of: `[{\"Path\": \"device_path\", \"Rate\": rate}]`\\n            device_read_iops: Limit read rate (IO per second) from a device.\\n            device_write_bps: Limit write rate (bytes per second) from a\\n                device.\\n            device_write_iops: Limit write rate (IO per second) from a device.\\n            devices (:py:class:`list`): Expose host devices to the container,\\n                as a list of strings in the form\\n                ``<path_on_host>:<path_in_container>:<cgroup_permissions>``.\\n\\n                For example, ``/dev/sda:/dev/xvda:rwm`` allows the container\\n                to have read-write access to the host\\'s ``/dev/sda`` via a\\n                node named ``/dev/xvda`` inside the container.\\n            dns (:py:class:`list`): Set custom DNS servers.\\n            dns_opt (:py:class:`list`): Additional options to be added to the\\n                container\\'s ``resolv.conf`` file\\n            dns_search (:py:class:`list`): DNS search domains.\\n            extra_hosts (dict): Additional hostnames to resolve inside the\\n                container, as a mapping of hostname to IP address.\\n            group_add (:py:class:`list`): List of additional group names and/or\\n                IDs that the container process will run as.\\n            init (bool): Run an init inside the container that forwards\\n                signals and reaps processes\\n            init_path (str): Path to the docker-init binary\\n            ipc_mode (str): Set the IPC mode for the container.\\n            isolation (str): Isolation technology to use. Default: ``None``.\\n            links (dict): Mapping of links using the\\n                ``{\\'container\\': \\'alias\\'}`` format. The alias is optional.\\n                Containers declared in this dict will be linked to the new\\n                container using the provided alias. Default: ``None``.\\n            log_config (LogConfig): Logging configuration\\n            lxc_conf (dict): LXC config.\\n            mem_limit (float or str): Memory limit. Accepts float values\\n                (which represent the memory limit of the created container in\\n                bytes) or a string with a units identification char\\n                (``100000b``, ``1000k``, ``128m``, ``1g``). If a string is\\n                specified without a units character, bytes are assumed as an\\n            mem_swappiness (int): Tune a container\\'s memory swappiness\\n                behavior. Accepts number between 0 and 100.\\n            memswap_limit (str or int): Maximum amount of memory + swap a\\n                container is allowed to consume.\\n            mounts (:py:class:`list`): Specification for mounts to be added to\\n                the container. More powerful alternative to ``binds``. Each\\n                item in the list is expected to be a\\n                :py:class:`docker.types.Mount` object.\\n            network_mode (str): One of:\\n\\n                - ``bridge`` Create a new network stack for the container on\\n                  the bridge network.\\n                - ``none`` No networking for this container.\\n                - ``container:<name|id>`` Reuse another container\\'s network\\n                  stack.\\n                - ``host`` Use the host network stack.\\n            oom_kill_disable (bool): Whether to disable OOM killer.\\n            oom_score_adj (int): An integer value containing the score given\\n                to the container in order to tune OOM killer preferences.\\n            pid_mode (str): If set to ``host``, use the host PID namespace\\n                inside the container.\\n            pids_limit (int): Tune a container\\'s pids limit. Set ``-1`` for\\n                unlimited.\\n            port_bindings (dict): See :py:meth:`create_container`\\n                    for more information.\\n            privileged (bool): Give extended privileges to this container.\\n            publish_all_ports (bool): Publish all ports to the host.\\n            read_only (bool): Mount the container\\'s root filesystem as read\\n                only.\\n            restart_policy (dict): Restart the container when it exits.\\n                Configured as a dictionary with keys:\\n\\n                - ``Name`` One of ``on-failure``, or ``always``.\\n                - ``MaximumRetryCount`` Number of times to restart the\\n                  container on failure.\\n            security_opt (:py:class:`list`): A list of string values to\\n                customize labels for MLS systems, such as SELinux.\\n            shm_size (str or int): Size of /dev/shm (e.g. ``1G``).\\n            storage_opt (dict): Storage driver options per container as a\\n                key-value mapping.\\n            sysctls (dict): Kernel parameters to set in the container.\\n            tmpfs (dict): Temporary filesystems to mount, as a dictionary\\n                mapping a path inside the container to options for that path.\\n\\n                For example:\\n\\n                .. code-block:: python\\n\\n                    {\\n                        \\'/mnt/vol2\\': \\'\\',\\n                        \\'/mnt/vol1\\': \\'size=3G,uid=1000\\'\\n                    }\\n\\n            ulimits (:py:class:`list`): Ulimits to set inside the container,\\n                as a list of :py:class:`docker.types.Ulimit` instances.\\n            userns_mode (str): Sets the user namespace mode for the container\\n                when user namespace remapping option is enabled. Supported\\n                values are: ``host``\\n            uts_mode (str): Sets the UTS namespace mode for the container.\\n                Supported values are: ``host``\\n            volumes_from (:py:class:`list`): List of container names or IDs to\\n                get volumes from.\\n            runtime (str): Runtime to use with this container.\\n\\n\\n        Returns:\\n            (dict) A dictionary which can be passed to the ``host_config``\\n            argument to :py:meth:`create_container`.\\n\\n        Example:\\n\\n            >>> cli.create_host_config(privileged=True, cap_drop=[\\'MKNOD\\'],\\n                                       volumes_from=[\\'nostalgic_newton\\'])\\n            {\\'CapDrop\\': [\\'MKNOD\\'], \\'LxcConf\\': None, \\'Privileged\\': True,\\n             \\'VolumesFrom\\': [\\'nostalgic_newton\\'], \\'PublishAllPorts\\': False}\\n\\n\"\"\"\\n        if not kwargs:\\n            kwargs = {}\\n        if \\'version\\' in kwargs:\\n            raise TypeError(\\n                \"create_host_config() got an unexpected \"\\n                \"keyword argument \\'version\\'\"\\n            )\\n        kwargs[\\'version\\'] = self._version\\n        return HostConfig(*args, **kwargs)', 'def diff(self, container):\\n        \"\"\"\\n        Inspect changes on a container\\'s filesystem.\\n\\n        Args:\\n            container (str): The container to diff\\n\\n        Returns:\\n            (str)\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self._result(\\n            self._get(self._url(\"/containers/{0}/changes\", container)), True\\n        )', 'def export(self, container, chunk_size=DEFAULT_DATA_CHUNK_SIZE):\\n        \"\"\"\\n        Export the contents of a filesystem as a tar archive.\\n\\n        Args:\\n            container (str): The container to export\\n            chunk_size (int): The number of bytes returned by each iteration\\n                of the generator. If ``None``, data will be streamed as it is\\n                received. Default: 2 MB\\n\\n        Returns:\\n            (generator): The archived filesystem data stream\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        res = self._get(\\n            self._url(\"/containers/{0}/export\", container), stream=True\\n        )\\n        return self._stream_raw_result(res, chunk_size, False)', 'def get_archive(self, container, path, chunk_size=DEFAULT_DATA_CHUNK_SIZE):\\n        \"\"\"\\n        Retrieve a file or folder from a container in the form of a tar\\n        archive.\\n\\n        Args:\\n            container (str): The container where the file is located\\n            path (str): Path to the file or folder to retrieve\\n            chunk_size (int): The number of bytes returned by each iteration\\n                of the generator. If ``None``, data will be streamed as it is\\n                received. Default: 2 MB\\n\\n        Returns:\\n            (tuple): First element is a raw tar data stream. Second element is\\n            a dict containing ``stat`` information on the specified ``path``.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Example:\\n\\n            >>> c = docker.APIClient()\\n            >>> f = open(\\'./sh_bin.tar\\', \\'wb\\')\\n            >>> bits, stat = c.get_archive(container, \\'/bin/sh\\')\\n            >>> print(stat)\\n            {\\'name\\': \\'sh\\', \\'size\\': 1075464, \\'mode\\': 493,\\n             \\'mtime\\': \\'2018-10-01T15:37:48-07:00\\', \\'linkTarget\\': \\'\\'}\\n            >>> for chunk in bits:\\n            ...    f.write(chunk)\\n            >>> f.close()\\n        \"\"\"\\n        params = {\\n            \\'path\\': path\\n        }\\n        url = self._url(\\'/containers/{0}/archive\\', container)\\n        res = self._get(url, params=params, stream=True)\\n        self._raise_for_status(res)\\n        encoded_stat = res.headers.get(\\'x-docker-container-path-stat\\')\\n        return (\\n            self._stream_raw_result(res, chunk_size, False),\\n            utils.decode_json_header(encoded_stat) if encoded_stat else None\\n        )', 'def inspect_container(self, container):\\n        \"\"\"\\n        Identical to the `docker inspect` command, but only for containers.\\n\\n        Args:\\n            container (str): The container to inspect\\n\\n        Returns:\\n            (dict): Similar to the output of `docker inspect`, but as a\\n            single dict\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self._result(\\n            self._get(self._url(\"/containers/{0}/json\", container)), True\\n        )', 'def kill(self, container, signal=None):\\n        \"\"\"\\n        Kill a container or send a signal to a container.\\n\\n        Args:\\n            container (str): The container to kill\\n            signal (str or int): The signal to send. Defaults to ``SIGKILL``\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        url = self._url(\"/containers/{0}/kill\", container)\\n        params = {}\\n        if signal is not None:\\n            if not isinstance(signal, six.string_types):\\n                signal = int(signal)\\n            params[\\'signal\\'] = signal\\n        res = self._post(url, params=params)\\n\\n        self._raise_for_status(res)', 'def logs(self, container, stdout=True, stderr=True, stream=False,\\n             timestamps=False, tail=\\'all\\', since=None, follow=None,\\n             until=None):\\n        \"\"\"\\n        Get logs from a container. Similar to the ``docker logs`` command.\\n\\n        The ``stream`` parameter makes the ``logs`` function return a blocking\\n        generator you can iterate over to retrieve log output as it happens.\\n\\n        Args:\\n            container (str): The container to get logs from\\n            stdout (bool): Get ``STDOUT``. Default ``True``\\n            stderr (bool): Get ``STDERR``. Default ``True``\\n            stream (bool): Stream the response. Default ``False``\\n            timestamps (bool): Show timestamps. Default ``False``\\n            tail (str or int): Output specified number of lines at the end of\\n                logs. Either an integer of number of lines or the string\\n                ``all``. Default ``all``\\n            since (datetime or int): Show logs since a given datetime or\\n                integer epoch (in seconds)\\n            follow (bool): Follow log output. Default ``False``\\n            until (datetime or int): Show logs that occurred before the given\\n                datetime or integer epoch (in seconds)\\n\\n        Returns:\\n            (generator or str)\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        if follow is None:\\n            follow = stream\\n        params = {\\'stderr\\': stderr and 1 or 0,\\n                  \\'stdout\\': stdout and 1 or 0,\\n                  \\'timestamps\\': timestamps and 1 or 0,\\n                  \\'follow\\': follow and 1 or 0,\\n                  }\\n        if tail != \\'all\\' and (not isinstance(tail, int) or tail < 0):\\n            tail = \\'all\\'\\n        params[\\'tail\\'] = tail\\n\\n        if since is not None:\\n            if isinstance(since, datetime):\\n                params[\\'since\\'] = utils.datetime_to_timestamp(since)\\n            elif (isinstance(since, int) and since > 0):\\n                params[\\'since\\'] = since\\n            else:\\n                raise errors.InvalidArgument(\\n                    \\'since value should be datetime or positive int, \\'\\n                    \\'not {}\\'.format(type(since))\\n                )\\n\\n        if until is not None:\\n            if utils.version_lt(self._version, \\'1.35\\'):\\n                raise errors.InvalidVersion(\\n                    \\'until is not supported for API version < 1.35\\'\\n                )\\n            if isinstance(until, datetime):\\n                params[\\'until\\'] = utils.datetime_to_timestamp(until)\\n            elif (isinstance(until, int) and until > 0):\\n                params[\\'until\\'] = until\\n            else:\\n                raise errors.InvalidArgument(\\n                    \\'until value should be datetime or positive int, \\'\\n                    \\'not {}\\'.format(type(until))\\n                )\\n\\n        url = self._url(\"/containers/{0}/logs\", container)\\n        res = self._get(url, params=params, stream=stream)\\n        output = self._get_result(container, stream, res)\\n\\n        if stream:\\n            return CancellableStream(output, res)\\n        else:\\n            return output', 'def pause(self, container):\\n        \"\"\"\\n        Pauses all processes within a container.\\n\\n        Args:\\n            container (str): The container to pause\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        url = self._url(\\'/containers/{0}/pause\\', container)\\n        res = self._post(url)\\n        self._raise_for_status(res)', 'def port(self, container, private_port):\\n        \"\"\"\\n        Lookup the public-facing port that is NAT-ed to ``private_port``.\\n        Identical to the ``docker port`` command.\\n\\n        Args:\\n            container (str): The container to look up\\n            private_port (int): The private port to inspect\\n\\n        Returns:\\n            (list of dict): The mapping for the host ports\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Example:\\n            .. code-block:: bash\\n\\n                $ docker run -d -p 80:80 ubuntu:14.04 /bin/sleep 30\\n                7174d6347063a83f412fad6124c99cffd25ffe1a0807eb4b7f9cec76ac8cb43b\\n\\n            .. code-block:: python\\n\\n                >>> cli.port(\\'7174d6347063\\', 80)\\n                [{\\'HostIp\\': \\'0.0.0.0\\', \\'HostPort\\': \\'80\\'}]\\n        \"\"\"\\n        res = self._get(self._url(\"/containers/{0}/json\", container))\\n        self._raise_for_status(res)\\n        json_ = res.json()\\n        private_port = str(private_port)\\n        h_ports = None\\n\\n        # Port settings is None when the container is running with\\n        # network_mode=host.\\n        port_settings = json_.get(\\'NetworkSettings\\', {}).get(\\'Ports\\')\\n        if port_settings is None:\\n            return None\\n\\n        if \\'/\\' in private_port:\\n            return port_settings.get(private_port)\\n\\n        for protocol in [\\'tcp\\', \\'udp\\', \\'sctp\\']:\\n            h_ports = port_settings.get(private_port + \\'/\\' + protocol)\\n            if h_ports:\\n                break\\n\\n        return h_ports', 'def put_archive(self, container, path, data):\\n        \"\"\"\\n        Insert a file or folder in an existing container using a tar archive as\\n        source.\\n\\n        Args:\\n            container (str): The container where the file(s) will be extracted\\n            path (str): Path inside the container where the file(s) will be\\n                extracted. Must exist.\\n            data (bytes): tar data to be extracted\\n\\n        Returns:\\n            (bool): True if the call succeeds.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        params = {\\'path\\': path}\\n        url = self._url(\\'/containers/{0}/archive\\', container)\\n        res = self._put(url, params=params, data=data)\\n        self._raise_for_status(res)\\n        return res.status_code == 200', 'def remove_container(self, container, v=False, link=False, force=False):\\n        \"\"\"\\n        Remove a container. Similar to the ``docker rm`` command.\\n\\n        Args:\\n            container (str): The container to remove\\n            v (bool): Remove the volumes associated with the container\\n            link (bool): Remove the specified link and not the underlying\\n                container\\n            force (bool): Force the removal of a running container (uses\\n                ``SIGKILL``)\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        params = {\\'v\\': v, \\'link\\': link, \\'force\\': force}\\n        res = self._delete(\\n            self._url(\"/containers/{0}\", container), params=params\\n        )\\n        self._raise_for_status(res)', 'def rename(self, container, name):\\n        \"\"\"\\n        Rename a container. Similar to the ``docker rename`` command.\\n\\n        Args:\\n            container (str): ID of the container to rename\\n            name (str): New name for the container\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        url = self._url(\"/containers/{0}/rename\", container)\\n        params = {\\'name\\': name}\\n        res = self._post(url, params=params)\\n        self._raise_for_status(res)', 'def resize(self, container, height, width):\\n        \"\"\"\\n        Resize the tty session.\\n\\n        Args:\\n            container (str or dict): The container to resize\\n            height (int): Height of tty session\\n            width (int): Width of tty session\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        params = {\\'h\\': height, \\'w\\': width}\\n        url = self._url(\"/containers/{0}/resize\", container)\\n        res = self._post(url, params=params)\\n        self._raise_for_status(res)', 'def restart(self, container, timeout=10):\\n        \"\"\"\\n        Restart a container. Similar to the ``docker restart`` command.\\n\\n        Args:\\n            container (str or dict): The container to restart. If a dict, the\\n                ``Id`` key is used.\\n            timeout (int): Number of seconds to try to stop for before killing\\n                the container. Once killed it will then be restarted. Default\\n                is 10 seconds.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        params = {\\'t\\': timeout}\\n        url = self._url(\"/containers/{0}/restart\", container)\\n        conn_timeout = self.timeout\\n        if conn_timeout is not None:\\n            conn_timeout += timeout\\n        res = self._post(url, params=params, timeout=conn_timeout)\\n        self._raise_for_status(res)', 'def start(self, container, *args, **kwargs):\\n        \"\"\"\\n        Start a container. Similar to the ``docker start`` command, but\\n        doesn\\'t support attach options.\\n\\n        **Deprecation warning:** Passing configuration options in ``start`` is\\n        no longer supported. Users are expected to provide host config options\\n        in the ``host_config`` parameter of\\n        :py:meth:`~ContainerApiMixin.create_container`.\\n\\n\\n        Args:\\n            container (str): The container to start\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n            :py:class:`docker.errors.DeprecatedMethod`\\n                If any argument besides ``container`` are provided.\\n\\n        Example:\\n\\n            >>> container = cli.create_container(\\n            ...     image=\\'busybox:latest\\',\\n            ...     command=\\'/bin/sleep 30\\')\\n            >>> cli.start(container=container.get(\\'Id\\'))\\n        \"\"\"\\n        if args or kwargs:\\n            raise errors.DeprecatedMethod(\\n                \\'Providing configuration in the start() method is no longer \\'\\n                \\'supported. Use the host_config param in create_container \\'\\n                \\'instead.\\'\\n            )\\n        url = self._url(\"/containers/{0}/start\", container)\\n        res = self._post(url)\\n        self._raise_for_status(res)', 'def stats(self, container, decode=None, stream=True):\\n        \"\"\"\\n        Stream statistics for a specific container. Similar to the\\n        ``docker stats`` command.\\n\\n        Args:\\n            container (str): The container to stream statistics from\\n            decode (bool): If set to true, stream will be decoded into dicts\\n                on the fly. Only applicable if ``stream`` is True.\\n                False by default.\\n            stream (bool): If set to false, only the current stats will be\\n                returned instead of a stream. True by default.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        \"\"\"\\n        url = self._url(\"/containers/{0}/stats\", container)\\n        if stream:\\n            return self._stream_helper(self._get(url, stream=True),\\n                                       decode=decode)\\n        else:\\n            if decode:\\n                raise errors.InvalidArgument(\\n                    \"decode is only available in conjuction with stream=True\"\\n                )\\n            return self._result(self._get(url, params={\\'stream\\': False}),\\n                                json=True)', 'def top(self, container, ps_args=None):\\n        \"\"\"\\n        Display the running processes of a container.\\n\\n        Args:\\n            container (str): The container to inspect\\n            ps_args (str): An optional arguments passed to ps (e.g. ``aux``)\\n\\n        Returns:\\n            (str): The output of the top\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        u = self._url(\"/containers/{0}/top\", container)\\n        params = {}\\n        if ps_args is not None:\\n            params[\\'ps_args\\'] = ps_args\\n        return self._result(self._get(u, params=params), True)', 'def unpause(self, container):\\n        \"\"\"\\n        Unpause all processes within a container.\\n\\n        Args:\\n            container (str): The container to unpause\\n        \"\"\"\\n        url = self._url(\\'/containers/{0}/unpause\\', container)\\n        res = self._post(url)\\n        self._raise_for_status(res)', 'def update_container(\\n        self, container, blkio_weight=None, cpu_period=None, cpu_quota=None,\\n        cpu_shares=None, cpuset_cpus=None, cpuset_mems=None, mem_limit=None,\\n        mem_reservation=None, memswap_limit=None, kernel_memory=None,\\n        restart_policy=None\\n    ):\\n        \"\"\"\\n        Update resource configs of one or more containers.\\n\\n        Args:\\n            container (str): The container to inspect\\n            blkio_weight (int): Block IO (relative weight), between 10 and 1000\\n            cpu_period (int): Limit CPU CFS (Completely Fair Scheduler) period\\n            cpu_quota (int): Limit CPU CFS (Completely Fair Scheduler) quota\\n            cpu_shares (int): CPU shares (relative weight)\\n            cpuset_cpus (str): CPUs in which to allow execution\\n            cpuset_mems (str): MEMs in which to allow execution\\n            mem_limit (int or str): Memory limit\\n            mem_reservation (int or str): Memory soft limit\\n            memswap_limit (int or str): Total memory (memory + swap), -1 to\\n                disable swap\\n            kernel_memory (int or str): Kernel memory limit\\n            restart_policy (dict): Restart policy dictionary\\n\\n        Returns:\\n            (dict): Dictionary containing a ``Warnings`` key.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        url = self._url(\\'/containers/{0}/update\\', container)\\n        data = {}\\n        if blkio_weight:\\n            data[\\'BlkioWeight\\'] = blkio_weight\\n        if cpu_period:\\n            data[\\'CpuPeriod\\'] = cpu_period\\n        if cpu_shares:\\n            data[\\'CpuShares\\'] = cpu_shares\\n        if cpu_quota:\\n            data[\\'CpuQuota\\'] = cpu_quota\\n        if cpuset_cpus:\\n            data[\\'CpusetCpus\\'] = cpuset_cpus\\n        if cpuset_mems:\\n            data[\\'CpusetMems\\'] = cpuset_mems\\n        if mem_limit:\\n            data[\\'Memory\\'] = utils.parse_bytes(mem_limit)\\n        if mem_reservation:\\n            data[\\'MemoryReservation\\'] = utils.parse_bytes(mem_reservation)\\n        if memswap_limit:\\n            data[\\'MemorySwap\\'] = utils.parse_bytes(memswap_limit)\\n        if kernel_memory:\\n            data[\\'KernelMemory\\'] = utils.parse_bytes(kernel_memory)\\n        if restart_policy:\\n            if utils.version_lt(self._version, \\'1.23\\'):\\n                raise errors.InvalidVersion(\\n                    \\'restart policy update is not supported \\'\\n                    \\'for API version < 1.23\\'\\n                )\\n            data[\\'RestartPolicy\\'] = restart_policy\\n\\n        res = self._post_json(url, data=data)\\n        return self._result(res, True)', 'def wait(self, container, timeout=None, condition=None):\\n        \"\"\"\\n        Block until a container stops, then return its exit code. Similar to\\n        the ``docker wait`` command.\\n\\n        Args:\\n            container (str or dict): The container to wait on. If a dict, the\\n                ``Id`` key is used.\\n            timeout (int): Request timeout\\n            condition (str): Wait until a container state reaches the given\\n                condition, either ``not-running`` (default), ``next-exit``,\\n                or ``removed``\\n\\n        Returns:\\n            (dict): The API\\'s response as a Python dictionary, including\\n                the container\\'s exit code under the ``StatusCode`` attribute.\\n\\n        Raises:\\n            :py:class:`requests.exceptions.ReadTimeout`\\n                If the timeout is exceeded.\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        url = self._url(\"/containers/{0}/wait\", container)\\n        params = {}\\n        if condition is not None:\\n            if utils.version_lt(self._version, \\'1.30\\'):\\n                raise errors.InvalidVersion(\\n                    \\'wait condition is not supported for API version < 1.30\\'\\n                )\\n            params[\\'condition\\'] = condition\\n\\n        res = self._post(url, timeout=timeout, params=params)\\n        return self._result(res, True)', 'def tags(self):\\n        \"\"\"\\n        The image\\'s tags.\\n        \"\"\"\\n        tags = self.attrs.get(\\'RepoTags\\')\\n        if tags is None:\\n            tags = []\\n        return [tag for tag in tags if tag != \\'<none>:<none>\\']', 'def save(self, chunk_size=DEFAULT_DATA_CHUNK_SIZE, named=False):\\n        \"\"\"\\n        Get a tarball of an image. Similar to the ``docker save`` command.\\n\\n        Args:\\n            chunk_size (int): The generator will return up to that much data\\n                per iteration, but may return less. If ``None``, data will be\\n                streamed as it is received. Default: 2 MB\\n            named (str or bool): If ``False`` (default), the tarball will not\\n                retain repository and tag information for this image. If set\\n                to ``True``, the first tag in the :py:attr:`~tags` list will\\n                be used to identify the image. Alternatively, any element of\\n                the :py:attr:`~tags` list can be used as an argument to use\\n                that specific tag as the saved identifier.\\n\\n        Returns:\\n            (generator): A stream of raw archive data.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Example:\\n\\n            >>> image = cli.get_image(\"busybox:latest\")\\n            >>> f = open(\\'/tmp/busybox-latest.tar\\', \\'wb\\')\\n            >>> for chunk in image:\\n            >>>   f.write(chunk)\\n            >>> f.close()\\n        \"\"\"\\n        img = self.id\\n        if named:\\n            img = self.tags[0] if self.tags else img\\n            if isinstance(named, six.string_types):\\n                if named not in self.tags:\\n                    raise InvalidArgument(\\n                        \"{} is not a valid tag for this image\".format(named)\\n                    )\\n                img = named\\n\\n        return self.client.api.get_image(img, chunk_size)', 'def tag(self, repository, tag=None, **kwargs):\\n        \"\"\"\\n        Tag this image into a repository. Similar to the ``docker tag``\\n        command.\\n\\n        Args:\\n            repository (str): The repository to set for the tag\\n            tag (str): The tag name\\n            force (bool): Force\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Returns:\\n            (bool): ``True`` if successful\\n        \"\"\"\\n        return self.client.api.tag(self.id, repository, tag=tag, **kwargs)', 'def pull(self, platform=None):\\n        \"\"\"\\n        Pull the image digest.\\n\\n        Args:\\n            platform (str): The platform to pull the image for.\\n            Default: ``None``\\n\\n        Returns:\\n            (:py:class:`Image`): A reference to the pulled image.\\n        \"\"\"\\n        repository, _ = parse_repository_tag(self.image_name)\\n        return self.collection.pull(repository, tag=self.id, platform=platform)', 'def has_platform(self, platform):\\n        \"\"\"\\n        Check whether the given platform identifier is available for this\\n        digest.\\n\\n        Args:\\n            platform (str or dict): A string using the ``os[/arch[/variant]]``\\n                format, or a platform dictionary.\\n\\n        Returns:\\n            (bool): ``True`` if the platform is recognized as available,\\n            ``False`` otherwise.\\n\\n        Raises:\\n            :py:class:`docker.errors.InvalidArgument`\\n                If the platform argument is not a valid descriptor.\\n        \"\"\"\\n        if platform and not isinstance(platform, dict):\\n            parts = platform.split(\\'/\\')\\n            if len(parts) > 3 or len(parts) < 1:\\n                raise InvalidArgument(\\n                    \\'\"{0}\" is not a valid platform descriptor\\'.format(platform)\\n                )\\n            platform = {\\'os\\': parts[0]}\\n            if len(parts) > 2:\\n                platform[\\'variant\\'] = parts[2]\\n            if len(parts) > 1:\\n                platform[\\'architecture\\'] = parts[1]\\n        return normalize_platform(\\n            platform, self.client.version()\\n        ) in self.attrs[\\'Platforms\\']', 'def build(self, **kwargs):\\n        \"\"\"\\n        Build an image and return it. Similar to the ``docker build``\\n        command. Either ``path`` or ``fileobj`` must be set.\\n\\n        If you have a tar file for the Docker build context (including a\\n        Dockerfile) already, pass a readable file-like object to ``fileobj``\\n        and also pass ``custom_context=True``. If the stream is compressed\\n        also, set ``encoding`` to the correct value (e.g ``gzip``).\\n\\n        If you want to get the raw output of the build, use the\\n        :py:meth:`~docker.api.build.BuildApiMixin.build` method in the\\n        low-level API.\\n\\n        Args:\\n            path (str): Path to the directory containing the Dockerfile\\n            fileobj: A file object to use as the Dockerfile. (Or a file-like\\n                object)\\n            tag (str): A tag to add to the final image\\n            quiet (bool): Whether to return the status\\n            nocache (bool): Don\\'t use the cache when set to ``True``\\n            rm (bool): Remove intermediate containers. The ``docker build``\\n                command now defaults to ``--rm=true``, but we have kept the old\\n                default of `False` to preserve backward compatibility\\n            timeout (int): HTTP timeout\\n            custom_context (bool): Optional if using ``fileobj``\\n            encoding (str): The encoding for a stream. Set to ``gzip`` for\\n                compressing\\n            pull (bool): Downloads any updates to the FROM image in Dockerfiles\\n            forcerm (bool): Always remove intermediate containers, even after\\n                unsuccessful builds\\n            dockerfile (str): path within the build context to the Dockerfile\\n            buildargs (dict): A dictionary of build arguments\\n            container_limits (dict): A dictionary of limits applied to each\\n                container created by the build process. Valid keys:\\n\\n                - memory (int): set memory limit for build\\n                - memswap (int): Total memory (memory + swap), -1 to disable\\n                    swap\\n                - cpushares (int): CPU shares (relative weight)\\n                - cpusetcpus (str): CPUs in which to allow execution, e.g.,\\n                    ``\"0-3\"``, ``\"0,1\"``\\n            shmsize (int): Size of `/dev/shm` in bytes. The size must be\\n                greater than 0. If omitted the system uses 64MB\\n            labels (dict): A dictionary of labels to set on the image\\n            cache_from (list): A list of images used for build cache\\n                resolution\\n            target (str): Name of the build-stage to build in a multi-stage\\n                Dockerfile\\n            network_mode (str): networking mode for the run commands during\\n                build\\n            squash (bool): Squash the resulting images layers into a\\n                single layer.\\n            extra_hosts (dict): Extra hosts to add to /etc/hosts in building\\n                containers, as a mapping of hostname to IP address.\\n            platform (str): Platform in the format ``os[/arch[/variant]]``.\\n            isolation (str): Isolation technology used during build.\\n                Default: `None`.\\n            use_config_proxy (bool): If ``True``, and if the docker client\\n                configuration file (``~/.docker/config.json`` by default)\\n                contains a proxy configuration, the corresponding environment\\n                variables will be set in the container being built.\\n\\n        Returns:\\n            (tuple): The first item is the :py:class:`Image` object for the\\n                image that was build. The second item is a generator of the\\n                build logs as JSON-decoded objects.\\n\\n        Raises:\\n            :py:class:`docker.errors.BuildError`\\n                If there is an error during the build.\\n            :py:class:`docker.errors.APIError`\\n                If the server returns any other error.\\n            ``TypeError``\\n                If neither ``path`` nor ``fileobj`` is specified.\\n        \"\"\"\\n        resp = self.client.api.build(**kwargs)\\n        if isinstance(resp, six.string_types):\\n            return self.get(resp)\\n        last_event = None\\n        image_id = None\\n        result_stream, internal_stream = itertools.tee(json_stream(resp))\\n        for chunk in internal_stream:\\n            if \\'error\\' in chunk:\\n                raise BuildError(chunk[\\'error\\'], result_stream)\\n            if \\'stream\\' in chunk:\\n                match = re.search(\\n                    r\\'(^Successfully built |sha256:)([0-9a-f]+)$\\',\\n                    chunk[\\'stream\\']\\n                )\\n                if match:\\n                    image_id = match.group(2)\\n            last_event = chunk\\n        if image_id:\\n            return (self.get(image_id), result_stream)\\n        raise BuildError(last_event or \\'Unknown\\', result_stream)', 'def get(self, name):\\n        \"\"\"\\n        Gets an image.\\n\\n        Args:\\n            name (str): The name of the image.\\n\\n        Returns:\\n            (:py:class:`Image`): The image.\\n\\n        Raises:\\n            :py:class:`docker.errors.ImageNotFound`\\n                If the image does not exist.\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.prepare_model(self.client.api.inspect_image(name))', 'def get_registry_data(self, name, auth_config=None):\\n        \"\"\"\\n        Gets the registry data for an image.\\n\\n        Args:\\n            name (str): The name of the image.\\n            auth_config (dict): Override the credentials that are found in the\\n                config for this request.  ``auth_config`` should contain the\\n                ``username`` and ``password`` keys to be valid.\\n\\n        Returns:\\n            (:py:class:`RegistryData`): The data object.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return RegistryData(\\n            image_name=name,\\n            attrs=self.client.api.inspect_distribution(name, auth_config),\\n            client=self.client,\\n            collection=self,\\n        )', 'def list(self, name=None, all=False, filters=None):\\n        \"\"\"\\n        List images on the server.\\n\\n        Args:\\n            name (str): Only show images belonging to the repository ``name``\\n            all (bool): Show intermediate image layers. By default, these are\\n                filtered out.\\n            filters (dict): Filters to be processed on the image list.\\n                Available filters:\\n                - ``dangling`` (bool)\\n                - ``label`` (str): format either ``key`` or ``key=value``\\n\\n        Returns:\\n            (list of :py:class:`Image`): The images.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        resp = self.client.api.images(name=name, all=all, filters=filters)\\n        return [self.get(r[\"Id\"]) for r in resp]', 'def load(self, data):\\n        \"\"\"\\n        Load an image that was previously saved using\\n        :py:meth:`~docker.models.images.Image.save` (or ``docker save``).\\n        Similar to ``docker load``.\\n\\n        Args:\\n            data (binary): Image data to be loaded.\\n\\n        Returns:\\n            (list of :py:class:`Image`): The images.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        resp = self.client.api.load_image(data)\\n        images = []\\n        for chunk in resp:\\n            if \\'stream\\' in chunk:\\n                match = re.search(\\n                    r\\'(^Loaded image ID: |^Loaded image: )(.+)$\\',\\n                    chunk[\\'stream\\']\\n                )\\n                if match:\\n                    image_id = match.group(2)\\n                    images.append(image_id)\\n            if \\'error\\' in chunk:\\n                raise ImageLoadError(chunk[\\'error\\'])\\n\\n        return [self.get(i) for i in images]', 'def pull(self, repository, tag=None, **kwargs):\\n        \"\"\"\\n        Pull an image of the given name and return it. Similar to the\\n        ``docker pull`` command.\\n        If no tag is specified, all tags from that repository will be\\n        pulled.\\n\\n        If you want to get the raw pull output, use the\\n        :py:meth:`~docker.api.image.ImageApiMixin.pull` method in the\\n        low-level API.\\n\\n        Args:\\n            repository (str): The repository to pull\\n            tag (str): The tag to pull\\n            auth_config (dict): Override the credentials that are found in the\\n                config for this request.  ``auth_config`` should contain the\\n                ``username`` and ``password`` keys to be valid.\\n            platform (str): Platform in the format ``os[/arch[/variant]]``\\n\\n        Returns:\\n            (:py:class:`Image` or list): The image that has been pulled.\\n                If no ``tag`` was specified, the method will return a list\\n                of :py:class:`Image` objects belonging to this repository.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Example:\\n\\n            >>> # Pull the image tagged `latest` in the busybox repo\\n            >>> image = client.images.pull(\\'busybox:latest\\')\\n\\n            >>> # Pull all tags in the busybox repo\\n            >>> images = client.images.pull(\\'busybox\\')\\n        \"\"\"\\n        if not tag:\\n            repository, tag = parse_repository_tag(repository)\\n\\n        if \\'stream\\' in kwargs:\\n            warnings.warn(\\n                \\'`stream` is not a valid parameter for this method\\'\\n                \\' and will be overridden\\'\\n            )\\n            del kwargs[\\'stream\\']\\n\\n        pull_log = self.client.api.pull(\\n            repository, tag=tag, stream=True, **kwargs\\n        )\\n        for _ in pull_log:\\n            # We don\\'t do anything with the logs, but we need\\n            # to keep the connection alive and wait for the image\\n            # to be pulled.\\n            pass\\n        if tag:\\n            return self.get(\\'{0}{2}{1}\\'.format(\\n                repository, tag, \\'@\\' if tag.startswith(\\'sha256:\\') else \\':\\'\\n            ))\\n        return self.list(repository)', 'def _create_container_args(kwargs):\\n    \"\"\"\\n    Convert arguments to create() to arguments to create_container().\\n    \"\"\"\\n    # Copy over kwargs which can be copied directly\\n    create_kwargs = {}\\n    for key in copy.copy(kwargs):\\n        if key in RUN_CREATE_KWARGS:\\n            create_kwargs[key] = kwargs.pop(key)\\n    host_config_kwargs = {}\\n    for key in copy.copy(kwargs):\\n        if key in RUN_HOST_CONFIG_KWARGS:\\n            host_config_kwargs[key] = kwargs.pop(key)\\n\\n    # Process kwargs which are split over both create and host_config\\n    ports = kwargs.pop(\\'ports\\', {})\\n    if ports:\\n        host_config_kwargs[\\'port_bindings\\'] = ports\\n\\n    volumes = kwargs.pop(\\'volumes\\', {})\\n    if volumes:\\n        host_config_kwargs[\\'binds\\'] = volumes\\n\\n    network = kwargs.pop(\\'network\\', None)\\n    if network:\\n        create_kwargs[\\'networking_config\\'] = {network: None}\\n        host_config_kwargs[\\'network_mode\\'] = network\\n\\n    # All kwargs should have been consumed by this point, so raise\\n    # error if any are left\\n    if kwargs:\\n        raise create_unexpected_kwargs_error(\\'run\\', kwargs)\\n\\n    create_kwargs[\\'host_config\\'] = HostConfig(**host_config_kwargs)\\n\\n    # Fill in any kwargs which need processing by create_host_config first\\n    port_bindings = create_kwargs[\\'host_config\\'].get(\\'PortBindings\\')\\n    if port_bindings:\\n        # sort to make consistent for tests\\n        create_kwargs[\\'ports\\'] = [tuple(p.split(\\'/\\', 1))\\n                                  for p in sorted(port_bindings.keys())]\\n    if volumes:\\n        if isinstance(volumes, dict):\\n            create_kwargs[\\'volumes\\'] = [\\n                v.get(\\'bind\\') for v in volumes.values()\\n            ]\\n        else:\\n            create_kwargs[\\'volumes\\'] = [\\n                _host_volume_from_bind(v) for v in volumes\\n            ]\\n    return create_kwargs', 'def image(self):\\n        \"\"\"\\n        The image of the container.\\n        \"\"\"\\n        image_id = self.attrs.get(\\'ImageID\\', self.attrs[\\'Image\\'])\\n        if image_id is None:\\n            return None\\n        return self.client.images.get(image_id.split(\\':\\')[1])', 'def status(self):\\n        \"\"\"\\n        The status of the container. For example, ``running``, or ``exited``.\\n        \"\"\"\\n        if isinstance(self.attrs[\\'State\\'], dict):\\n            return self.attrs[\\'State\\'][\\'Status\\']\\n        return self.attrs[\\'State\\']', 'def attach(self, **kwargs):\\n        \"\"\"\\n        Attach to this container.\\n\\n        :py:meth:`logs` is a wrapper around this method, which you can\\n        use instead if you want to fetch/stream container output without first\\n        retrieving the entire backlog.\\n\\n        Args:\\n            stdout (bool): Include stdout.\\n            stderr (bool): Include stderr.\\n            stream (bool): Return container output progressively as an iterator\\n                of strings, rather than a single string.\\n            logs (bool): Include the container\\'s previous output.\\n\\n        Returns:\\n            By default, the container\\'s output as a single string.\\n\\n            If ``stream=True``, an iterator of output strings.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.attach(self.id, **kwargs)', 'def attach_socket(self, **kwargs):\\n        \"\"\"\\n        Like :py:meth:`attach`, but returns the underlying socket-like object\\n        for the HTTP request.\\n\\n        Args:\\n            params (dict): Dictionary of request parameters (e.g. ``stdout``,\\n                ``stderr``, ``stream``).\\n            ws (bool): Use websockets instead of raw HTTP.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.attach_socket(self.id, **kwargs)', 'def commit(self, repository=None, tag=None, **kwargs):\\n        \"\"\"\\n        Commit a container to an image. Similar to the ``docker commit``\\n        command.\\n\\n        Args:\\n            repository (str): The repository to push the image to\\n            tag (str): The tag to push\\n            message (str): A commit message\\n            author (str): The name of the author\\n            changes (str): Dockerfile instructions to apply while committing\\n            conf (dict): The configuration for the container. See the\\n                `Engine API documentation\\n                <https://docs.docker.com/reference/api/docker_remote_api/>`_\\n                for full details.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n\\n        resp = self.client.api.commit(self.id, repository=repository, tag=tag,\\n                                      **kwargs)\\n        return self.client.images.get(resp[\\'Id\\'])', 'def exec_run(self, cmd, stdout=True, stderr=True, stdin=False, tty=False,\\n                 privileged=False, user=\\'\\', detach=False, stream=False,\\n                 socket=False, environment=None, workdir=None, demux=False):\\n        \"\"\"\\n        Run a command inside this container. Similar to\\n        ``docker exec``.\\n\\n        Args:\\n            cmd (str or list): Command to be executed\\n            stdout (bool): Attach to stdout. Default: ``True``\\n            stderr (bool): Attach to stderr. Default: ``True``\\n            stdin (bool): Attach to stdin. Default: ``False``\\n            tty (bool): Allocate a pseudo-TTY. Default: False\\n            privileged (bool): Run as privileged.\\n            user (str): User to execute command as. Default: root\\n            detach (bool): If true, detach from the exec command.\\n                Default: False\\n            stream (bool): Stream response data. Default: False\\n            socket (bool): Return the connection socket to allow custom\\n                read/write operations. Default: False\\n            environment (dict or list): A dictionary or a list of strings in\\n                the following format ``[\"PASSWORD=xxx\"]`` or\\n                ``{\"PASSWORD\": \"xxx\"}``.\\n            workdir (str): Path to working directory for this exec session\\n            demux (bool): Return stdout and stderr separately\\n\\n        Returns:\\n            (ExecResult): A tuple of (exit_code, output)\\n                exit_code: (int):\\n                    Exit code for the executed command or ``None`` if\\n                    either ``stream`` or ``socket`` is ``True``.\\n                output: (generator, bytes, or tuple):\\n                    If ``stream=True``, a generator yielding response chunks.\\n                    If ``socket=True``, a socket object for the connection.\\n                    If ``demux=True``, a tuple of two bytes: stdout and stderr.\\n                    A bytestring containing response data otherwise.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        resp = self.client.api.exec_create(\\n            self.id, cmd, stdout=stdout, stderr=stderr, stdin=stdin, tty=tty,\\n            privileged=privileged, user=user, environment=environment,\\n            workdir=workdir,\\n        )\\n        exec_output = self.client.api.exec_start(\\n            resp[\\'Id\\'], detach=detach, tty=tty, stream=stream, socket=socket,\\n            demux=demux\\n        )\\n        if socket or stream:\\n            return ExecResult(None, exec_output)\\n\\n        return ExecResult(\\n            self.client.api.exec_inspect(resp[\\'Id\\'])[\\'ExitCode\\'],\\n            exec_output\\n        )', 'def export(self, chunk_size=DEFAULT_DATA_CHUNK_SIZE):\\n        \"\"\"\\n        Export the contents of the container\\'s filesystem as a tar archive.\\n\\n        Args:\\n            chunk_size (int): The number of bytes returned by each iteration\\n                of the generator. If ``None``, data will be streamed as it is\\n                received. Default: 2 MB\\n\\n        Returns:\\n            (str): The filesystem tar archive\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.export(self.id, chunk_size)', 'def get_archive(self, path, chunk_size=DEFAULT_DATA_CHUNK_SIZE):\\n        \"\"\"\\n        Retrieve a file or folder from the container in the form of a tar\\n        archive.\\n\\n        Args:\\n            path (str): Path to the file or folder to retrieve\\n            chunk_size (int): The number of bytes returned by each iteration\\n                of the generator. If ``None``, data will be streamed as it is\\n                received. Default: 2 MB\\n\\n        Returns:\\n            (tuple): First element is a raw tar data stream. Second element is\\n            a dict containing ``stat`` information on the specified ``path``.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Example:\\n\\n            >>> f = open(\\'./sh_bin.tar\\', \\'wb\\')\\n            >>> bits, stat = container.get_archive(\\'/bin/sh\\')\\n            >>> print(stat)\\n            {\\'name\\': \\'sh\\', \\'size\\': 1075464, \\'mode\\': 493,\\n             \\'mtime\\': \\'2018-10-01T15:37:48-07:00\\', \\'linkTarget\\': \\'\\'}\\n            >>> for chunk in bits:\\n            ...    f.write(chunk)\\n            >>> f.close()\\n        \"\"\"\\n        return self.client.api.get_archive(self.id, path, chunk_size)', 'def kill(self, signal=None):\\n        \"\"\"\\n        Kill or send a signal to the container.\\n\\n        Args:\\n            signal (str or int): The signal to send. Defaults to ``SIGKILL``\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n\\n        return self.client.api.kill(self.id, signal=signal)', 'def logs(self, **kwargs):\\n        \"\"\"\\n        Get logs from this container. Similar to the ``docker logs`` command.\\n\\n        The ``stream`` parameter makes the ``logs`` function return a blocking\\n        generator you can iterate over to retrieve log output as it happens.\\n\\n        Args:\\n            stdout (bool): Get ``STDOUT``. Default ``True``\\n            stderr (bool): Get ``STDERR``. Default ``True``\\n            stream (bool): Stream the response. Default ``False``\\n            timestamps (bool): Show timestamps. Default ``False``\\n            tail (str or int): Output specified number of lines at the end of\\n                logs. Either an integer of number of lines or the string\\n                ``all``. Default ``all``\\n            since (datetime or int): Show logs since a given datetime or\\n                integer epoch (in seconds)\\n            follow (bool): Follow log output. Default ``False``\\n            until (datetime or int): Show logs that occurred before the given\\n                datetime or integer epoch (in seconds)\\n\\n        Returns:\\n            (generator or str): Logs from the container.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.logs(self.id, **kwargs)', 'def put_archive(self, path, data):\\n        \"\"\"\\n        Insert a file or folder in this container using a tar archive as\\n        source.\\n\\n        Args:\\n            path (str): Path inside the container where the file(s) will be\\n                extracted. Must exist.\\n            data (bytes): tar data to be extracted\\n\\n        Returns:\\n            (bool): True if the call succeeds.\\n\\n        Raises:\\n            :py:class:`~docker.errors.APIError` If an error occurs.\\n        \"\"\"\\n        return self.client.api.put_archive(self.id, path, data)', 'def remove(self, **kwargs):\\n        \"\"\"\\n        Remove this container. Similar to the ``docker rm`` command.\\n\\n        Args:\\n            v (bool): Remove the volumes associated with the container\\n            link (bool): Remove the specified link and not the underlying\\n                container\\n            force (bool): Force the removal of a running container (uses\\n                ``SIGKILL``)\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.remove_container(self.id, **kwargs)', 'def rename(self, name):\\n        \"\"\"\\n        Rename this container. Similar to the ``docker rename`` command.\\n\\n        Args:\\n            name (str): New name for the container\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.rename(self.id, name)', 'def resize(self, height, width):\\n        \"\"\"\\n        Resize the tty session.\\n\\n        Args:\\n            height (int): Height of tty session\\n            width (int): Width of tty session\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.resize(self.id, height, width)', 'def restart(self, **kwargs):\\n        \"\"\"\\n        Restart this container. Similar to the ``docker restart`` command.\\n\\n        Args:\\n            timeout (int): Number of seconds to try to stop for before killing\\n                the container. Once killed it will then be restarted. Default\\n                is 10 seconds.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.restart(self.id, **kwargs)', 'def start(self, **kwargs):\\n        \"\"\"\\n        Start this container. Similar to the ``docker start`` command, but\\n        doesn\\'t support attach options.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.start(self.id, **kwargs)', 'def stats(self, **kwargs):\\n        \"\"\"\\n        Stream statistics for this container. Similar to the\\n        ``docker stats`` command.\\n\\n        Args:\\n            decode (bool): If set to true, stream will be decoded into dicts\\n                on the fly. Only applicable if ``stream`` is True.\\n                False by default.\\n            stream (bool): If set to false, only the current stats will be\\n                returned instead of a stream. True by default.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.stats(self.id, **kwargs)', 'def stop(self, **kwargs):\\n        \"\"\"\\n        Stops a container. Similar to the ``docker stop`` command.\\n\\n        Args:\\n            timeout (int): Timeout in seconds to wait for the container to\\n                stop before sending a ``SIGKILL``. Default: 10\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.stop(self.id, **kwargs)', 'def top(self, **kwargs):\\n        \"\"\"\\n        Display the running processes of the container.\\n\\n        Args:\\n            ps_args (str): An optional arguments passed to ps (e.g. ``aux``)\\n\\n        Returns:\\n            (str): The output of the top\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.top(self.id, **kwargs)', 'def update(self, **kwargs):\\n        \"\"\"\\n        Update resource configuration of the containers.\\n\\n        Args:\\n            blkio_weight (int): Block IO (relative weight), between 10 and 1000\\n            cpu_period (int): Limit CPU CFS (Completely Fair Scheduler) period\\n            cpu_quota (int): Limit CPU CFS (Completely Fair Scheduler) quota\\n            cpu_shares (int): CPU shares (relative weight)\\n            cpuset_cpus (str): CPUs in which to allow execution\\n            cpuset_mems (str): MEMs in which to allow execution\\n            mem_limit (int or str): Memory limit\\n            mem_reservation (int or str): Memory soft limit\\n            memswap_limit (int or str): Total memory (memory + swap), -1 to\\n                disable swap\\n            kernel_memory (int or str): Kernel memory limit\\n            restart_policy (dict): Restart policy dictionary\\n\\n        Returns:\\n            (dict): Dictionary containing a ``Warnings`` key.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.update_container(self.id, **kwargs)', 'def wait(self, **kwargs):\\n        \"\"\"\\n        Block until the container stops, then return its exit code. Similar to\\n        the ``docker wait`` command.\\n\\n        Args:\\n            timeout (int): Request timeout\\n            condition (str): Wait until a container state reaches the given\\n                condition, either ``not-running`` (default), ``next-exit``,\\n                or ``removed``\\n\\n        Returns:\\n            (dict): The API\\'s response as a Python dictionary, including\\n                the container\\'s exit code under the ``StatusCode`` attribute.\\n\\n        Raises:\\n            :py:class:`requests.exceptions.ReadTimeout`\\n                If the timeout is exceeded.\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return self.client.api.wait(self.id, **kwargs)', 'def run(self, image, command=None, stdout=True, stderr=False,\\n            remove=False, **kwargs):\\n        \"\"\"\\n        Run a container. By default, it will wait for the container to finish\\n        and return its logs, similar to ``docker run``.\\n\\n        If the ``detach`` argument is ``True``, it will start the container\\n        and immediately return a :py:class:`Container` object, similar to\\n        ``docker run -d``.\\n\\n        Example:\\n            Run a container and get its output:\\n\\n            >>> import docker\\n            >>> client = docker.from_env()\\n            >>> client.containers.run(\\'alpine\\', \\'echo hello world\\')\\n            b\\'hello world\\\\\\\\n\\'\\n\\n            Run a container and detach:\\n\\n            >>> container = client.containers.run(\\'bfirsh/reticulate-splines\\',\\n                                                  detach=True)\\n            >>> container.logs()\\n            \\'Reticulating spline 1...\\\\\\\\nReticulating spline 2...\\\\\\\\n\\'\\n\\n        Args:\\n            image (str): The image to run.\\n            command (str or list): The command to run in the container.\\n            auto_remove (bool): enable auto-removal of the container on daemon\\n                side when the container\\'s process exits.\\n            blkio_weight_device: Block IO weight (relative device weight) in\\n                the form of: ``[{\"Path\": \"device_path\", \"Weight\": weight}]``.\\n            blkio_weight: Block IO weight (relative weight), accepts a weight\\n                value between 10 and 1000.\\n            cap_add (list of str): Add kernel capabilities. For example,\\n                ``[\"SYS_ADMIN\", \"MKNOD\"]``.\\n            cap_drop (list of str): Drop kernel capabilities.\\n            cgroup_parent (str): Override the default parent cgroup.\\n            cpu_count (int): Number of usable CPUs (Windows only).\\n            cpu_percent (int): Usable percentage of the available CPUs\\n                (Windows only).\\n            cpu_period (int): The length of a CPU period in microseconds.\\n            cpu_quota (int): Microseconds of CPU time that the container can\\n                get in a CPU period.\\n            cpu_rt_period (int): Limit CPU real-time period in microseconds.\\n            cpu_rt_runtime (int): Limit CPU real-time runtime in microseconds.\\n            cpu_shares (int): CPU shares (relative weight).\\n            cpuset_cpus (str): CPUs in which to allow execution (``0-3``,\\n                ``0,1``).\\n            cpuset_mems (str): Memory nodes (MEMs) in which to allow execution\\n                (``0-3``, ``0,1``). Only effective on NUMA systems.\\n            detach (bool): Run container in the background and return a\\n                :py:class:`Container` object.\\n            device_cgroup_rules (:py:class:`list`): A list of cgroup rules to\\n                apply to the container.\\n            device_read_bps: Limit read rate (bytes per second) from a device\\n                in the form of: `[{\"Path\": \"device_path\", \"Rate\": rate}]`\\n            device_read_iops: Limit read rate (IO per second) from a device.\\n            device_write_bps: Limit write rate (bytes per second) from a\\n                device.\\n            device_write_iops: Limit write rate (IO per second) from a device.\\n            devices (:py:class:`list`): Expose host devices to the container,\\n                as a list of strings in the form\\n                ``<path_on_host>:<path_in_container>:<cgroup_permissions>``.\\n\\n                For example, ``/dev/sda:/dev/xvda:rwm`` allows the container\\n                to have read-write access to the host\\'s ``/dev/sda`` via a\\n                node named ``/dev/xvda`` inside the container.\\n            dns (:py:class:`list`): Set custom DNS servers.\\n            dns_opt (:py:class:`list`): Additional options to be added to the\\n                container\\'s ``resolv.conf`` file.\\n            dns_search (:py:class:`list`): DNS search domains.\\n            domainname (str or list): Set custom DNS search domains.\\n            entrypoint (str or list): The entrypoint for the container.\\n            environment (dict or list): Environment variables to set inside\\n                the container, as a dictionary or a list of strings in the\\n                format ``[\"SOMEVARIABLE=xxx\"]``.\\n            extra_hosts (dict): Additional hostnames to resolve inside the\\n                container, as a mapping of hostname to IP address.\\n            group_add (:py:class:`list`): List of additional group names and/or\\n                IDs that the container process will run as.\\n            healthcheck (dict): Specify a test to perform to check that the\\n                container is healthy.\\n            hostname (str): Optional hostname for the container.\\n            init (bool): Run an init inside the container that forwards\\n                signals and reaps processes\\n            init_path (str): Path to the docker-init binary\\n            ipc_mode (str): Set the IPC mode for the container.\\n            isolation (str): Isolation technology to use. Default: `None`.\\n            kernel_memory (int or str): Kernel memory limit\\n            labels (dict or list): A dictionary of name-value labels (e.g.\\n                ``{\"label1\": \"value1\", \"label2\": \"value2\"}``) or a list of\\n                names of labels to set with empty values (e.g.\\n                ``[\"label1\", \"label2\"]``)\\n            links (dict): Mapping of links using the\\n                ``{\\'container\\': \\'alias\\'}`` format. The alias is optional.\\n                Containers declared in this dict will be linked to the new\\n                container using the provided alias. Default: ``None``.\\n            log_config (LogConfig): Logging configuration.\\n            lxc_conf (dict): LXC config.\\n            mac_address (str): MAC address to assign to the container.\\n            mem_limit (int or str): Memory limit. Accepts float values\\n                (which represent the memory limit of the created container in\\n                bytes) or a string with a units identification char\\n                (``100000b``, ``1000k``, ``128m``, ``1g``). If a string is\\n                specified without a units character, bytes are assumed as an\\n                intended unit.\\n            mem_reservation (int or str): Memory soft limit\\n            mem_swappiness (int): Tune a container\\'s memory swappiness\\n                behavior. Accepts number between 0 and 100.\\n            memswap_limit (str or int): Maximum amount of memory + swap a\\n                container is allowed to consume.\\n            mounts (:py:class:`list`): Specification for mounts to be added to\\n                the container. More powerful alternative to ``volumes``. Each\\n                item in the list is expected to be a\\n                :py:class:`docker.types.Mount` object.\\n            name (str): The name for this container.\\n            nano_cpus (int):  CPU quota in units of 1e-9 CPUs.\\n            network (str): Name of the network this container will be connected\\n                to at creation time. You can connect to additional networks\\n                using :py:meth:`Network.connect`. Incompatible with\\n                ``network_mode``.\\n            network_disabled (bool): Disable networking.\\n            network_mode (str): One of:\\n\\n                - ``bridge`` Create a new network stack for the container on\\n                  on the bridge network.\\n                - ``none`` No networking for this container.\\n                - ``container:<name|id>`` Reuse another container\\'s network\\n                  stack.\\n                - ``host`` Use the host network stack.\\n\\n                Incompatible with ``network``.\\n            oom_kill_disable (bool): Whether to disable OOM killer.\\n            oom_score_adj (int): An integer value containing the score given\\n                to the container in order to tune OOM killer preferences.\\n            pid_mode (str): If set to ``host``, use the host PID namespace\\n                inside the container.\\n            pids_limit (int): Tune a container\\'s pids limit. Set ``-1`` for\\n                unlimited.\\n            platform (str): Platform in the format ``os[/arch[/variant]]``.\\n                Only used if the method needs to pull the requested image.\\n            ports (dict): Ports to bind inside the container.\\n\\n                The keys of the dictionary are the ports to bind inside the\\n                container, either as an integer or a string in the form\\n                ``port/protocol``, where the protocol is either ``tcp``,\\n                ``udp``, or ``sctp``.\\n\\n                The values of the dictionary are the corresponding ports to\\n                open on the host, which can be either:\\n\\n                - The port number, as an integer. For example,\\n                  ``{\\'2222/tcp\\': 3333}`` will expose port 2222 inside the\\n                  container as port 3333 on the host.\\n                - ``None``, to assign a random host port. For example,\\n                  ``{\\'2222/tcp\\': None}``.\\n                - A tuple of ``(address, port)`` if you want to specify the\\n                  host interface. For example,\\n                  ``{\\'1111/tcp\\': (\\'127.0.0.1\\', 1111)}``.\\n                - A list of integers, if you want to bind multiple host ports\\n                  to a single container port. For example,\\n                  ``{\\'1111/tcp\\': [1234, 4567]}``.\\n\\n            privileged (bool): Give extended privileges to this container.\\n            publish_all_ports (bool): Publish all ports to the host.\\n            read_only (bool): Mount the container\\'s root filesystem as read\\n                only.\\n            remove (bool): Remove the container when it has finished running.\\n                Default: ``False``.\\n            restart_policy (dict): Restart the container when it exits.\\n                Configured as a dictionary with keys:\\n\\n                - ``Name`` One of ``on-failure``, or ``always``.\\n                - ``MaximumRetryCount`` Number of times to restart the\\n                  container on failure.\\n\\n                For example:\\n                ``{\"Name\": \"on-failure\", \"MaximumRetryCount\": 5}``\\n\\n            runtime (str): Runtime to use with this container.\\n            security_opt (:py:class:`list`): A list of string values to\\n                customize labels for MLS systems, such as SELinux.\\n            shm_size (str or int): Size of /dev/shm (e.g. ``1G``).\\n            stdin_open (bool): Keep ``STDIN`` open even if not attached.\\n            stdout (bool): Return logs from ``STDOUT`` when ``detach=False``.\\n                Default: ``True``.\\n            stderr (bool): Return logs from ``STDERR`` when ``detach=False``.\\n                Default: ``False``.\\n            stop_signal (str): The stop signal to use to stop the container\\n                (e.g. ``SIGINT``).\\n            storage_opt (dict): Storage driver options per container as a\\n                key-value mapping.\\n            stream (bool): If true and ``detach`` is false, return a log\\n                generator instead of a string. Ignored if ``detach`` is true.\\n                Default: ``False``.\\n            sysctls (dict): Kernel parameters to set in the container.\\n            tmpfs (dict): Temporary filesystems to mount, as a dictionary\\n                mapping a path inside the container to options for that path.\\n\\n                For example:\\n\\n                .. code-block:: python\\n\\n                    {\\n                        \\'/mnt/vol2\\': \\'\\',\\n                        \\'/mnt/vol1\\': \\'size=3G,uid=1000\\'\\n                    }\\n\\n            tty (bool): Allocate a pseudo-TTY.\\n            ulimits (:py:class:`list`): Ulimits to set inside the container,\\n                as a list of :py:class:`docker.types.Ulimit` instances.\\n            use_config_proxy (bool): If ``True``, and if the docker client\\n                configuration file (``~/.docker/config.json`` by default)\\n                contains a proxy configuration, the corresponding environment\\n                variables will be set in the container being built.\\n            user (str or int): Username or UID to run commands as inside the\\n                container.\\n            userns_mode (str): Sets the user namespace mode for the container\\n                when user namespace remapping option is enabled. Supported\\n                values are: ``host``\\n            uts_mode (str): Sets the UTS namespace mode for the container.\\n                Supported values are: ``host``\\n            version (str): The version of the API to use. Set to ``auto`` to\\n                automatically detect the server\\'s version. Default: ``1.35``\\n            volume_driver (str): The name of a volume driver/plugin.\\n            volumes (dict or list): A dictionary to configure volumes mounted\\n                inside the container. The key is either the host path or a\\n                volume name, and the value is a dictionary with the keys:\\n\\n                - ``bind`` The path to mount the volume inside the container\\n                - ``mode`` Either ``rw`` to mount the volume read/write, or\\n                  ``ro`` to mount it read-only.\\n\\n                For example:\\n\\n                .. code-block:: python\\n\\n                    {\\'/home/user1/\\': {\\'bind\\': \\'/mnt/vol2\\', \\'mode\\': \\'rw\\'},\\n                     \\'/var/www\\': {\\'bind\\': \\'/mnt/vol1\\', \\'mode\\': \\'ro\\'}}\\n\\n            volumes_from (:py:class:`list`): List of container names or IDs to\\n                get volumes from.\\n            working_dir (str): Path to the working directory.\\n\\n        Returns:\\n            The container logs, either ``STDOUT``, ``STDERR``, or both,\\n            depending on the value of the ``stdout`` and ``stderr`` arguments.\\n\\n            ``STDOUT`` and ``STDERR`` may be read only if either ``json-file``\\n            or ``journald`` logging driver used. Thus, if you are using none of\\n            these drivers, a ``None`` object is returned instead. See the\\n            `Engine API documentation\\n            <https://docs.docker.com/engine/api/v1.30/#operation/ContainerLogs/>`_\\n            for full details.\\n\\n            If ``detach`` is ``True``, a :py:class:`Container` object is\\n            returned instead.\\n\\n        Raises:\\n            :py:class:`docker.errors.ContainerError`\\n                If the container exits with a non-zero exit code and\\n                ``detach`` is ``False``.\\n            :py:class:`docker.errors.ImageNotFound`\\n                If the specified image does not exist.\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        if isinstance(image, Image):\\n            image = image.id\\n        stream = kwargs.pop(\\'stream\\', False)\\n        detach = kwargs.pop(\\'detach\\', False)\\n        platform = kwargs.pop(\\'platform\\', None)\\n\\n        if detach and remove:\\n            if version_gte(self.client.api._version, \\'1.25\\'):\\n                kwargs[\"auto_remove\"] = True\\n            else:\\n                raise RuntimeError(\"The options \\'detach\\' and \\'remove\\' cannot \"\\n                                   \"be used together in api versions < 1.25.\")\\n\\n        if kwargs.get(\\'network\\') and kwargs.get(\\'network_mode\\'):\\n            raise RuntimeError(\\n                \\'The options \"network\" and \"network_mode\" can not be used \\'\\n                \\'together.\\'\\n            )\\n\\n        try:\\n            container = self.create(image=image, command=command,\\n                                    detach=detach, **kwargs)\\n        except ImageNotFound:\\n            self.client.images.pull(image, platform=platform)\\n            container = self.create(image=image, command=command,\\n                                    detach=detach, **kwargs)\\n\\n        container.start()\\n\\n        if detach:\\n            return container\\n\\n        logging_driver = container.attrs[\\'HostConfig\\'][\\'LogConfig\\'][\\'Type\\']\\n\\n        out = None\\n        if logging_driver == \\'json-file\\' or logging_driver == \\'journald\\':\\n            out = container.logs(\\n                stdout=stdout, stderr=stderr, stream=True, follow=True\\n            )\\n\\n        exit_status = container.wait()[\\'StatusCode\\']\\n        if exit_status != 0:\\n            out = None\\n            if not kwargs.get(\\'auto_remove\\'):\\n                out = container.logs(stdout=False, stderr=True)\\n\\n        if remove:\\n            container.remove()\\n        if exit_status != 0:\\n            raise ContainerError(\\n                container, exit_status, command, image, out\\n            )\\n\\n        return out if stream or out is None else b\\'\\'.join(\\n            [line for line in out]\\n        )', 'def create(self, image, command=None, **kwargs):\\n        \"\"\"\\n        Create a container without starting it. Similar to ``docker create``.\\n\\n        Takes the same arguments as :py:meth:`run`, except for ``stdout``,\\n        ``stderr``, and ``remove``.\\n\\n        Returns:\\n            A :py:class:`Container` object.\\n\\n        Raises:\\n            :py:class:`docker.errors.ImageNotFound`\\n                If the specified image does not exist.\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        if isinstance(image, Image):\\n            image = image.id\\n        kwargs[\\'image\\'] = image\\n        kwargs[\\'command\\'] = command\\n        kwargs[\\'version\\'] = self.client.api._version\\n        create_kwargs = _create_container_args(kwargs)\\n        resp = self.client.api.create_container(**create_kwargs)\\n        return self.get(resp[\\'Id\\'])', 'def get(self, container_id):\\n        \"\"\"\\n        Get a container by name or ID.\\n\\n        Args:\\n            container_id (str): Container name or ID.\\n\\n        Returns:\\n            A :py:class:`Container` object.\\n\\n        Raises:\\n            :py:class:`docker.errors.NotFound`\\n                If the container does not exist.\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        resp = self.client.api.inspect_container(container_id)\\n        return self.prepare_model(resp)', 'def list(self, all=False, before=None, filters=None, limit=-1, since=None,\\n             sparse=False, ignore_removed=False):\\n        \"\"\"\\n        List containers. Similar to the ``docker ps`` command.\\n\\n        Args:\\n            all (bool): Show all containers. Only running containers are shown\\n                by default\\n            since (str): Show only containers created since Id or Name, include\\n                non-running ones\\n            before (str): Show only container created before Id or Name,\\n                include non-running ones\\n            limit (int): Show `limit` last created containers, include\\n                non-running ones\\n            filters (dict): Filters to be processed on the image list.\\n                Available filters:\\n\\n                - `exited` (int): Only containers with specified exit code\\n                - `status` (str): One of ``restarting``, ``running``,\\n                    ``paused``, ``exited``\\n                - `label` (str): format either ``\"key\"`` or ``\"key=value\"``\\n                - `id` (str): The id of the container.\\n                - `name` (str): The name of the container.\\n                - `ancestor` (str): Filter by container ancestor. Format of\\n                    ``<image-name>[:tag]``, ``<image-id>``, or\\n                    ``<image@digest>``.\\n                - `before` (str): Only containers created before a particular\\n                    container. Give the container name or id.\\n                - `since` (str): Only containers created after a particular\\n                    container. Give container name or id.\\n\\n                A comprehensive list can be found in the documentation for\\n                `docker ps\\n                <https://docs.docker.com/engine/reference/commandline/ps>`_.\\n\\n            sparse (bool): Do not inspect containers. Returns partial\\n                information, but guaranteed not to block. Use\\n                :py:meth:`Container.reload` on resulting objects to retrieve\\n                all attributes. Default: ``False``\\n            ignore_removed (bool): Ignore failures due to missing containers\\n                when attempting to inspect containers from the original list.\\n                Set to ``True`` if race conditions are likely. Has no effect\\n                if ``sparse=True``. Default: ``False``\\n\\n        Returns:\\n            (list of :py:class:`Container`)\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        resp = self.client.api.containers(all=all, before=before,\\n                                          filters=filters, limit=limit,\\n                                          since=since)\\n        if sparse:\\n            return [self.prepare_model(r) for r in resp]\\n        else:\\n            containers = []\\n            for r in resp:\\n                try:\\n                    containers.append(self.get(r[\\'Id\\']))\\n                # a container may have been removed while iterating\\n                except NotFound:\\n                    if not ignore_removed:\\n                        raise\\n            return containers', 'def tasks(self, filters=None):\\n        \"\"\"\\n        List the tasks in this service.\\n\\n        Args:\\n            filters (dict): A map of filters to process on the tasks list.\\n                Valid filters: ``id``, ``name``, ``node``,\\n                ``label``, and ``desired-state``.\\n\\n        Returns:\\n            :py:class:`list`: List of task dictionaries.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        if filters is None:\\n            filters = {}\\n        filters[\\'service\\'] = self.id\\n        return self.client.api.tasks(filters=filters)', 'def update(self, **kwargs):\\n        \"\"\"\\n        Update a service\\'s configuration. Similar to the ``docker service\\n        update`` command.\\n\\n        Takes the same parameters as :py:meth:`~ServiceCollection.create`.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        # Image is required, so if it hasn\\'t been set, use current image\\n        if \\'image\\' not in kwargs:\\n            spec = self.attrs[\\'Spec\\'][\\'TaskTemplate\\'][\\'ContainerSpec\\']\\n            kwargs[\\'image\\'] = spec[\\'Image\\']\\n\\n        if kwargs.get(\\'force_update\\') is True:\\n            task_template = self.attrs[\\'Spec\\'][\\'TaskTemplate\\']\\n            current_value = int(task_template.get(\\'ForceUpdate\\', 0))\\n            kwargs[\\'force_update\\'] = current_value + 1\\n\\n        create_kwargs = _get_create_service_kwargs(\\'update\\', kwargs)\\n\\n        return self.client.api.update_service(\\n            self.id,\\n            self.version,\\n            **create_kwargs\\n        )', 'def logs(self, **kwargs):\\n        \"\"\"\\n        Get log stream for the service.\\n        Note: This method works only for services with the ``json-file``\\n        or ``journald`` logging drivers.\\n\\n        Args:\\n            details (bool): Show extra details provided to logs.\\n                Default: ``False``\\n            follow (bool): Keep connection open to read logs as they are\\n                sent by the Engine. Default: ``False``\\n            stdout (bool): Return logs from ``stdout``. Default: ``False``\\n            stderr (bool): Return logs from ``stderr``. Default: ``False``\\n            since (int): UNIX timestamp for the logs staring point.\\n                Default: 0\\n            timestamps (bool): Add timestamps to every log line.\\n            tail (string or int): Number of log lines to be returned,\\n                counting from the current end of the logs. Specify an\\n                integer or ``\\'all\\'`` to output all log lines.\\n                Default: ``all``\\n\\n        Returns:\\n            generator: Logs for the service.\\n        \"\"\"\\n        is_tty = self.attrs[\\'Spec\\'][\\'TaskTemplate\\'][\\'ContainerSpec\\'].get(\\n            \\'TTY\\', False\\n        )\\n        return self.client.api.service_logs(self.id, is_tty=is_tty, **kwargs)', 'def scale(self, replicas):\\n        \"\"\"\\n        Scale service container.\\n\\n        Args:\\n            replicas (int): The number of containers that should be running.\\n\\n        Returns:\\n            bool: ``True`` if successful.\\n        \"\"\"\\n\\n        if \\'Global\\' in self.attrs[\\'Spec\\'][\\'Mode\\'].keys():\\n            raise InvalidArgument(\\'Cannot scale a global container\\')\\n\\n        service_mode = ServiceMode(\\'replicated\\', replicas)\\n        return self.client.api.update_service(self.id, self.version,\\n                                              mode=service_mode,\\n                                              fetch_current_spec=True)', 'def create(self, image, command=None, **kwargs):\\n        \"\"\"\\n        Create a service. Similar to the ``docker service create`` command.\\n\\n        Args:\\n            image (str): The image name to use for the containers.\\n            command (list of str or str): Command to run.\\n            args (list of str): Arguments to the command.\\n            constraints (list of str): :py:class:`~docker.types.Placement`\\n                constraints.\\n            preferences (list of tuple): :py:class:`~docker.types.Placement`\\n                preferences.\\n            platforms (list of tuple): A list of platform constraints\\n                expressed as ``(arch, os)`` tuples.\\n            container_labels (dict): Labels to apply to the container.\\n            endpoint_spec (EndpointSpec): Properties that can be configured to\\n                access and load balance a service. Default: ``None``.\\n            env (list of str): Environment variables, in the form\\n                ``KEY=val``.\\n            hostname (string): Hostname to set on the container.\\n            init (boolean): Run an init inside the container that forwards\\n                signals and reaps processes\\n            isolation (string): Isolation technology used by the service\\'s\\n                containers. Only used for Windows containers.\\n            labels (dict): Labels to apply to the service.\\n            log_driver (str): Log driver to use for containers.\\n            log_driver_options (dict): Log driver options.\\n            mode (ServiceMode): Scheduling mode for the service.\\n                Default:``None``\\n            mounts (list of str): Mounts for the containers, in the form\\n                ``source:target:options``, where options is either\\n                ``ro`` or ``rw``.\\n            name (str): Name to give to the service.\\n            networks (list of str): List of network names or IDs to attach\\n                the service to. Default: ``None``.\\n            resources (Resources): Resource limits and reservations.\\n            restart_policy (RestartPolicy): Restart policy for containers.\\n            secrets (list of :py:class:`docker.types.SecretReference`): List\\n                of secrets accessible to containers for this service.\\n            stop_grace_period (int): Amount of time to wait for\\n                containers to terminate before forcefully killing them.\\n            update_config (UpdateConfig): Specification for the update strategy\\n                of the service. Default: ``None``\\n            rollback_config (RollbackConfig): Specification for the rollback\\n                strategy of the service. Default: ``None``\\n            user (str): User to run commands as.\\n            workdir (str): Working directory for commands to run.\\n            tty (boolean): Whether a pseudo-TTY should be allocated.\\n            groups (:py:class:`list`): A list of additional groups that the\\n                container process will run as.\\n            open_stdin (boolean): Open ``stdin``\\n            read_only (boolean): Mount the container\\'s root filesystem as read\\n                only.\\n            stop_signal (string): Set signal to stop the service\\'s containers\\n            healthcheck (Healthcheck): Healthcheck\\n                configuration for this service.\\n            hosts (:py:class:`dict`): A set of host to IP mappings to add to\\n                the container\\'s `hosts` file.\\n            dns_config (DNSConfig): Specification for DNS\\n                related configurations in resolver configuration file.\\n            configs (:py:class:`list`): List of :py:class:`ConfigReference`\\n                that will be exposed to the service.\\n            privileges (Privileges): Security options for the service\\'s\\n                containers.\\n\\n        Returns:\\n            :py:class:`Service`: The created service.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        kwargs[\\'image\\'] = image\\n        kwargs[\\'command\\'] = command\\n        create_kwargs = _get_create_service_kwargs(\\'create\\', kwargs)\\n        service_id = self.client.api.create_service(**create_kwargs)\\n        return self.get(service_id)', 'def get(self, service_id, insert_defaults=None):\\n        \"\"\"\\n        Get a service.\\n\\n        Args:\\n            service_id (str): The ID of the service.\\n            insert_defaults (boolean): If true, default values will be merged\\n                into the output.\\n\\n        Returns:\\n            :py:class:`Service`: The service.\\n\\n        Raises:\\n            :py:class:`docker.errors.NotFound`\\n                If the service does not exist.\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n            :py:class:`docker.errors.InvalidVersion`\\n                If one of the arguments is not supported with the current\\n                API version.\\n        \"\"\"\\n        return self.prepare_model(\\n            self.client.api.inspect_service(service_id, insert_defaults)\\n        )', 'def list(self, **kwargs):\\n        \"\"\"\\n        List services.\\n\\n        Args:\\n            filters (dict): Filters to process on the nodes list. Valid\\n                filters: ``id``, ``name`` , ``label`` and ``mode``.\\n                Default: ``None``.\\n\\n        Returns:\\n            list of :py:class:`Service`: The services.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        return [\\n            self.prepare_model(s)\\n            for s in self.client.api.services(**kwargs)\\n        ]', 'def reload(self):\\n        \"\"\"\\n        Load this object from the server again and update ``attrs`` with the\\n        new data.\\n        \"\"\"\\n        new_model = self.collection.get(self.id)\\n        self.attrs = new_model.attrs', 'def prepare_model(self, attrs):\\n        \"\"\"\\n        Create a model from a set of attributes.\\n        \"\"\"\\n        if isinstance(attrs, Model):\\n            attrs.client = self.client\\n            attrs.collection = self\\n            return attrs\\n        elif isinstance(attrs, dict):\\n            return self.model(attrs=attrs, client=self.client, collection=self)\\n        else:\\n            raise Exception(\"Can\\'t create %s from %s\" %\\n                            (self.model.__name__, attrs))', 'def containers(self):\\n        \"\"\"\\n        The containers that are connected to the network, as a list of\\n        :py:class:`~docker.models.containers.Container` objects.\\n        \"\"\"\\n        return [\\n            self.client.containers.get(cid) for cid in\\n            (self.attrs.get(\\'Containers\\') or {}).keys()\\n        ]', 'def connect(self, container, *args, **kwargs):\\n        \"\"\"\\n        Connect a container to this network.\\n\\n        Args:\\n            container (str): Container to connect to this network, as either\\n                an ID, name, or :py:class:`~docker.models.containers.Container`\\n                object.\\n            aliases (:py:class:`list`): A list of aliases for this endpoint.\\n                Names in that list can be used within the network to reach the\\n                container. Defaults to ``None``.\\n            links (:py:class:`list`): A list of links for this endpoint.\\n                Containers declared in this list will be linkedto this\\n                container. Defaults to ``None``.\\n            ipv4_address (str): The IP address of this container on the\\n                network, using the IPv4 protocol. Defaults to ``None``.\\n            ipv6_address (str): The IP address of this container on the\\n                network, using the IPv6 protocol. Defaults to ``None``.\\n            link_local_ips (:py:class:`list`): A list of link-local (IPv4/IPv6)\\n                addresses.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        if isinstance(container, Container):\\n            container = container.id\\n        return self.client.api.connect_container_to_network(\\n            container, self.id, *args, **kwargs\\n        )', 'def disconnect(self, container, *args, **kwargs):\\n        \"\"\"\\n        Disconnect a container from this network.\\n\\n        Args:\\n            container (str): Container to disconnect from this network, as\\n                either an ID, name, or\\n                :py:class:`~docker.models.containers.Container` object.\\n            force (bool): Force the container to disconnect from a network.\\n                Default: ``False``\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        if isinstance(container, Container):\\n            container = container.id\\n        return self.client.api.disconnect_container_from_network(\\n            container, self.id, *args, **kwargs\\n        )', 'def create(self, name, *args, **kwargs):\\n        \"\"\"\\n        Create a network. Similar to the ``docker network create``.\\n\\n        Args:\\n            name (str): Name of the network\\n            driver (str): Name of the driver used to create the network\\n            options (dict): Driver options as a key-value dictionary\\n            ipam (IPAMConfig): Optional custom IP scheme for the network.\\n            check_duplicate (bool): Request daemon to check for networks with\\n                same name. Default: ``None``.\\n            internal (bool): Restrict external access to the network. Default\\n                ``False``.\\n            labels (dict): Map of labels to set on the network. Default\\n                ``None``.\\n            enable_ipv6 (bool): Enable IPv6 on the network. Default ``False``.\\n            attachable (bool): If enabled, and the network is in the global\\n                scope,  non-service containers on worker nodes will be able to\\n                connect to the network.\\n            scope (str): Specify the network\\'s scope (``local``, ``global`` or\\n                ``swarm``)\\n            ingress (bool): If set, create an ingress network which provides\\n                the routing-mesh in swarm mode.\\n\\n        Returns:\\n            (:py:class:`Network`): The network that was created.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        Example:\\n            A network using the bridge driver:\\n\\n                >>> client.networks.create(\"network1\", driver=\"bridge\")\\n\\n            You can also create more advanced networks with custom IPAM\\n            configurations. For example, setting the subnet to\\n            ``192.168.52.0/24`` and gateway address to ``192.168.52.254``.\\n\\n            .. code-block:: python\\n\\n                >>> ipam_pool = docker.types.IPAMPool(\\n                    subnet=\\'192.168.52.0/24\\',\\n                    gateway=\\'192.168.52.254\\'\\n                )\\n                >>> ipam_config = docker.types.IPAMConfig(\\n                    pool_configs=[ipam_pool]\\n                )\\n                >>> client.networks.create(\\n                    \"network1\",\\n                    driver=\"bridge\",\\n                    ipam=ipam_config\\n                )\\n\\n        \"\"\"\\n        resp = self.client.api.create_network(name, *args, **kwargs)\\n        return self.get(resp[\\'Id\\'])', 'def get(self, network_id, *args, **kwargs):\\n        \"\"\"\\n        Get a network by its ID.\\n\\n        Args:\\n            network_id (str): The ID of the network.\\n            verbose (bool): Retrieve the service details across the cluster in\\n                swarm mode.\\n            scope (str): Filter the network by scope (``swarm``, ``global``\\n                or ``local``).\\n\\n        Returns:\\n            (:py:class:`Network`) The network.\\n\\n        Raises:\\n            :py:class:`docker.errors.NotFound`\\n                If the network does not exist.\\n\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n\\n        \"\"\"\\n        return self.prepare_model(\\n            self.client.api.inspect_network(network_id, *args, **kwargs)\\n        )', 'def list(self, *args, **kwargs):\\n        \"\"\"\\n        List networks. Similar to the ``docker networks ls`` command.\\n\\n        Args:\\n            names (:py:class:`list`): List of names to filter by.\\n            ids (:py:class:`list`): List of ids to filter by.\\n            filters (dict): Filters to be processed on the network list.\\n                Available filters:\\n                - ``driver=[<driver-name>]`` Matches a network\\'s driver.\\n                - ``label=[<key>]`` or ``label=[<key>=<value>]``.\\n                - ``type=[\"custom\"|\"builtin\"]`` Filters networks by type.\\n            greedy (bool): Fetch more details for each network individually.\\n                You might want this to get the containers attached to them.\\n\\n        Returns:\\n            (list of :py:class:`Network`) The networks on the server.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        greedy = kwargs.pop(\\'greedy\\', False)\\n        resp = self.client.api.networks(*args, **kwargs)\\n        networks = [self.prepare_model(item) for item in resp]\\n        if greedy and version_gte(self.client.api._version, \\'1.28\\'):\\n            for net in networks:\\n                net.reload()\\n        return networks', 'def parse_auth(cls, entries, raise_on_error=False):\\n        \"\"\"\\n        Parses authentication entries\\n\\n        Args:\\n          entries:        Dict of authentication entries.\\n          raise_on_error: If set to true, an invalid format will raise\\n                          InvalidConfigFile\\n\\n        Returns:\\n          Authentication registry.\\n        \"\"\"\\n\\n        conf = {}\\n        for registry, entry in six.iteritems(entries):\\n            if not isinstance(entry, dict):\\n                log.debug(\\n                    \\'Config entry for key {0} is not auth config\\'.format(\\n                        registry\\n                    )\\n                )\\n                # We sometimes fall back to parsing the whole config as if it\\n                # was the auth config by itself, for legacy purposes. In that\\n                # case, we fail silently and return an empty conf if any of the\\n                # keys is not formatted properly.\\n                if raise_on_error:\\n                    raise errors.InvalidConfigFile(\\n                        \\'Invalid configuration for registry {0}\\'.format(\\n                            registry\\n                        )\\n                    )\\n                return {}\\n            if \\'identitytoken\\' in entry:\\n                log.debug(\\n                    \\'Found an IdentityToken entry for registry {0}\\'.format(\\n                        registry\\n                    )\\n                )\\n                conf[registry] = {\\n                    \\'IdentityToken\\': entry[\\'identitytoken\\']\\n                }\\n                continue  # Other values are irrelevant if we have a token\\n\\n            if \\'auth\\' not in entry:\\n                # Starting with engine v1.11 (API 1.23), an empty dictionary is\\n                # a valid value in the auths config.\\n                # https://github.com/docker/compose/issues/3265\\n                log.debug(\\n                    \\'Auth data for {0} is absent. Client might be using a \\'\\n                    \\'credentials store instead.\\'.format(registry)\\n                )\\n                conf[registry] = {}\\n                continue\\n\\n            username, password = decode_auth(entry[\\'auth\\'])\\n            log.debug(\\n                \\'Found entry (registry={0}, username={1})\\'\\n                .format(repr(registry), repr(username))\\n            )\\n\\n            conf[registry] = {\\n                \\'username\\': username,\\n                \\'password\\': password,\\n                \\'email\\': entry.get(\\'email\\'),\\n                \\'serveraddress\\': registry,\\n            }\\n        return conf', 'def resolve_authconfig(self, registry=None):\\n        \"\"\"\\n        Returns the authentication data from the given auth configuration for a\\n        specific registry. As with the Docker client, legacy entries in the\\n        config with full URLs are stripped down to hostnames before checking\\n        for a match. Returns None if no match was found.\\n        \"\"\"\\n\\n        if self.creds_store or self.cred_helpers:\\n            store_name = self.get_credential_store(registry)\\n            if store_name is not None:\\n                log.debug(\\n                    \\'Using credentials store \"{0}\"\\'.format(store_name)\\n                )\\n                cfg = self._resolve_authconfig_credstore(registry, store_name)\\n                if cfg is not None:\\n                    return cfg\\n                log.debug(\\'No entry in credstore - fetching from auth dict\\')\\n\\n        # Default to the public index server\\n        registry = resolve_index_name(registry) if registry else INDEX_NAME\\n        log.debug(\"Looking for auth entry for {0}\".format(repr(registry)))\\n\\n        if registry in self.auths:\\n            log.debug(\"Found {0}\".format(repr(registry)))\\n            return self.auths[registry]\\n\\n        for key, conf in six.iteritems(self.auths):\\n            if resolve_index_name(key) == registry:\\n                log.debug(\"Found {0}\".format(repr(key)))\\n                return conf\\n\\n        log.debug(\"No entry found\")\\n        return None', 'def exec_create(self, container, cmd, stdout=True, stderr=True,\\n                    stdin=False, tty=False, privileged=False, user=\\'\\',\\n                    environment=None, workdir=None, detach_keys=None):\\n        \"\"\"\\n        Sets up an exec instance in a running container.\\n\\n        Args:\\n            container (str): Target container where exec instance will be\\n                created\\n            cmd (str or list): Command to be executed\\n            stdout (bool): Attach to stdout. Default: ``True``\\n            stderr (bool): Attach to stderr. Default: ``True``\\n            stdin (bool): Attach to stdin. Default: ``False``\\n            tty (bool): Allocate a pseudo-TTY. Default: False\\n            privileged (bool): Run as privileged.\\n            user (str): User to execute command as. Default: root\\n            environment (dict or list): A dictionary or a list of strings in\\n                the following format ``[\"PASSWORD=xxx\"]`` or\\n                ``{\"PASSWORD\": \"xxx\"}``.\\n            workdir (str): Path to working directory for this exec session\\n            detach_keys (str): Override the key sequence for detaching\\n                a container. Format is a single character `[a-Z]`\\n                or `ctrl-<value>` where `<value>` is one of:\\n                `a-z`, `@`, `^`, `[`, `,` or `_`.\\n                ~/.docker/config.json is used by default.\\n\\n        Returns:\\n            (dict): A dictionary with an exec ``Id`` key.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n\\n        if environment is not None and utils.version_lt(self._version, \\'1.25\\'):\\n            raise errors.InvalidVersion(\\n                \\'Setting environment for exec is not supported in API < 1.25\\'\\n            )\\n\\n        if isinstance(cmd, six.string_types):\\n            cmd = utils.split_command(cmd)\\n\\n        if isinstance(environment, dict):\\n            environment = utils.utils.format_environment(environment)\\n\\n        data = {\\n            \\'Container\\': container,\\n            \\'User\\': user,\\n            \\'Privileged\\': privileged,\\n            \\'Tty\\': tty,\\n            \\'AttachStdin\\': stdin,\\n            \\'AttachStdout\\': stdout,\\n            \\'AttachStderr\\': stderr,\\n            \\'Cmd\\': cmd,\\n            \\'Env\\': environment,\\n        }\\n\\n        if workdir is not None:\\n            if utils.version_lt(self._version, \\'1.35\\'):\\n                raise errors.InvalidVersion(\\n                    \\'workdir is not supported for API version < 1.35\\'\\n                )\\n            data[\\'WorkingDir\\'] = workdir\\n\\n        if detach_keys:\\n            data[\\'detachKeys\\'] = detach_keys\\n        elif \\'detachKeys\\' in self._general_configs:\\n            data[\\'detachKeys\\'] = self._general_configs[\\'detachKeys\\']\\n\\n        url = self._url(\\'/containers/{0}/exec\\', container)\\n        res = self._post_json(url, data=data)\\n        return self._result(res, True)', 'def exec_inspect(self, exec_id):\\n        \"\"\"\\n        Return low-level information about an exec command.\\n\\n        Args:\\n            exec_id (str): ID of the exec instance\\n\\n        Returns:\\n            (dict): Dictionary of values returned by the endpoint.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        if isinstance(exec_id, dict):\\n            exec_id = exec_id.get(\\'Id\\')\\n        res = self._get(self._url(\"/exec/{0}/json\", exec_id))\\n        return self._result(res, True)', 'def exec_resize(self, exec_id, height=None, width=None):\\n        \"\"\"\\n        Resize the tty session used by the specified exec command.\\n\\n        Args:\\n            exec_id (str): ID of the exec instance\\n            height (int): Height of tty session\\n            width (int): Width of tty session\\n        \"\"\"\\n\\n        if isinstance(exec_id, dict):\\n            exec_id = exec_id.get(\\'Id\\')\\n\\n        params = {\\'h\\': height, \\'w\\': width}\\n        url = self._url(\"/exec/{0}/resize\", exec_id)\\n        res = self._post(url, params=params)\\n        self._raise_for_status(res)', 'def exec_start(self, exec_id, detach=False, tty=False, stream=False,\\n                   socket=False, demux=False):\\n        \"\"\"\\n        Start a previously set up exec instance.\\n\\n        Args:\\n            exec_id (str): ID of the exec instance\\n            detach (bool): If true, detach from the exec command.\\n                Default: False\\n            tty (bool): Allocate a pseudo-TTY. Default: False\\n            stream (bool): Stream response data. Default: False\\n            socket (bool): Return the connection socket to allow custom\\n                read/write operations.\\n            demux (bool): Return stdout and stderr separately\\n\\n        Returns:\\n\\n            (generator or str or tuple): If ``stream=True``, a generator\\n            yielding response chunks. If ``socket=True``, a socket object for\\n            the connection. A string containing response data otherwise. If\\n            ``demux=True``, a tuple with two elements of type byte: stdout and\\n            stderr.\\n\\n        Raises:\\n            :py:class:`docker.errors.APIError`\\n                If the server returns an error.\\n        \"\"\"\\n        # we want opened socket if socket == True\\n\\n        data = {\\n            \\'Tty\\': tty,\\n            \\'Detach\\': detach\\n        }\\n\\n        headers = {} if detach else {\\n            \\'Connection\\': \\'Upgrade\\',\\n            \\'Upgrade\\': \\'tcp\\'\\n        }\\n\\n        res = self._post_json(\\n            self._url(\\'/exec/{0}/start\\', exec_id),\\n            headers=headers,\\n            data=data,\\n            stream=True\\n        )\\n        if detach:\\n            return self._result(res)\\n        if socket:\\n            return self._get_raw_response_socket(res)\\n        return self._read_from_socket(res, stream, tty=tty, demux=demux)', 'def _raise_for_status(self, response):\\n        \"\"\"Raises stored :class:`APIError`, if one occurred.\"\"\"\\n        try:\\n            response.raise_for_status()\\n        except requests.exceptions.HTTPError as e:\\n            raise create_api_error_from_http_exception(e)', 'def _stream_helper(self, response, decode=False):\\n        \"\"\"Generator for data coming from a chunked-encoded HTTP response.\"\"\"\\n\\n        if response.raw._fp.chunked:\\n            if decode:\\n                for chunk in json_stream(self._stream_helper(response, False)):\\n                    yield chunk\\n            else:\\n                reader = response.raw\\n                while not reader.closed:\\n                    # this read call will block until we get a chunk\\n                    data = reader.read(1)\\n                    if not data:\\n                        break\\n                    if reader._fp.chunk_left:\\n                        data += reader.read(reader._fp.chunk_left)\\n                    yield data\\n        else:\\n            # Response isn\\'t chunked, meaning we probably\\n            # encountered an error immediately\\n            yield self._result(response, json=decode)', 'def _multiplexed_buffer_helper(self, response):\\n        \"\"\"A generator of multiplexed data blocks read from a buffered\\n        response.\"\"\"\\n        buf = self._result(response, binary=True)\\n        buf_length = len(buf)\\n        walker = 0\\n        while True:\\n            if buf_length - walker < STREAM_HEADER_SIZE_BYTES:\\n                break\\n            header = buf[walker:walker + STREAM_HEADER_SIZE_BYTES]\\n            _, length = struct.unpack_from(\\'>BxxxL\\', header)\\n            start = walker + STREAM_HEADER_SIZE_BYTES\\n            end = start + length\\n            walker = end\\n            yield buf[start:end]', 'def _multiplexed_response_stream_helper(self, response):\\n        \"\"\"A generator of multiplexed data blocks coming from a response\\n        stream.\"\"\"\\n\\n        # Disable timeout on the underlying socket to prevent\\n        # Read timed out(s) for long running processes\\n        socket = self._get_raw_response_socket(response)\\n        self._disable_socket_timeout(socket)\\n\\n        while True:\\n            header = response.raw.read(STREAM_HEADER_SIZE_BYTES)\\n            if not header:\\n                break\\n            _, length = struct.unpack(\\'>BxxxL\\', header)\\n            if not length:\\n                continue\\n            data = response.raw.read(length)\\n            if not data:\\n                break\\n            yield data', \"def _stream_raw_result(self, response, chunk_size=1, decode=True):\\n        ''' Stream result for TTY-enabled container and raw binary data'''\\n        self._raise_for_status(response)\\n        for out in response.iter_content(chunk_size, decode):\\n            yield out\", 'def _disable_socket_timeout(self, socket):\\n        \"\"\" Depending on the combination of python version and whether we\\'re\\n        connecting over http or https, we might need to access _sock, which\\n        may or may not exist; or we may need to just settimeout on socket\\n        itself, which also may or may not have settimeout on it. To avoid\\n        missing the correct one, we try both.\\n\\n        We also do not want to set the timeout if it is already disabled, as\\n        you run the risk of changing a socket that was non-blocking to\\n        blocking, for example when using gevent.\\n        \"\"\"\\n        sockets = [socket, getattr(socket, \\'_sock\\', None)]\\n\\n        for s in sockets:\\n            if not hasattr(s, \\'settimeout\\'):\\n                continue\\n\\n            timeout = -1\\n\\n            if hasattr(s, \\'gettimeout\\'):\\n                timeout = s.gettimeout()\\n\\n            # Don\\'t change the timeout if it is already disabled.\\n            if timeout is None or timeout == 0.0:\\n                continue\\n\\n            s.settimeout(None)', 'def reload_config(self, dockercfg_path=None):\\n        \"\"\"\\n        Force a reload of the auth configuration\\n\\n        Args:\\n            dockercfg_path (str): Use a custom path for the Docker config file\\n                (default ``$HOME/.docker/config.json`` if present,\\n                otherwise``$HOME/.dockercfg``)\\n\\n        Returns:\\n            None\\n        \"\"\"\\n        self._auth_configs = auth.load_config(\\n            dockercfg_path, credstore_env=self.credstore_env\\n        )', 'def acquire(self, blocking=None, blocking_timeout=None, token=None):\\n        \"\"\"\\n        Use Redis to hold a shared, distributed lock named ``name``.\\n        Returns True once the lock is acquired.\\n\\n        If ``blocking`` is False, always return immediately. If the lock\\n        was acquired, return True, otherwise return False.\\n\\n        ``blocking_timeout`` specifies the maximum number of seconds to\\n        wait trying to acquire the lock.\\n\\n        ``token`` specifies the token value to be used. If provided, token\\n        must be a bytes object or a string that can be encoded to a bytes\\n        object with the default encoding. If a token isn\\'t specified, a UUID\\n        will be generated.\\n        \"\"\"\\n        sleep = self.sleep\\n        if token is None:\\n            token = uuid.uuid1().hex.encode()\\n        else:\\n            encoder = self.redis.connection_pool.get_encoder()\\n            token = encoder.encode(token)\\n        if blocking is None:\\n            blocking = self.blocking\\n        if blocking_timeout is None:\\n            blocking_timeout = self.blocking_timeout\\n        stop_trying_at = None\\n        if blocking_timeout is not None:\\n            stop_trying_at = mod_time.time() + blocking_timeout\\n        while True:\\n            if self.do_acquire(token):\\n                self.local.token = token\\n                return True\\n            if not blocking:\\n                return False\\n            if stop_trying_at is not None and mod_time.time() > stop_trying_at:\\n                return False\\n            mod_time.sleep(sleep)', 'def owned(self):\\n        \"\"\"\\n        Returns True if this key is locked by this lock, otherwise False.\\n        \"\"\"\\n        stored_token = self.redis.get(self.name)\\n        # need to always compare bytes to bytes\\n        # TODO: this can be simplified when the context manager is finished\\n        if stored_token and not isinstance(stored_token, bytes):\\n            encoder = self.redis.connection_pool.get_encoder()\\n            stored_token = encoder.encode(stored_token)\\n        return self.local.token is not None and \\\\\\n            stored_token == self.local.token', 'def reacquire(self):\\n        \"\"\"\\n        Resets a TTL of an already acquired lock back to a timeout value.\\n        \"\"\"\\n        if self.local.token is None:\\n            raise LockError(\"Cannot reacquire an unlocked lock\")\\n        if self.timeout is None:\\n            raise LockError(\"Cannot reacquire a lock with no timeout\")\\n        return self.do_reacquire()', 'def timestamp_to_datetime(response):\\n    \"Converts a unix timestamp to a Python datetime object\"\\n    if not response:\\n        return None\\n    try:\\n        response = int(response)\\n    except ValueError:\\n        return None\\n    return datetime.datetime.fromtimestamp(response)', 'def pairs_to_dict(response, decode_keys=False):\\n    \"Create a dict given a list of key/value pairs\"\\n    if response is None:\\n        return {}\\n    if decode_keys:\\n        # the iter form is faster, but I don\\'t know how to make that work\\n        # with a nativestr() map\\n        return dict(izip(imap(nativestr, response[::2]), response[1::2]))\\n    else:\\n        it = iter(response)\\n        return dict(izip(it, it))', 'def zset_score_pairs(response, **options):\\n    \"\"\"\\n    If ``withscores`` is specified in the options, return the response as\\n    a list of (value, score) pairs\\n    \"\"\"\\n    if not response or not options.get(\\'withscores\\'):\\n        return response\\n    score_cast_func = options.get(\\'score_cast_func\\', float)\\n    it = iter(response)\\n    return list(izip(it, imap(score_cast_func, it)))', 'def sort_return_tuples(response, **options):\\n    \"\"\"\\n    If ``groups`` is specified, return the response as a list of\\n    n-element tuples with n being the value found in options[\\'groups\\']\\n    \"\"\"\\n    if not response or not options.get(\\'groups\\'):\\n        return response\\n    n = options[\\'groups\\']\\n    return list(izip(*[response[i::n] for i in range(n)]))', 'def from_url(cls, url, db=None, **kwargs):\\n        \"\"\"\\n        Return a Redis client object configured from the given URL\\n\\n        For example::\\n\\n            redis://[:password]@localhost:6379/0\\n            rediss://[:password]@localhost:6379/0\\n            unix://[:password]@/path/to/socket.sock?db=0\\n\\n        Three URL schemes are supported:\\n\\n        - ```redis://``\\n          <http://www.iana.org/assignments/uri-schemes/prov/redis>`_ creates a\\n          normal TCP socket connection\\n        - ```rediss://``\\n          <http://www.iana.org/assignments/uri-schemes/prov/rediss>`_ creates a\\n          SSL wrapped TCP socket connection\\n        - ``unix://`` creates a Unix Domain Socket connection\\n\\n        There are several ways to specify a database number. The parse function\\n        will return the first specified option:\\n            1. A ``db`` querystring option, e.g. redis://localhost?db=0\\n            2. If using the redis:// scheme, the path argument of the url, e.g.\\n               redis://localhost/0\\n            3. The ``db`` argument to this function.\\n\\n        If none of these options are specified, db=0 is used.\\n\\n        Any additional querystring arguments and keyword arguments will be\\n        passed along to the ConnectionPool class\\'s initializer. In the case\\n        of conflicting arguments, querystring arguments always win.\\n        \"\"\"\\n        connection_pool = ConnectionPool.from_url(url, db=db, **kwargs)\\n        return cls(connection_pool=connection_pool)', 'def lock(self, name, timeout=None, sleep=0.1, blocking_timeout=None,\\n             lock_class=None, thread_local=True):\\n        \"\"\"\\n        Return a new Lock object using key ``name`` that mimics\\n        the behavior of threading.Lock.\\n\\n        If specified, ``timeout`` indicates a maximum life for the lock.\\n        By default, it will remain locked until release() is called.\\n\\n        ``sleep`` indicates the amount of time to sleep per loop iteration\\n        when the lock is in blocking mode and another client is currently\\n        holding the lock.\\n\\n        ``blocking_timeout`` indicates the maximum amount of time in seconds to\\n        spend trying to acquire the lock. A value of ``None`` indicates\\n        continue trying forever. ``blocking_timeout`` can be specified as a\\n        float or integer, both representing the number of seconds to wait.\\n\\n        ``lock_class`` forces the specified lock implementation.\\n\\n        ``thread_local`` indicates whether the lock token is placed in\\n        thread-local storage. By default, the token is placed in thread local\\n        storage so that a thread only sees its token, not a token set by\\n        another thread. Consider the following timeline:\\n\\n            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.\\n                     thread-1 sets the token to \"abc\"\\n            time: 1, thread-2 blocks trying to acquire `my-lock` using the\\n                     Lock instance.\\n            time: 5, thread-1 has not yet completed. redis expires the lock\\n                     key.\\n            time: 5, thread-2 acquired `my-lock` now that it\\'s available.\\n                     thread-2 sets the token to \"xyz\"\\n            time: 6, thread-1 finishes its work and calls release(). if the\\n                     token is *not* stored in thread local storage, then\\n                     thread-1 would see the token value as \"xyz\" and would be\\n                     able to successfully release the thread-2\\'s lock.\\n\\n        In some use cases it\\'s necessary to disable thread local storage. For\\n        example, if you have code where one thread acquires a lock and passes\\n        that lock instance to a worker thread to release later. If thread\\n        local storage isn\\'t disabled in this case, the worker thread won\\'t see\\n        the token set by the thread that acquired the lock. Our assumption\\n        is that these cases aren\\'t common and as such default to using\\n        thread local storage.        \"\"\"\\n        if lock_class is None:\\n            lock_class = Lock\\n        return lock_class(self, name, timeout=timeout, sleep=sleep,\\n                          blocking_timeout=blocking_timeout,\\n                          thread_local=thread_local)', 'def execute_command(self, *args, **options):\\n        \"Execute a command and return a parsed response\"\\n        pool = self.connection_pool\\n        command_name = args[0]\\n        connection = pool.get_connection(command_name, **options)\\n        try:\\n            connection.send_command(*args)\\n            return self.parse_response(connection, command_name, **options)\\n        except (ConnectionError, TimeoutError) as e:\\n            connection.disconnect()\\n            if not (connection.retry_on_timeout and\\n                    isinstance(e, TimeoutError)):\\n                raise\\n            connection.send_command(*args)\\n            return self.parse_response(connection, command_name, **options)\\n        finally:\\n            pool.release(connection)', 'def parse_response(self, connection, command_name, **options):\\n        \"Parses a response from the Redis server\"\\n        try:\\n            response = connection.read_response()\\n        except ResponseError:\\n            if EMPTY_RESPONSE in options:\\n                return options[EMPTY_RESPONSE]\\n            raise\\n        if command_name in self.response_callbacks:\\n            return self.response_callbacks[command_name](response, **options)\\n        return response', 'def client_kill_filter(self, _id=None, _type=None, addr=None, skipme=None):\\n        \"\"\"\\n        Disconnects client(s) using a variety of filter options\\n        :param id: Kills a client by its unique ID field\\n        :param type: Kills a client by type where type is one of \\'normal\\',\\n        \\'master\\', \\'slave\\' or \\'pubsub\\'\\n        :param addr: Kills a client by its \\'address:port\\'\\n        :param skipme: If True, then the client calling the command\\n        will not get killed even if it is identified by one of the filter\\n        options. If skipme is not provided, the server defaults to skipme=True\\n        \"\"\"\\n        args = []\\n        if _type is not None:\\n            client_types = (\\'normal\\', \\'master\\', \\'slave\\', \\'pubsub\\')\\n            if str(_type).lower() not in client_types:\\n                raise DataError(\"CLIENT KILL type must be one of %r\" % (\\n                                client_types,))\\n            args.extend((Token.get_token(\\'TYPE\\'), _type))\\n        if skipme is not None:\\n            if not isinstance(skipme, bool):\\n                raise DataError(\"CLIENT KILL skipme must be a bool\")\\n            if skipme:\\n                args.extend((Token.get_token(\\'SKIPME\\'),\\n                             Token.get_token(\\'YES\\')))\\n            else:\\n                args.extend((Token.get_token(\\'SKIPME\\'),\\n                             Token.get_token(\\'NO\\')))\\n        if _id is not None:\\n            args.extend((Token.get_token(\\'ID\\'), _id))\\n        if addr is not None:\\n            args.extend((Token.get_token(\\'ADDR\\'), addr))\\n        if not args:\\n            raise DataError(\"CLIENT KILL <filter> <value> ... ... <filter> \"\\n                            \"<value> must specify at least one filter\")\\n        return self.execute_command(\\'CLIENT KILL\\', *args)', 'def client_list(self, _type=None):\\n        \"\"\"\\n        Returns a list of currently connected clients.\\n        If type of client specified, only that type will be returned.\\n        :param _type: optional. one of the client types (normal, master,\\n         replica, pubsub)\\n        \"\"\"\\n        \"Returns a list of currently connected clients\"\\n        if _type is not None:\\n            client_types = (\\'normal\\', \\'master\\', \\'replica\\', \\'pubsub\\')\\n            if str(_type).lower() not in client_types:\\n                raise DataError(\"CLIENT LIST _type must be one of %r\" % (\\n                                client_types,))\\n            return self.execute_command(\\'CLIENT LIST\\', Token.get_token(\\'TYPE\\'),\\n                                        _type)\\n        return self.execute_command(\\'CLIENT LIST\\')', 'def client_unblock(self, client_id, error=False):\\n        \"\"\"\\n        Unblocks a connection by its client id.\\n        If ``error`` is True, unblocks the client with a special error message.\\n        If ``error`` is False (default), the client is unblocked using the\\n        regular timeout mechanism.\\n        \"\"\"\\n        args = [\\'CLIENT UNBLOCK\\', int(client_id)]\\n        if error:\\n            args.append(Token.get_token(\\'ERROR\\'))\\n        return self.execute_command(*args)', 'def client_pause(self, timeout):\\n        \"\"\"\\n        Suspend all the Redis clients for the specified amount of time\\n        :param timeout: milliseconds to pause clients\\n        \"\"\"\\n        if not isinstance(timeout, (int, long)):\\n            raise DataError(\"CLIENT PAUSE timeout must be an integer\")\\n        return self.execute_command(\\'CLIENT PAUSE\\', str(timeout))', 'def flushall(self, asynchronous=False):\\n        \"\"\"\\n        Delete all keys in all databases on the current host.\\n\\n        ``asynchronous`` indicates whether the operation is\\n        executed asynchronously by the server.\\n        \"\"\"\\n        args = []\\n        if asynchronous:\\n            args.append(Token.get_token(\\'ASYNC\\'))\\n        return self.execute_command(\\'FLUSHALL\\', *args)', 'def migrate(self, host, port, keys, destination_db, timeout,\\n                copy=False, replace=False, auth=None):\\n        \"\"\"\\n        Migrate 1 or more keys from the current Redis server to a different\\n        server specified by the ``host``, ``port`` and ``destination_db``.\\n\\n        The ``timeout``, specified in milliseconds, indicates the maximum\\n        time the connection between the two servers can be idle before the\\n        command is interrupted.\\n\\n        If ``copy`` is True, the specified ``keys`` are NOT deleted from\\n        the source server.\\n\\n        If ``replace`` is True, this operation will overwrite the keys\\n        on the destination server if they exist.\\n\\n        If ``auth`` is specified, authenticate to the destination server with\\n        the password provided.\\n        \"\"\"\\n        keys = list_or_args(keys, [])\\n        if not keys:\\n            raise DataError(\\'MIGRATE requires at least one key\\')\\n        pieces = []\\n        if copy:\\n            pieces.append(Token.get_token(\\'COPY\\'))\\n        if replace:\\n            pieces.append(Token.get_token(\\'REPLACE\\'))\\n        if auth:\\n            pieces.append(Token.get_token(\\'AUTH\\'))\\n            pieces.append(auth)\\n        pieces.append(Token.get_token(\\'KEYS\\'))\\n        pieces.extend(keys)\\n        return self.execute_command(\\'MIGRATE\\', host, port, \\'\\', destination_db,\\n                                    timeout, *pieces)', 'def object(self, infotype, key):\\n        \"Return the encoding, idletime, or refcount about the key\"\\n        return self.execute_command(\\'OBJECT\\', infotype, key, infotype=infotype)', 'def memory_usage(self, key, samples=None):\\n        \"\"\"\\n        Return the total memory usage for key, its value and associated\\n        administrative overheads.\\n\\n        For nested data structures, ``samples`` is the number of elements to\\n        sample. If left unspecified, the server\\'s default is 5. Use 0 to sample\\n        all elements.\\n        \"\"\"\\n        args = []\\n        if isinstance(samples, int):\\n            args.extend([Token.get_token(\\'SAMPLES\\'), samples])\\n        return self.execute_command(\\'MEMORY USAGE\\', key, *args)', 'def shutdown(self, save=False, nosave=False):\\n        \"\"\"Shutdown the Redis server.  If Redis has persistence configured,\\n        data will be flushed before shutdown.  If the \"save\" option is set,\\n        a data flush will be attempted even if there is no persistence\\n        configured.  If the \"nosave\" option is set, no data flush will be\\n        attempted.  The \"save\" and \"nosave\" options cannot both be set.\\n        \"\"\"\\n        if save and nosave:\\n            raise DataError(\\'SHUTDOWN save and nosave cannot both be set\\')\\n        args = [\\'SHUTDOWN\\']\\n        if save:\\n            args.append(\\'SAVE\\')\\n        if nosave:\\n            args.append(\\'NOSAVE\\')\\n        try:\\n            self.execute_command(*args)\\n        except ConnectionError:\\n            # a ConnectionError here is expected\\n            return\\n        raise RedisError(\"SHUTDOWN seems to have failed.\")', 'def slaveof(self, host=None, port=None):\\n        \"\"\"\\n        Set the server to be a replicated slave of the instance identified\\n        by the ``host`` and ``port``. If called without arguments, the\\n        instance is promoted to a master instead.\\n        \"\"\"\\n        if host is None and port is None:\\n            return self.execute_command(\\'SLAVEOF\\', Token.get_token(\\'NO\\'),\\n                                        Token.get_token(\\'ONE\\'))\\n        return self.execute_command(\\'SLAVEOF\\', host, port)', 'def bitop(self, operation, dest, *keys):\\n        \"\"\"\\n        Perform a bitwise operation using ``operation`` between ``keys`` and\\n        store the result in ``dest``.\\n        \"\"\"\\n        return self.execute_command(\\'BITOP\\', operation, dest, *keys)', 'def bitpos(self, key, bit, start=None, end=None):\\n        \"\"\"\\n        Return the position of the first bit set to 1 or 0 in a string.\\n        ``start`` and ``end`` difines search range. The range is interpreted\\n        as a range of bytes and not a range of bits, so start=0 and end=2\\n        means to look at the first three bytes.\\n        \"\"\"\\n        if bit not in (0, 1):\\n            raise DataError(\\'bit must be 0 or 1\\')\\n        params = [key, bit]\\n\\n        start is not None and params.append(start)\\n\\n        if start is not None and end is not None:\\n            params.append(end)\\n        elif start is None and end is not None:\\n            raise DataError(\"start argument is not set, \"\\n                            \"when end is specified\")\\n        return self.execute_command(\\'BITPOS\\', *params)', 'def expireat(self, name, when):\\n        \"\"\"\\n        Set an expire flag on key ``name``. ``when`` can be represented\\n        as an integer indicating unix time or a Python datetime object.\\n        \"\"\"\\n        if isinstance(when, datetime.datetime):\\n            when = int(mod_time.mktime(when.timetuple()))\\n        return self.execute_command(\\'EXPIREAT\\', name, when)', 'def mget(self, keys, *args):\\n        \"\"\"\\n        Returns a list of values ordered identically to ``keys``\\n        \"\"\"\\n        args = list_or_args(keys, args)\\n        options = {}\\n        if not args:\\n            options[EMPTY_RESPONSE] = []\\n        return self.execute_command(\\'MGET\\', *args, **options)', 'def mset(self, mapping):\\n        \"\"\"\\n        Sets key/values based on a mapping. Mapping is a dictionary of\\n        key/value pairs. Both keys and values should be strings or types that\\n        can be cast to a string via str().\\n        \"\"\"\\n        items = []\\n        for pair in iteritems(mapping):\\n            items.extend(pair)\\n        return self.execute_command(\\'MSET\\', *items)', 'def msetnx(self, mapping):\\n        \"\"\"\\n        Sets key/values based on a mapping if none of the keys are already set.\\n        Mapping is a dictionary of key/value pairs. Both keys and values\\n        should be strings or types that can be cast to a string via str().\\n        Returns a boolean indicating if the operation was successful.\\n        \"\"\"\\n        items = []\\n        for pair in iteritems(mapping):\\n            items.extend(pair)\\n        return self.execute_command(\\'MSETNX\\', *items)', 'def pexpire(self, name, time):\\n        \"\"\"\\n        Set an expire flag on key ``name`` for ``time`` milliseconds.\\n        ``time`` can be represented by an integer or a Python timedelta\\n        object.\\n        \"\"\"\\n        if isinstance(time, datetime.timedelta):\\n            time = int(time.total_seconds() * 1000)\\n        return self.execute_command(\\'PEXPIRE\\', name, time)', 'def pexpireat(self, name, when):\\n        \"\"\"\\n        Set an expire flag on key ``name``. ``when`` can be represented\\n        as an integer representing unix time in milliseconds (unix time * 1000)\\n        or a Python datetime object.\\n        \"\"\"\\n        if isinstance(when, datetime.datetime):\\n            ms = int(when.microsecond / 1000)\\n            when = int(mod_time.mktime(when.timetuple())) * 1000 + ms\\n        return self.execute_command(\\'PEXPIREAT\\', name, when)', 'def psetex(self, name, time_ms, value):\\n        \"\"\"\\n        Set the value of key ``name`` to ``value`` that expires in ``time_ms``\\n        milliseconds. ``time_ms`` can be represented by an integer or a Python\\n        timedelta object\\n        \"\"\"\\n        if isinstance(time_ms, datetime.timedelta):\\n            time_ms = int(time_ms.total_seconds() * 1000)\\n        return self.execute_command(\\'PSETEX\\', name, time_ms, value)', 'def restore(self, name, ttl, value, replace=False):\\n        \"\"\"\\n        Create a key using the provided serialized value, previously obtained\\n        using DUMP.\\n        \"\"\"\\n        params = [name, ttl, value]\\n        if replace:\\n            params.append(\\'REPLACE\\')\\n        return self.execute_command(\\'RESTORE\\', *params)', 'def set(self, name, value, ex=None, px=None, nx=False, xx=False):\\n        \"\"\"\\n        Set the value at key ``name`` to ``value``\\n\\n        ``ex`` sets an expire flag on key ``name`` for ``ex`` seconds.\\n\\n        ``px`` sets an expire flag on key ``name`` for ``px`` milliseconds.\\n\\n        ``nx`` if set to True, set the value at key ``name`` to ``value`` only\\n            if it does not exist.\\n\\n        ``xx`` if set to True, set the value at key ``name`` to ``value`` only\\n            if it already exists.\\n        \"\"\"\\n        pieces = [name, value]\\n        if ex is not None:\\n            pieces.append(\\'EX\\')\\n            if isinstance(ex, datetime.timedelta):\\n                ex = int(ex.total_seconds())\\n            pieces.append(ex)\\n        if px is not None:\\n            pieces.append(\\'PX\\')\\n            if isinstance(px, datetime.timedelta):\\n                px = int(px.total_seconds() * 1000)\\n            pieces.append(px)\\n\\n        if nx:\\n            pieces.append(\\'NX\\')\\n        if xx:\\n            pieces.append(\\'XX\\')\\n        return self.execute_command(\\'SET\\', *pieces)', 'def setbit(self, name, offset, value):\\n        \"\"\"\\n        Flag the ``offset`` in ``name`` as ``value``. Returns a boolean\\n        indicating the previous value of ``offset``.\\n        \"\"\"\\n        value = value and 1 or 0\\n        return self.execute_command(\\'SETBIT\\', name, offset, value)', 'def setex(self, name, time, value):\\n        \"\"\"\\n        Set the value of key ``name`` to ``value`` that expires in ``time``\\n        seconds. ``time`` can be represented by an integer or a Python\\n        timedelta object.\\n        \"\"\"\\n        if isinstance(time, datetime.timedelta):\\n            time = int(time.total_seconds())\\n        return self.execute_command(\\'SETEX\\', name, time, value)', 'def setrange(self, name, offset, value):\\n        \"\"\"\\n        Overwrite bytes in the value of ``name`` starting at ``offset`` with\\n        ``value``. If ``offset`` plus the length of ``value`` exceeds the\\n        length of the original value, the new value will be larger than before.\\n        If ``offset`` exceeds the length of the original value, null bytes\\n        will be used to pad between the end of the previous value and the start\\n        of what\\'s being injected.\\n\\n        Returns the length of the new string.\\n        \"\"\"\\n        return self.execute_command(\\'SETRANGE\\', name, offset, value)', 'def blpop(self, keys, timeout=0):\\n        \"\"\"\\n        LPOP a value off of the first non-empty list\\n        named in the ``keys`` list.\\n\\n        If none of the lists in ``keys`` has a value to LPOP, then block\\n        for ``timeout`` seconds, or until a value gets pushed on to one\\n        of the lists.\\n\\n        If timeout is 0, then block indefinitely.\\n        \"\"\"\\n        if timeout is None:\\n            timeout = 0\\n        keys = list_or_args(keys, None)\\n        keys.append(timeout)\\n        return self.execute_command(\\'BLPOP\\', *keys)', 'def brpop(self, keys, timeout=0):\\n        \"\"\"\\n        RPOP a value off of the first non-empty list\\n        named in the ``keys`` list.\\n\\n        If none of the lists in ``keys`` has a value to RPOP, then block\\n        for ``timeout`` seconds, or until a value gets pushed on to one\\n        of the lists.\\n\\n        If timeout is 0, then block indefinitely.\\n        \"\"\"\\n        if timeout is None:\\n            timeout = 0\\n        keys = list_or_args(keys, None)\\n        keys.append(timeout)\\n        return self.execute_command(\\'BRPOP\\', *keys)', 'def lrange(self, name, start, end):\\n        \"\"\"\\n        Return a slice of the list ``name`` between\\n        position ``start`` and ``end``\\n\\n        ``start`` and ``end`` can be negative numbers just like\\n        Python slicing notation\\n        \"\"\"\\n        return self.execute_command(\\'LRANGE\\', name, start, end)', 'def lrem(self, name, count, value):\\n        \"\"\"\\n        Remove the first ``count`` occurrences of elements equal to ``value``\\n        from the list stored at ``name``.\\n\\n        The count argument influences the operation in the following ways:\\n            count > 0: Remove elements equal to value moving from head to tail.\\n            count < 0: Remove elements equal to value moving from tail to head.\\n            count = 0: Remove all elements equal to value.\\n        \"\"\"\\n        return self.execute_command(\\'LREM\\', name, count, value)', 'def lset(self, name, index, value):\\n        \"Set ``position`` of list ``name`` to ``value``\"\\n        return self.execute_command(\\'LSET\\', name, index, value)', 'def sort(self, name, start=None, num=None, by=None, get=None,\\n             desc=False, alpha=False, store=None, groups=False):\\n        \"\"\"\\n        Sort and return the list, set or sorted set at ``name``.\\n\\n        ``start`` and ``num`` allow for paging through the sorted data\\n\\n        ``by`` allows using an external key to weight and sort the items.\\n            Use an \"*\" to indicate where in the key the item value is located\\n\\n        ``get`` allows for returning items from external keys rather than the\\n            sorted data itself.  Use an \"*\" to indicate where int he key\\n            the item value is located\\n\\n        ``desc`` allows for reversing the sort\\n\\n        ``alpha`` allows for sorting lexicographically rather than numerically\\n\\n        ``store`` allows for storing the result of the sort into\\n            the key ``store``\\n\\n        ``groups`` if set to True and if ``get`` contains at least two\\n            elements, sort will return a list of tuples, each containing the\\n            values fetched from the arguments to ``get``.\\n\\n        \"\"\"\\n        if (start is not None and num is None) or \\\\\\n                (num is not None and start is None):\\n            raise DataError(\"``start`` and ``num`` must both be specified\")\\n\\n        pieces = [name]\\n        if by is not None:\\n            pieces.append(Token.get_token(\\'BY\\'))\\n            pieces.append(by)\\n        if start is not None and num is not None:\\n            pieces.append(Token.get_token(\\'LIMIT\\'))\\n            pieces.append(start)\\n            pieces.append(num)\\n        if get is not None:\\n            # If get is a string assume we want to get a single value.\\n            # Otherwise assume it\\'s an interable and we want to get multiple\\n            # values. We can\\'t just iterate blindly because strings are\\n            # iterable.\\n            if isinstance(get, (bytes, basestring)):\\n                pieces.append(Token.get_token(\\'GET\\'))\\n                pieces.append(get)\\n            else:\\n                for g in get:\\n                    pieces.append(Token.get_token(\\'GET\\'))\\n                    pieces.append(g)\\n        if desc:\\n            pieces.append(Token.get_token(\\'DESC\\'))\\n        if alpha:\\n            pieces.append(Token.get_token(\\'ALPHA\\'))\\n        if store is not None:\\n            pieces.append(Token.get_token(\\'STORE\\'))\\n            pieces.append(store)\\n\\n        if groups:\\n            if not get or isinstance(get, (bytes, basestring)) or len(get) < 2:\\n                raise DataError(\\'when using \"groups\" the \"get\" argument \\'\\n                                \\'must be specified and contain at least \\'\\n                                \\'two keys\\')\\n\\n        options = {\\'groups\\': len(get) if groups else None}\\n        return self.execute_command(\\'SORT\\', *pieces, **options)', 'def scan(self, cursor=0, match=None, count=None):\\n        \"\"\"\\n        Incrementally return lists of key names. Also return a cursor\\n        indicating the scan position.\\n\\n        ``match`` allows for filtering the keys by pattern\\n\\n        ``count`` allows for hint the minimum number of returns\\n        \"\"\"\\n        pieces = [cursor]\\n        if match is not None:\\n            pieces.extend([Token.get_token(\\'MATCH\\'), match])\\n        if count is not None:\\n            pieces.extend([Token.get_token(\\'COUNT\\'), count])\\n        return self.execute_command(\\'SCAN\\', *pieces)', 'def scan_iter(self, match=None, count=None):\\n        \"\"\"\\n        Make an iterator using the SCAN command so that the client doesn\\'t\\n        need to remember the cursor position.\\n\\n        ``match`` allows for filtering the keys by pattern\\n\\n        ``count`` allows for hint the minimum number of returns\\n        \"\"\"\\n        cursor = \\'0\\'\\n        while cursor != 0:\\n            cursor, data = self.scan(cursor=cursor, match=match, count=count)\\n            for item in data:\\n                yield item', 'def sscan(self, name, cursor=0, match=None, count=None):\\n        \"\"\"\\n        Incrementally return lists of elements in a set. Also return a cursor\\n        indicating the scan position.\\n\\n        ``match`` allows for filtering the keys by pattern\\n\\n        ``count`` allows for hint the minimum number of returns\\n        \"\"\"\\n        pieces = [name, cursor]\\n        if match is not None:\\n            pieces.extend([Token.get_token(\\'MATCH\\'), match])\\n        if count is not None:\\n            pieces.extend([Token.get_token(\\'COUNT\\'), count])\\n        return self.execute_command(\\'SSCAN\\', *pieces)', 'def zscan(self, name, cursor=0, match=None, count=None,\\n              score_cast_func=float):\\n        \"\"\"\\n        Incrementally return lists of elements in a sorted set. Also return a\\n        cursor indicating the scan position.\\n\\n        ``match`` allows for filtering the keys by pattern\\n\\n        ``count`` allows for hint the minimum number of returns\\n\\n        ``score_cast_func`` a callable used to cast the score return value\\n        \"\"\"\\n        pieces = [name, cursor]\\n        if match is not None:\\n            pieces.extend([Token.get_token(\\'MATCH\\'), match])\\n        if count is not None:\\n            pieces.extend([Token.get_token(\\'COUNT\\'), count])\\n        options = {\\'score_cast_func\\': score_cast_func}\\n        return self.execute_command(\\'ZSCAN\\', *pieces, **options)', 'def smove(self, src, dst, value):\\n        \"Move ``value`` from set ``src`` to set ``dst`` atomically\"\\n        return self.execute_command(\\'SMOVE\\', src, dst, value)', 'def spop(self, name, count=None):\\n        \"Remove and return a random member of set ``name``\"\\n        args = (count is not None) and [count] or []\\n        return self.execute_command(\\'SPOP\\', name, *args)', 'def xack(self, name, groupname, *ids):\\n        \"\"\"\\n        Acknowledges the successful processing of one or more messages.\\n        name: name of the stream.\\n        groupname: name of the consumer group.\\n        *ids: message ids to acknowlege.\\n        \"\"\"\\n        return self.execute_command(\\'XACK\\', name, groupname, *ids)', 'def xadd(self, name, fields, id=\\'*\\', maxlen=None, approximate=True):\\n        \"\"\"\\n        Add to a stream.\\n        name: name of the stream\\n        fields: dict of field/value pairs to insert into the stream\\n        id: Location to insert this record. By default it is appended.\\n        maxlen: truncate old stream members beyond this size\\n        approximate: actual stream length may be slightly more than maxlen\\n\\n        \"\"\"\\n        pieces = []\\n        if maxlen is not None:\\n            if not isinstance(maxlen, (int, long)) or maxlen < 1:\\n                raise DataError(\\'XADD maxlen must be a positive integer\\')\\n            pieces.append(Token.get_token(\\'MAXLEN\\'))\\n            if approximate:\\n                pieces.append(Token.get_token(\\'~\\'))\\n            pieces.append(str(maxlen))\\n        pieces.append(id)\\n        if not isinstance(fields, dict) or len(fields) == 0:\\n            raise DataError(\\'XADD fields must be a non-empty dict\\')\\n        for pair in iteritems(fields):\\n            pieces.extend(pair)\\n        return self.execute_command(\\'XADD\\', name, *pieces)', 'def xclaim(self, name, groupname, consumername, min_idle_time, message_ids,\\n               idle=None, time=None, retrycount=None, force=False,\\n               justid=False):\\n        \"\"\"\\n        Changes the ownership of a pending message.\\n        name: name of the stream.\\n        groupname: name of the consumer group.\\n        consumername: name of a consumer that claims the message.\\n        min_idle_time: filter messages that were idle less than this amount of\\n        milliseconds\\n        message_ids: non-empty list or tuple of message IDs to claim\\n        idle: optional. Set the idle time (last time it was delivered) of the\\n         message in ms\\n        time: optional integer. This is the same as idle but instead of a\\n         relative amount of milliseconds, it sets the idle time to a specific\\n         Unix time (in milliseconds).\\n        retrycount: optional integer. set the retry counter to the specified\\n         value. This counter is incremented every time a message is delivered\\n         again.\\n        force: optional boolean, false by default. Creates the pending message\\n         entry in the PEL even if certain specified IDs are not already in the\\n         PEL assigned to a different client.\\n        justid: optional boolean, false by default. Return just an array of IDs\\n         of messages successfully claimed, without returning the actual message\\n        \"\"\"\\n        if not isinstance(min_idle_time, (int, long)) or min_idle_time < 0:\\n            raise DataError(\"XCLAIM min_idle_time must be a non negative \"\\n                            \"integer\")\\n        if not isinstance(message_ids, (list, tuple)) or not message_ids:\\n            raise DataError(\"XCLAIM message_ids must be a non empty list or \"\\n                            \"tuple of message IDs to claim\")\\n\\n        kwargs = {}\\n        pieces = [name, groupname, consumername, str(min_idle_time)]\\n        pieces.extend(list(message_ids))\\n\\n        if idle is not None:\\n            if not isinstance(idle, (int, long)):\\n                raise DataError(\"XCLAIM idle must be an integer\")\\n            pieces.extend((Token.get_token(\\'IDLE\\'), str(idle)))\\n        if time is not None:\\n            if not isinstance(time, (int, long)):\\n                raise DataError(\"XCLAIM time must be an integer\")\\n            pieces.extend((Token.get_token(\\'TIME\\'), str(time)))\\n        if retrycount is not None:\\n            if not isinstance(retrycount, (int, long)):\\n                raise DataError(\"XCLAIM retrycount must be an integer\")\\n            pieces.extend((Token.get_token(\\'RETRYCOUNT\\'), str(retrycount)))\\n\\n        if force:\\n            if not isinstance(force, bool):\\n                raise DataError(\"XCLAIM force must be a boolean\")\\n            pieces.append(Token.get_token(\\'FORCE\\'))\\n        if justid:\\n            if not isinstance(justid, bool):\\n                raise DataError(\"XCLAIM justid must be a boolean\")\\n            pieces.append(Token.get_token(\\'JUSTID\\'))\\n            kwargs[\\'parse_justid\\'] = True\\n        return self.execute_command(\\'XCLAIM\\', *pieces, **kwargs)', 'def xgroup_create(self, name, groupname, id=\\'$\\', mkstream=False):\\n        \"\"\"\\n        Create a new consumer group associated with a stream.\\n        name: name of the stream.\\n        groupname: name of the consumer group.\\n        id: ID of the last item in the stream to consider already delivered.\\n        \"\"\"\\n        pieces = [\\'XGROUP CREATE\\', name, groupname, id]\\n        if mkstream:\\n            pieces.append(Token.get_token(\\'MKSTREAM\\'))\\n        return self.execute_command(*pieces)', 'def xgroup_delconsumer(self, name, groupname, consumername):\\n        \"\"\"\\n        Remove a specific consumer from a consumer group.\\n        Returns the number of pending messages that the consumer had before it\\n        was deleted.\\n        name: name of the stream.\\n        groupname: name of the consumer group.\\n        consumername: name of consumer to delete\\n        \"\"\"\\n        return self.execute_command(\\'XGROUP DELCONSUMER\\', name, groupname,\\n                                    consumername)', 'def xgroup_setid(self, name, groupname, id):\\n        \"\"\"\\n        Set the consumer group last delivered ID to something else.\\n        name: name of the stream.\\n        groupname: name of the consumer group.\\n        id: ID of the last item in the stream to consider already delivered.\\n        \"\"\"\\n        return self.execute_command(\\'XGROUP SETID\\', name, groupname, id)', 'def xpending_range(self, name, groupname, min, max, count,\\n                       consumername=None):\\n        \"\"\"\\n        Returns information about pending messages, in a range.\\n        name: name of the stream.\\n        groupname: name of the consumer group.\\n        min: minimum stream ID.\\n        max: maximum stream ID.\\n        count: number of messages to return\\n        consumername: name of a consumer to filter by (optional).\\n        \"\"\"\\n        pieces = [name, groupname]\\n        if min is not None or max is not None or count is not None:\\n            if min is None or max is None or count is None:\\n                raise DataError(\"XPENDING must be provided with min, max \"\\n                                \"and count parameters, or none of them. \")\\n            if not isinstance(count, (int, long)) or count < -1:\\n                raise DataError(\"XPENDING count must be a integer >= -1\")\\n            pieces.extend((min, max, str(count)))\\n        if consumername is not None:\\n            if min is None or max is None or count is None:\\n                raise DataError(\"if XPENDING is provided with consumername,\"\\n                                \" it must be provided with min, max and\"\\n                                \" count parameters\")\\n            pieces.append(consumername)\\n        return self.execute_command(\\'XPENDING\\', *pieces, parse_detail=True)', 'def xrange(self, name, min=\\'-\\', max=\\'+\\', count=None):\\n        \"\"\"\\n        Read stream values within an interval.\\n        name: name of the stream.\\n        start: first stream ID. defaults to \\'-\\',\\n               meaning the earliest available.\\n        finish: last stream ID. defaults to \\'+\\',\\n                meaning the latest available.\\n        count: if set, only return this many items, beginning with the\\n               earliest available.\\n        \"\"\"\\n        pieces = [min, max]\\n        if count is not None:\\n            if not isinstance(count, (int, long)) or count < 1:\\n                raise DataError(\\'XRANGE count must be a positive integer\\')\\n            pieces.append(Token.get_token(\\'COUNT\\'))\\n            pieces.append(str(count))\\n\\n        return self.execute_command(\\'XRANGE\\', name, *pieces)', 'def xread(self, streams, count=None, block=None):\\n        \"\"\"\\n        Block and monitor multiple streams for new data.\\n        streams: a dict of stream names to stream IDs, where\\n                   IDs indicate the last ID already seen.\\n        count: if set, only return this many items, beginning with the\\n               earliest available.\\n        block: number of milliseconds to wait, if nothing already present.\\n        \"\"\"\\n        pieces = []\\n        if block is not None:\\n            if not isinstance(block, (int, long)) or block < 0:\\n                raise DataError(\\'XREAD block must be a non-negative integer\\')\\n            pieces.append(Token.get_token(\\'BLOCK\\'))\\n            pieces.append(str(block))\\n        if count is not None:\\n            if not isinstance(count, (int, long)) or count < 1:\\n                raise DataError(\\'XREAD count must be a positive integer\\')\\n            pieces.append(Token.get_token(\\'COUNT\\'))\\n            pieces.append(str(count))\\n        if not isinstance(streams, dict) or len(streams) == 0:\\n            raise DataError(\\'XREAD streams must be a non empty dict\\')\\n        pieces.append(Token.get_token(\\'STREAMS\\'))\\n        keys, values = izip(*iteritems(streams))\\n        pieces.extend(keys)\\n        pieces.extend(values)\\n        return self.execute_command(\\'XREAD\\', *pieces)', 'def xreadgroup(self, groupname, consumername, streams, count=None,\\n                   block=None, noack=False):\\n        \"\"\"\\n        Read from a stream via a consumer group.\\n        groupname: name of the consumer group.\\n        consumername: name of the requesting consumer.\\n        streams: a dict of stream names to stream IDs, where\\n               IDs indicate the last ID already seen.\\n        count: if set, only return this many items, beginning with the\\n               earliest available.\\n        block: number of milliseconds to wait, if nothing already present.\\n        noack: do not add messages to the PEL\\n        \"\"\"\\n        pieces = [Token.get_token(\\'GROUP\\'), groupname, consumername]\\n        if count is not None:\\n            if not isinstance(count, (int, long)) or count < 1:\\n                raise DataError(\"XREADGROUP count must be a positive integer\")\\n            pieces.append(Token.get_token(\"COUNT\"))\\n            pieces.append(str(count))\\n        if block is not None:\\n            if not isinstance(block, (int, long)) or block < 0:\\n                raise DataError(\"XREADGROUP block must be a non-negative \"\\n                                \"integer\")\\n            pieces.append(Token.get_token(\"BLOCK\"))\\n            pieces.append(str(block))\\n        if noack:\\n            pieces.append(Token.get_token(\"NOACK\"))\\n        if not isinstance(streams, dict) or len(streams) == 0:\\n            raise DataError(\\'XREADGROUP streams must be a non empty dict\\')\\n        pieces.append(Token.get_token(\\'STREAMS\\'))\\n        pieces.extend(streams.keys())\\n        pieces.extend(streams.values())\\n        return self.execute_command(\\'XREADGROUP\\', *pieces)', 'def xtrim(self, name, maxlen, approximate=True):\\n        \"\"\"\\n        Trims old messages from a stream.\\n        name: name of the stream.\\n        maxlen: truncate old stream messages beyond this size\\n        approximate: actual stream length may be slightly more than maxlen\\n        \"\"\"\\n        pieces = [Token.get_token(\\'MAXLEN\\')]\\n        if approximate:\\n            pieces.append(Token.get_token(\\'~\\'))\\n        pieces.append(maxlen)\\n        return self.execute_command(\\'XTRIM\\', name, *pieces)', 'def zadd(self, name, mapping, nx=False, xx=False, ch=False, incr=False):\\n        \"\"\"\\n        Set any number of element-name, score pairs to the key ``name``. Pairs\\n        are specified as a dict of element-names keys to score values.\\n\\n        ``nx`` forces ZADD to only create new elements and not to update\\n        scores for elements that already exist.\\n\\n        ``xx`` forces ZADD to only update scores of elements that already\\n        exist. New elements will not be added.\\n\\n        ``ch`` modifies the return value to be the numbers of elements changed.\\n        Changed elements include new elements that were added and elements\\n        whose scores changed.\\n\\n        ``incr`` modifies ZADD to behave like ZINCRBY. In this mode only a\\n        single element/score pair can be specified and the score is the amount\\n        the existing score will be incremented by. When using this mode the\\n        return value of ZADD will be the new score of the element.\\n\\n        The return value of ZADD varies based on the mode specified. With no\\n        options, ZADD returns the number of new elements added to the sorted\\n        set.\\n        \"\"\"\\n        if not mapping:\\n            raise DataError(\"ZADD requires at least one element/score pair\")\\n        if nx and xx:\\n            raise DataError(\"ZADD allows either \\'nx\\' or \\'xx\\', not both\")\\n        if incr and len(mapping) != 1:\\n            raise DataError(\"ZADD option \\'incr\\' only works when passing a \"\\n                            \"single element/score pair\")\\n        pieces = []\\n        options = {}\\n        if nx:\\n            pieces.append(Token.get_token(\\'NX\\'))\\n        if xx:\\n            pieces.append(Token.get_token(\\'XX\\'))\\n        if ch:\\n            pieces.append(Token.get_token(\\'CH\\'))\\n        if incr:\\n            pieces.append(Token.get_token(\\'INCR\\'))\\n            options[\\'as_score\\'] = True\\n        for pair in iteritems(mapping):\\n            pieces.append(pair[1])\\n            pieces.append(pair[0])\\n        return self.execute_command(\\'ZADD\\', name, *pieces, **options)', 'def zcount(self, name, min, max):\\n        \"\"\"\\n        Returns the number of elements in the sorted set at key ``name`` with\\n        a score between ``min`` and ``max``.\\n        \"\"\"\\n        return self.execute_command(\\'ZCOUNT\\', name, min, max)', 'def zincrby(self, name, amount, value):\\n        \"Increment the score of ``value`` in sorted set ``name`` by ``amount``\"\\n        return self.execute_command(\\'ZINCRBY\\', name, amount, value)', 'def zinterstore(self, dest, keys, aggregate=None):\\n        \"\"\"\\n        Intersect multiple sorted sets specified by ``keys`` into\\n        a new sorted set, ``dest``. Scores in the destination will be\\n        aggregated based on the ``aggregate``, or SUM if none is provided.\\n        \"\"\"\\n        return self._zaggregate(\\'ZINTERSTORE\\', dest, keys, aggregate)', 'def zlexcount(self, name, min, max):\\n        \"\"\"\\n        Return the number of items in the sorted set ``name`` between the\\n        lexicographical range ``min`` and ``max``.\\n        \"\"\"\\n        return self.execute_command(\\'ZLEXCOUNT\\', name, min, max)', 'def zpopmax(self, name, count=None):\\n        \"\"\"\\n        Remove and return up to ``count`` members with the highest scores\\n        from the sorted set ``name``.\\n        \"\"\"\\n        args = (count is not None) and [count] or []\\n        options = {\\n            \\'withscores\\': True\\n        }\\n        return self.execute_command(\\'ZPOPMAX\\', name, *args, **options)', 'def zpopmin(self, name, count=None):\\n        \"\"\"\\n        Remove and return up to ``count`` members with the lowest scores\\n        from the sorted set ``name``.\\n        \"\"\"\\n        args = (count is not None) and [count] or []\\n        options = {\\n            \\'withscores\\': True\\n        }\\n        return self.execute_command(\\'ZPOPMIN\\', name, *args, **options)', 'def bzpopmax(self, keys, timeout=0):\\n        \"\"\"\\n        ZPOPMAX a value off of the first non-empty sorted set\\n        named in the ``keys`` list.\\n\\n        If none of the sorted sets in ``keys`` has a value to ZPOPMAX,\\n        then block for ``timeout`` seconds, or until a member gets added\\n        to one of the sorted sets.\\n\\n        If timeout is 0, then block indefinitely.\\n        \"\"\"\\n        if timeout is None:\\n            timeout = 0\\n        keys = list_or_args(keys, None)\\n        keys.append(timeout)\\n        return self.execute_command(\\'BZPOPMAX\\', *keys)', 'def bzpopmin(self, keys, timeout=0):\\n        \"\"\"\\n        ZPOPMIN a value off of the first non-empty sorted set\\n        named in the ``keys`` list.\\n\\n        If none of the sorted sets in ``keys`` has a value to ZPOPMIN,\\n        then block for ``timeout`` seconds, or until a member gets added\\n        to one of the sorted sets.\\n\\n        If timeout is 0, then block indefinitely.\\n        \"\"\"\\n        if timeout is None:\\n            timeout = 0\\n        keys = list_or_args(keys, None)\\n        keys.append(timeout)\\n        return self.execute_command(\\'BZPOPMIN\\', *keys)', 'def zrangebylex(self, name, min, max, start=None, num=None):\\n        \"\"\"\\n        Return the lexicographical range of values from sorted set ``name``\\n        between ``min`` and ``max``.\\n\\n        If ``start`` and ``num`` are specified, then return a slice of the\\n        range.\\n        \"\"\"\\n        if (start is not None and num is None) or \\\\\\n                (num is not None and start is None):\\n            raise DataError(\"``start`` and ``num`` must both be specified\")\\n        pieces = [\\'ZRANGEBYLEX\\', name, min, max]\\n        if start is not None and num is not None:\\n            pieces.extend([Token.get_token(\\'LIMIT\\'), start, num])\\n        return self.execute_command(*pieces)', 'def zremrangebylex(self, name, min, max):\\n        \"\"\"\\n        Remove all elements in the sorted set ``name`` between the\\n        lexicographical range specified by ``min`` and ``max``.\\n\\n        Returns the number of elements removed.\\n        \"\"\"\\n        return self.execute_command(\\'ZREMRANGEBYLEX\\', name, min, max)', 'def zremrangebyrank(self, name, min, max):\\n        \"\"\"\\n        Remove all elements in the sorted set ``name`` with ranks between\\n        ``min`` and ``max``. Values are 0-based, ordered from smallest score\\n        to largest. Values can be negative indicating the highest scores.\\n        Returns the number of elements removed\\n        \"\"\"\\n        return self.execute_command(\\'ZREMRANGEBYRANK\\', name, min, max)', 'def zrevrangebyscore(self, name, max, min, start=None, num=None,\\n                         withscores=False, score_cast_func=float):\\n        \"\"\"\\n        Return a range of values from the sorted set ``name`` with scores\\n        between ``min`` and ``max`` in descending order.\\n\\n        If ``start`` and ``num`` are specified, then return a slice\\n        of the range.\\n\\n        ``withscores`` indicates to return the scores along with the values.\\n        The return type is a list of (value, score) pairs\\n\\n        ``score_cast_func`` a callable used to cast the score return value\\n        \"\"\"\\n        if (start is not None and num is None) or \\\\\\n                (num is not None and start is None):\\n            raise DataError(\"``start`` and ``num`` must both be specified\")\\n        pieces = [\\'ZREVRANGEBYSCORE\\', name, max, min]\\n        if start is not None and num is not None:\\n            pieces.extend([Token.get_token(\\'LIMIT\\'), start, num])\\n        if withscores:\\n            pieces.append(Token.get_token(\\'WITHSCORES\\'))\\n        options = {\\n            \\'withscores\\': withscores,\\n            \\'score_cast_func\\': score_cast_func\\n        }\\n        return self.execute_command(*pieces, **options)', 'def zunionstore(self, dest, keys, aggregate=None):\\n        \"\"\"\\n        Union multiple sorted sets specified by ``keys`` into\\n        a new sorted set, ``dest``. Scores in the destination will be\\n        aggregated based on the ``aggregate``, or SUM if none is provided.\\n        \"\"\"\\n        return self._zaggregate(\\'ZUNIONSTORE\\', dest, keys, aggregate)', 'def hmget(self, name, keys, *args):\\n        \"Returns a list of values ordered identically to ``keys``\"\\n        args = list_or_args(keys, args)\\n        return self.execute_command(\\'HMGET\\', name, *args)', 'def eval(self, script, numkeys, *keys_and_args):\\n        \"\"\"\\n        Execute the Lua ``script``, specifying the ``numkeys`` the script\\n        will touch and the key names and argument values in ``keys_and_args``.\\n        Returns the result of the script.\\n\\n        In practice, use the object returned by ``register_script``. This\\n        function exists purely for Redis API completion.\\n        \"\"\"\\n        return self.execute_command(\\'EVAL\\', script, numkeys, *keys_and_args)', 'def evalsha(self, sha, numkeys, *keys_and_args):\\n        \"\"\"\\n        Use the ``sha`` to execute a Lua script already registered via EVAL\\n        or SCRIPT LOAD. Specify the ``numkeys`` the script will touch and the\\n        key names and argument values in ``keys_and_args``. Returns the result\\n        of the script.\\n\\n        In practice, use the object returned by ``register_script``. This\\n        function exists purely for Redis API completion.\\n        \"\"\"\\n        return self.execute_command(\\'EVALSHA\\', sha, numkeys, *keys_and_args)', 'def geoadd(self, name, *values):\\n        \"\"\"\\n        Add the specified geospatial items to the specified key identified\\n        by the ``name`` argument. The Geospatial items are given as ordered\\n        members of the ``values`` argument, each item or place is formed by\\n        the triad longitude, latitude and name.\\n        \"\"\"\\n        if len(values) % 3 != 0:\\n            raise DataError(\"GEOADD requires places with lon, lat and name\"\\n                            \" values\")\\n        return self.execute_command(\\'GEOADD\\', name, *values)', 'def geodist(self, name, place1, place2, unit=None):\\n        \"\"\"\\n        Return the distance between ``place1`` and ``place2`` members of the\\n        ``name`` key.\\n        The units must be one of the following : m, km mi, ft. By default\\n        meters are used.\\n        \"\"\"\\n        pieces = [name, place1, place2]\\n        if unit and unit not in (\\'m\\', \\'km\\', \\'mi\\', \\'ft\\'):\\n            raise DataError(\"GEODIST invalid unit\")\\n        elif unit:\\n            pieces.append(unit)\\n        return self.execute_command(\\'GEODIST\\', *pieces)', 'def on_connect(self, connection):\\n        \"Re-subscribe to any channels and patterns previously subscribed to\"\\n        # NOTE: for python3, we can\\'t pass bytestrings as keyword arguments\\n        # so we need to decode channel/pattern names back to unicode strings\\n        # before passing them to [p]subscribe.\\n        self.pending_unsubscribe_channels.clear()\\n        self.pending_unsubscribe_patterns.clear()\\n        if self.channels:\\n            channels = {}\\n            for k, v in iteritems(self.channels):\\n                channels[self.encoder.decode(k, force=True)] = v\\n            self.subscribe(**channels)\\n        if self.patterns:\\n            patterns = {}\\n            for k, v in iteritems(self.patterns):\\n                patterns[self.encoder.decode(k, force=True)] = v\\n            self.psubscribe(**patterns)', 'def parse_response(self, block=True, timeout=0):\\n        \"Parse the response from a publish/subscribe command\"\\n        connection = self.connection\\n        if connection is None:\\n            raise RuntimeError(\\n                \\'pubsub connection not set: \\'\\n                \\'did you forget to call subscribe() or psubscribe()?\\')\\n        if not block and not connection.can_read(timeout=timeout):\\n            return None\\n        return self._execute(connection, connection.read_response)', 'def _normalize_keys(self, data):\\n        \"\"\"\\n        normalize channel/pattern names to be either bytes or strings\\n        based on whether responses are automatically decoded. this saves us\\n        from coercing the value for each message coming in.\\n        \"\"\"\\n        encode = self.encoder.encode\\n        decode = self.encoder.decode\\n        return {decode(encode(k)): v for k, v in iteritems(data)}', 'def psubscribe(self, *args, **kwargs):\\n        \"\"\"\\n        Subscribe to channel patterns. Patterns supplied as keyword arguments\\n        expect a pattern name as the key and a callable as the value. A\\n        pattern\\'s callable will be invoked automatically when a message is\\n        received on that pattern rather than producing a message via\\n        ``listen()``.\\n        \"\"\"\\n        if args:\\n            args = list_or_args(args[0], args[1:])\\n        new_patterns = dict.fromkeys(args)\\n        new_patterns.update(kwargs)\\n        ret_val = self.execute_command(\\'PSUBSCRIBE\\', *iterkeys(new_patterns))\\n        # update the patterns dict AFTER we send the command. we don\\'t want to\\n        # subscribe twice to these patterns, once for the command and again\\n        # for the reconnection.\\n        new_patterns = self._normalize_keys(new_patterns)\\n        self.patterns.update(new_patterns)\\n        self.pending_unsubscribe_patterns.difference_update(new_patterns)\\n        return ret_val', 'def punsubscribe(self, *args):\\n        \"\"\"\\n        Unsubscribe from the supplied patterns. If empty, unsubscribe from\\n        all patterns.\\n        \"\"\"\\n        if args:\\n            args = list_or_args(args[0], args[1:])\\n            patterns = self._normalize_keys(dict.fromkeys(args))\\n        else:\\n            patterns = self.patterns\\n        self.pending_unsubscribe_patterns.update(patterns)\\n        return self.execute_command(\\'PUNSUBSCRIBE\\', *args)', 'def subscribe(self, *args, **kwargs):\\n        \"\"\"\\n        Subscribe to channels. Channels supplied as keyword arguments expect\\n        a channel name as the key and a callable as the value. A channel\\'s\\n        callable will be invoked automatically when a message is received on\\n        that channel rather than producing a message via ``listen()`` or\\n        ``get_message()``.\\n        \"\"\"\\n        if args:\\n            args = list_or_args(args[0], args[1:])\\n        new_channels = dict.fromkeys(args)\\n        new_channels.update(kwargs)\\n        ret_val = self.execute_command(\\'SUBSCRIBE\\', *iterkeys(new_channels))\\n        # update the channels dict AFTER we send the command. we don\\'t want to\\n        # subscribe twice to these channels, once for the command and again\\n        # for the reconnection.\\n        new_channels = self._normalize_keys(new_channels)\\n        self.channels.update(new_channels)\\n        self.pending_unsubscribe_channels.difference_update(new_channels)\\n        return ret_val', 'def unsubscribe(self, *args):\\n        \"\"\"\\n        Unsubscribe from the supplied channels. If empty, unsubscribe from\\n        all channels\\n        \"\"\"\\n        if args:\\n            args = list_or_args(args[0], args[1:])\\n            channels = self._normalize_keys(dict.fromkeys(args))\\n        else:\\n            channels = self.channels\\n        self.pending_unsubscribe_channels.update(channels)\\n        return self.execute_command(\\'UNSUBSCRIBE\\', *args)', 'def handle_message(self, response, ignore_subscribe_messages=False):\\n        \"\"\"\\n        Parses a pub/sub message. If the channel or pattern was subscribed to\\n        with a message handler, the handler is invoked instead of a parsed\\n        message being returned.\\n        \"\"\"\\n        message_type = nativestr(response[0])\\n        if message_type == \\'pmessage\\':\\n            message = {\\n                \\'type\\': message_type,\\n                \\'pattern\\': response[1],\\n                \\'channel\\': response[2],\\n                \\'data\\': response[3]\\n            }\\n        elif message_type == \\'pong\\':\\n            message = {\\n                \\'type\\': message_type,\\n                \\'pattern\\': None,\\n                \\'channel\\': None,\\n                \\'data\\': response[1]\\n            }\\n        else:\\n            message = {\\n                \\'type\\': message_type,\\n                \\'pattern\\': None,\\n                \\'channel\\': response[1],\\n                \\'data\\': response[2]\\n            }\\n\\n        # if this is an unsubscribe message, remove it from memory\\n        if message_type in self.UNSUBSCRIBE_MESSAGE_TYPES:\\n            if message_type == \\'punsubscribe\\':\\n                pattern = response[1]\\n                if pattern in self.pending_unsubscribe_patterns:\\n                    self.pending_unsubscribe_patterns.remove(pattern)\\n                    self.patterns.pop(pattern, None)\\n            else:\\n                channel = response[1]\\n                if channel in self.pending_unsubscribe_channels:\\n                    self.pending_unsubscribe_channels.remove(channel)\\n                    self.channels.pop(channel, None)\\n\\n        if message_type in self.PUBLISH_MESSAGE_TYPES:\\n            # if there\\'s a message handler, invoke it\\n            if message_type == \\'pmessage\\':\\n                handler = self.patterns.get(message[\\'pattern\\'], None)\\n            else:\\n                handler = self.channels.get(message[\\'channel\\'], None)\\n            if handler:\\n                handler(message)\\n                return None\\n        elif message_type != \\'pong\\':\\n            # this is a subscribe/unsubscribe message. ignore if we don\\'t\\n            # want them\\n            if ignore_subscribe_messages or self.ignore_subscribe_messages:\\n                return None\\n\\n        return message', 'def multi(self):\\n        \"\"\"\\n        Start a transactional block of the pipeline after WATCH commands\\n        are issued. End the transactional block with `execute`.\\n        \"\"\"\\n        if self.explicit_transaction:\\n            raise RedisError(\\'Cannot issue nested calls to MULTI\\')\\n        if self.command_stack:\\n            raise RedisError(\\'Commands without an initial WATCH have already \\'\\n                             \\'been issued\\')\\n        self.explicit_transaction = True', 'def pipeline_execute_command(self, *args, **options):\\n        \"\"\"\\n        Stage a command to be executed when execute() is next called\\n\\n        Returns the current Pipeline object back so commands can be\\n        chained together, such as:\\n\\n        pipe = pipe.set(\\'foo\\', \\'bar\\').incr(\\'baz\\').decr(\\'bang\\')\\n\\n        At some other point, you can then run: pipe.execute(),\\n        which will execute all commands queued in the pipe.\\n        \"\"\"\\n        self.command_stack.append((args, options))\\n        return self', 'def reset(self):\\n        \"\"\"\\n        Reset the state of the instance to when it was constructed\\n        \"\"\"\\n        self.operations = []\\n        self._last_overflow = \\'WRAP\\'\\n        self.overflow(self._default_overflow or self._last_overflow)', 'def overflow(self, overflow):\\n        \"\"\"\\n        Update the overflow algorithm of successive INCRBY operations\\n        :param overflow: Overflow algorithm, one of WRAP, SAT, FAIL. See the\\n            Redis docs for descriptions of these algorithmsself.\\n        :returns: a :py:class:`BitFieldOperation` instance.\\n        \"\"\"\\n        overflow = overflow.upper()\\n        if overflow != self._last_overflow:\\n            self._last_overflow = overflow\\n            self.operations.append((\\'OVERFLOW\\', overflow))\\n        return self', 'def get(self, fmt, offset):\\n        \"\"\"\\n        Get the value of a given bitfield.\\n        :param fmt: format-string for the bitfield being read, e.g. \\'u8\\' for\\n            an unsigned 8-bit integer.\\n        :param offset: offset (in number of bits). If prefixed with a\\n            \\'#\\', this is an offset multiplier, e.g. given the arguments\\n            fmt=\\'u8\\', offset=\\'#2\\', the offset will be 16.\\n        :returns: a :py:class:`BitFieldOperation` instance.\\n        \"\"\"\\n        self.operations.append((\\'GET\\', fmt, offset))\\n        return self', 'def set(self, fmt, offset, value):\\n        \"\"\"\\n        Set the value of a given bitfield.\\n        :param fmt: format-string for the bitfield being read, e.g. \\'u8\\' for\\n            an unsigned 8-bit integer.\\n        :param offset: offset (in number of bits). If prefixed with a\\n            \\'#\\', this is an offset multiplier, e.g. given the arguments\\n            fmt=\\'u8\\', offset=\\'#2\\', the offset will be 16.\\n        :param int value: value to set at the given position.\\n        :returns: a :py:class:`BitFieldOperation` instance.\\n        \"\"\"\\n        self.operations.append((\\'SET\\', fmt, offset, value))\\n        return self', 'def execute(self):\\n        \"\"\"\\n        Execute the operation(s) in a single BITFIELD command. The return value\\n        is a list of values corresponding to each operation. If the client\\n        used to create this instance was a pipeline, the list of values\\n        will be present within the pipeline\\'s execute.\\n        \"\"\"\\n        command = self.command\\n        self.reset()\\n        return self.client.execute_command(*command)', 'def get_token(cls, value):\\n        \"Gets a cached token object or creates a new one if not already cached\"\\n\\n        # Use try/except because after running for a short time most tokens\\n        # should already be cached\\n        try:\\n            return cls._cache[value]\\n        except KeyError:\\n            token = Token(value)\\n            cls._cache[value] = token\\n            return token', 'def encode(self, value):\\n        \"Return a bytestring representation of the value\"\\n        if isinstance(value, Token):\\n            return value.encoded_value\\n        elif isinstance(value, bytes):\\n            return value\\n        elif isinstance(value, bool):\\n            # special case bool since it is a subclass of int\\n            raise DataError(\"Invalid input of type: \\'bool\\'. Convert to a \"\\n                            \"byte, string or number first.\")\\n        elif isinstance(value, float):\\n            value = repr(value).encode()\\n        elif isinstance(value, (int, long)):\\n            # python 2 repr() on longs is \\'123L\\', so use str() instead\\n            value = str(value).encode()\\n        elif not isinstance(value, basestring):\\n            # a value we don\\'t know how to deal with. throw an error\\n            typename = type(value).__name__\\n            raise DataError(\"Invalid input of type: \\'%s\\'. Convert to a \"\\n                            \"byte, string or number first.\" % typename)\\n        if isinstance(value, unicode):\\n            value = value.encode(self.encoding, self.encoding_errors)\\n        return value', 'def decode(self, value, force=False):\\n        \"Return a unicode string from the byte representation\"\\n        if (self.decode_responses or force) and isinstance(value, bytes):\\n            value = value.decode(self.encoding, self.encoding_errors)\\n        return value', 'def parse_error(self, response):\\n        \"Parse an error response\"\\n        error_code = response.split(\\' \\')[0]\\n        if error_code in self.EXCEPTION_CLASSES:\\n            response = response[len(error_code) + 1:]\\n            exception_class = self.EXCEPTION_CLASSES[error_code]\\n            if isinstance(exception_class, dict):\\n                exception_class = exception_class.get(response, ResponseError)\\n            return exception_class(response)\\n        return ResponseError(response)', 'def on_connect(self, connection):\\n        \"Called when the socket connects\"\\n        self._sock = connection._sock\\n        self._buffer = SocketBuffer(self._sock, self.socket_read_size)\\n        self.encoder = connection.encoder', 'def on_disconnect(self):\\n        \"Called when the socket disconnects\"\\n        self._sock = None\\n        if self._buffer is not None:\\n            self._buffer.close()\\n            self._buffer = None\\n        self.encoder = None', 'def connect(self):\\n        \"Connects to the Redis server if not already connected\"\\n        if self._sock:\\n            return\\n        try:\\n            sock = self._connect()\\n        except socket.timeout:\\n            raise TimeoutError(\"Timeout connecting to server\")\\n        except socket.error:\\n            e = sys.exc_info()[1]\\n            raise ConnectionError(self._error_message(e))\\n\\n        self._sock = sock\\n        self._selector = DefaultSelector(sock)\\n        try:\\n            self.on_connect()\\n        except RedisError:\\n            # clean up after any error in on_connect\\n            self.disconnect()\\n            raise\\n\\n        # run any user callbacks. right now the only internal callback\\n        # is for pubsub channel/pattern resubscription\\n        for callback in self._connect_callbacks:\\n            callback(self)', 'def disconnect(self):\\n        \"Disconnects from the Redis server\"\\n        self._parser.on_disconnect()\\n        if self._sock is None:\\n            return\\n        if self._selector is not None:\\n            self._selector.close()\\n            self._selector = None\\n        try:\\n            if os.getpid() == self.pid:\\n                self._sock.shutdown(socket.SHUT_RDWR)\\n            self._sock.close()\\n        except socket.error:\\n            pass\\n        self._sock = None', 'def send_packed_command(self, command):\\n        \"Send an already packed command to the Redis server\"\\n        if not self._sock:\\n            self.connect()\\n        try:\\n            if isinstance(command, str):\\n                command = [command]\\n            for item in command:\\n                self._sock.sendall(item)\\n        except socket.timeout:\\n            self.disconnect()\\n            raise TimeoutError(\"Timeout writing to socket\")\\n        except socket.error:\\n            e = sys.exc_info()[1]\\n            self.disconnect()\\n            if len(e.args) == 1:\\n                errno, errmsg = \\'UNKNOWN\\', e.args[0]\\n            else:\\n                errno = e.args[0]\\n                errmsg = e.args[1]\\n            raise ConnectionError(\"Error %s while writing to socket. %s.\" %\\n                                  (errno, errmsg))\\n        except:  # noqa: E722\\n            self.disconnect()\\n            raise', 'def can_read(self, timeout=0):\\n        \"Poll the socket to see if there\\'s data that can be read.\"\\n        sock = self._sock\\n        if not sock:\\n            self.connect()\\n            sock = self._sock\\n        return self._parser.can_read() or self._selector.can_read(timeout)', 'def read_response(self):\\n        \"Read the response from a previously sent command\"\\n        try:\\n            response = self._parser.read_response()\\n        except socket.timeout:\\n            self.disconnect()\\n            raise TimeoutError(\"Timeout reading from %s:%s\" %\\n                               (self.host, self.port))\\n        except socket.error:\\n            self.disconnect()\\n            e = sys.exc_info()[1]\\n            raise ConnectionError(\"Error while reading from %s:%s : %s\" %\\n                                  (self.host, self.port, e.args))\\n        except:  # noqa: E722\\n            self.disconnect()\\n            raise\\n        if isinstance(response, ResponseError):\\n            raise response\\n        return response', 'def pack_command(self, *args):\\n        \"Pack a series of arguments into the Redis protocol\"\\n        output = []\\n        # the client might have included 1 or more literal arguments in\\n        # the command name, e.g., \\'CONFIG GET\\'. The Redis server expects these\\n        # arguments to be sent separately, so split the first argument\\n        # manually. All of these arguements get wrapped in the Token class\\n        # to prevent them from being encoded.\\n        command = args[0]\\n        if \\' \\' in command:\\n            args = tuple(Token.get_token(s)\\n                         for s in command.split()) + args[1:]\\n        else:\\n            args = (Token.get_token(command),) + args[1:]\\n\\n        buff = SYM_EMPTY.join((SYM_STAR, str(len(args)).encode(), SYM_CRLF))\\n\\n        buffer_cutoff = self._buffer_cutoff\\n        for arg in imap(self.encoder.encode, args):\\n            # to avoid large string mallocs, chunk the command into the\\n            # output list if we\\'re sending large values\\n            if len(buff) > buffer_cutoff or len(arg) > buffer_cutoff:\\n                buff = SYM_EMPTY.join(\\n                    (buff, SYM_DOLLAR, str(len(arg)).encode(), SYM_CRLF))\\n                output.append(buff)\\n                output.append(arg)\\n                buff = SYM_CRLF\\n            else:\\n                buff = SYM_EMPTY.join(\\n                    (buff, SYM_DOLLAR, str(len(arg)).encode(),\\n                     SYM_CRLF, arg, SYM_CRLF))\\n        output.append(buff)\\n        return output', 'def _connect(self):\\n        \"Wrap the socket with SSL support\"\\n        sock = super(SSLConnection, self)._connect()\\n        if hasattr(ssl, \"create_default_context\"):\\n            context = ssl.create_default_context()\\n            context.check_hostname = False\\n            context.verify_mode = self.cert_reqs\\n            if self.certfile and self.keyfile:\\n                context.load_cert_chain(certfile=self.certfile,\\n                                        keyfile=self.keyfile)\\n            if self.ca_certs:\\n                context.load_verify_locations(self.ca_certs)\\n            sock = context.wrap_socket(sock, server_hostname=self.host)\\n        else:\\n            # In case this code runs in a version which is older than 2.7.9,\\n            # we want to fall back to old code\\n            sock = ssl.wrap_socket(sock,\\n                                   cert_reqs=self.cert_reqs,\\n                                   keyfile=self.keyfile,\\n                                   certfile=self.certfile,\\n                                   ca_certs=self.ca_certs)\\n        return sock', 'def _connect(self):\\n        \"Create a Unix domain socket connection\"\\n        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\\n        sock.settimeout(self.socket_timeout)\\n        sock.connect(self.path)\\n        return sock', 'def from_url(cls, url, db=None, decode_components=False, **kwargs):\\n        \"\"\"\\n        Return a connection pool configured from the given URL.\\n\\n        For example::\\n\\n            redis://[:password]@localhost:6379/0\\n            rediss://[:password]@localhost:6379/0\\n            unix://[:password]@/path/to/socket.sock?db=0\\n\\n        Three URL schemes are supported:\\n\\n        - ```redis://``\\n          <https://www.iana.org/assignments/uri-schemes/prov/redis>`_ creates a\\n          normal TCP socket connection\\n        - ```rediss://``\\n          <https://www.iana.org/assignments/uri-schemes/prov/rediss>`_ creates\\n          a SSL wrapped TCP socket connection\\n        - ``unix://`` creates a Unix Domain Socket connection\\n\\n        There are several ways to specify a database number. The parse function\\n        will return the first specified option:\\n            1. A ``db`` querystring option, e.g. redis://localhost?db=0\\n            2. If using the redis:// scheme, the path argument of the url, e.g.\\n               redis://localhost/0\\n            3. The ``db`` argument to this function.\\n\\n        If none of these options are specified, db=0 is used.\\n\\n        The ``decode_components`` argument allows this function to work with\\n        percent-encoded URLs. If this argument is set to ``True`` all ``%xx``\\n        escapes will be replaced by their single-character equivalents after\\n        the URL has been parsed. This only applies to the ``hostname``,\\n        ``path``, and ``password`` components.\\n\\n        Any additional querystring arguments and keyword arguments will be\\n        passed along to the ConnectionPool class\\'s initializer. The querystring\\n        arguments ``socket_connect_timeout`` and ``socket_timeout`` if supplied\\n        are parsed as float values. The arguments ``socket_keepalive`` and\\n        ``retry_on_timeout`` are parsed to boolean values that accept\\n        True/False, Yes/No values to indicate state. Invalid types cause a\\n        ``UserWarning`` to be raised. In the case of conflicting arguments,\\n        querystring arguments always win.\\n\\n        \"\"\"\\n        url = urlparse(url)\\n        url_options = {}\\n\\n        for name, value in iteritems(parse_qs(url.query)):\\n            if value and len(value) > 0:\\n                parser = URL_QUERY_ARGUMENT_PARSERS.get(name)\\n                if parser:\\n                    try:\\n                        url_options[name] = parser(value[0])\\n                    except (TypeError, ValueError):\\n                        warnings.warn(UserWarning(\\n                            \"Invalid value for `%s` in connection URL.\" % name\\n                        ))\\n                else:\\n                    url_options[name] = value[0]\\n\\n        if decode_components:\\n            password = unquote(url.password) if url.password else None\\n            path = unquote(url.path) if url.path else None\\n            hostname = unquote(url.hostname) if url.hostname else None\\n        else:\\n            password = url.password\\n            path = url.path\\n            hostname = url.hostname\\n\\n        # We only support redis://, rediss:// and unix:// schemes.\\n        if url.scheme == \\'unix\\':\\n            url_options.update({\\n                \\'password\\': password,\\n                \\'path\\': path,\\n                \\'connection_class\\': UnixDomainSocketConnection,\\n            })\\n\\n        elif url.scheme in (\\'redis\\', \\'rediss\\'):\\n            url_options.update({\\n                \\'host\\': hostname,\\n                \\'port\\': int(url.port or 6379),\\n                \\'password\\': password,\\n            })\\n\\n            # If there\\'s a path argument, use it as the db argument if a\\n            # querystring value wasn\\'t specified\\n            if \\'db\\' not in url_options and path:\\n                try:\\n                    url_options[\\'db\\'] = int(path.replace(\\'/\\', \\'\\'))\\n                except (AttributeError, ValueError):\\n                    pass\\n\\n            if url.scheme == \\'rediss\\':\\n                url_options[\\'connection_class\\'] = SSLConnection\\n        else:\\n            valid_schemes = \\', \\'.join((\\'redis://\\', \\'rediss://\\', \\'unix://\\'))\\n            raise ValueError(\\'Redis URL must specify one of the following\\'\\n                             \\'schemes (%s)\\' % valid_schemes)\\n\\n        # last shot at the db value\\n        url_options[\\'db\\'] = int(url_options.get(\\'db\\', db or 0))\\n\\n        # update the arguments from the URL values\\n        kwargs.update(url_options)\\n\\n        # backwards compatability\\n        if \\'charset\\' in kwargs:\\n            warnings.warn(DeprecationWarning(\\n                \\'\"charset\" is deprecated. Use \"encoding\" instead\\'))\\n            kwargs[\\'encoding\\'] = kwargs.pop(\\'charset\\')\\n        if \\'errors\\' in kwargs:\\n            warnings.warn(DeprecationWarning(\\n                \\'\"errors\" is deprecated. Use \"encoding_errors\" instead\\'))\\n            kwargs[\\'encoding_errors\\'] = kwargs.pop(\\'errors\\')\\n\\n        return cls(**kwargs)', 'def get_connection(self, command_name, *keys, **options):\\n        \"Get a connection from the pool\"\\n        self._checkpid()\\n        try:\\n            connection = self._available_connections.pop()\\n        except IndexError:\\n            connection = self.make_connection()\\n        self._in_use_connections.add(connection)\\n        try:\\n            # ensure this connection is connected to Redis\\n            connection.connect()\\n            # connections that the pool provides should be ready to send\\n            # a command. if not, the connection was either returned to the\\n            # pool before all data has been read or the socket has been\\n            # closed. either way, reconnect and verify everything is good.\\n            if not connection.is_ready_for_command():\\n                connection.disconnect()\\n                connection.connect()\\n                if not connection.is_ready_for_command():\\n                    raise ConnectionError(\\'Connection not ready\\')\\n        except:  # noqa: E722\\n            # release the connection back to the pool so that we don\\'t leak it\\n            self.release(connection)\\n            raise\\n\\n        return connection', 'def get_encoder(self):\\n        \"Return an encoder based on encoding settings\"\\n        kwargs = self.connection_kwargs\\n        return Encoder(\\n            encoding=kwargs.get(\\'encoding\\', \\'utf-8\\'),\\n            encoding_errors=kwargs.get(\\'encoding_errors\\', \\'strict\\'),\\n            decode_responses=kwargs.get(\\'decode_responses\\', False)\\n        )', 'def make_connection(self):\\n        \"Create a new connection\"\\n        if self._created_connections >= self.max_connections:\\n            raise ConnectionError(\"Too many connections\")\\n        self._created_connections += 1\\n        return self.connection_class(**self.connection_kwargs)', 'def disconnect(self):\\n        \"Disconnects all connections in the pool\"\\n        self._checkpid()\\n        all_conns = chain(self._available_connections,\\n                          self._in_use_connections)\\n        for connection in all_conns:\\n            connection.disconnect()', 'def make_connection(self):\\n        \"Make a fresh connection.\"\\n        connection = self.connection_class(**self.connection_kwargs)\\n        self._connections.append(connection)\\n        return connection', 'def get_connection(self, command_name, *keys, **options):\\n        \"\"\"\\n        Get a connection, blocking for ``self.timeout`` until a connection\\n        is available from the pool.\\n\\n        If the connection returned is ``None`` then creates a new connection.\\n        Because we use a last-in first-out queue, the existing connections\\n        (having been returned to the pool after the initial ``None`` values\\n        were added) will be returned before ``None`` values. This means we only\\n        create new connections when we need to, i.e.: the actual number of\\n        connections will only increase in response to demand.\\n        \"\"\"\\n        # Make sure we haven\\'t changed process.\\n        self._checkpid()\\n\\n        # Try and get a connection from the pool. If one isn\\'t available within\\n        # self.timeout then raise a ``ConnectionError``.\\n        connection = None\\n        try:\\n            connection = self.pool.get(block=True, timeout=self.timeout)\\n        except Empty:\\n            # Note that this is not caught by the redis client and will be\\n            # raised unless handled by application code. If you want never to\\n            raise ConnectionError(\"No connection available.\")\\n\\n        # If the ``connection`` is actually ``None`` then that\\'s a cue to make\\n        # a new connection to add to the pool.\\n        if connection is None:\\n            connection = self.make_connection()\\n\\n        try:\\n            # ensure this connection is connected to Redis\\n            connection.connect()\\n            # connections that the pool provides should be ready to send\\n            # a command. if not, the connection was either returned to the\\n            # pool before all data has been read or the socket has been\\n            # closed. either way, reconnect and verify everything is good.\\n            if not connection.is_ready_for_command():\\n                connection.disconnect()\\n                connection.connect()\\n                if not connection.is_ready_for_command():\\n                    raise ConnectionError(\\'Connection not ready\\')\\n        except:  # noqa: E722\\n            # release the connection back to the pool so that we don\\'t leak it\\n            self.release(connection)\\n            raise\\n\\n        return connection', 'def release(self, connection):\\n        \"Releases the connection back to the pool.\"\\n        # Make sure we haven\\'t changed process.\\n        self._checkpid()\\n        if connection.pid != self.pid:\\n            return\\n\\n        # Put the connection back into the pool.\\n        try:\\n            self.pool.put_nowait(connection)\\n        except Full:\\n            # perhaps the pool has been reset() after a fork? regardless,\\n            # we don\\'t want this connection\\n            pass', 'def pack_command(self, *args):\\n        \"Pack a series of arguments into a value Redis command\"\\n        args_output = SYM_EMPTY.join([\\n            SYM_EMPTY.join(\\n                (SYM_DOLLAR, str(len(k)).encode(), SYM_CRLF, k, SYM_CRLF))\\n            for k in imap(self.encoder.encode, args)])\\n        output = SYM_EMPTY.join(\\n            (SYM_STAR, str(len(args)).encode(), SYM_CRLF, args_output))\\n        return output', 'def has_selector(selector):\\n    \"Determine if the current platform has the selector available\"\\n    try:\\n        if selector == \\'poll\\':\\n            # the select module offers the poll selector even if the platform\\n            # doesn\\'t support it. Attempt to poll for nothing to make sure\\n            # poll is available\\n            p = select.poll()\\n            p.poll(0)\\n        else:\\n            # the other selectors will fail when instantiated\\n            getattr(select, selector)().close()\\n        return True\\n    except (OSError, AttributeError):\\n        return False', 'def DefaultSelector(sock):\\n    \"Return the best selector for the platform\"\\n    global _DEFAULT_SELECTOR\\n    if _DEFAULT_SELECTOR is None:\\n        if has_selector(\\'poll\\'):\\n            _DEFAULT_SELECTOR = PollSelector\\n        elif hasattr(select, \\'select\\'):\\n            _DEFAULT_SELECTOR = SelectSelector\\n        else:\\n            raise RedisError(\\'Platform does not support any selectors\\')\\n    return _DEFAULT_SELECTOR(sock)', 'def can_read(self, timeout=0):\\n        \"\"\"\\n        Return True if data is ready to be read from the socket,\\n        otherwise False.\\n\\n        This doesn\\'t guarentee that the socket is still connected, just that\\n        there is data to read.\\n\\n        Automatically retries EINTR errors based on PEP 475.\\n        \"\"\"\\n        while True:\\n            try:\\n                return self.check_can_read(timeout)\\n            except (select.error, IOError) as ex:\\n                if self.errno_from_exception(ex) == errno.EINTR:\\n                    continue\\n                return False', 'def is_ready_for_command(self, timeout=0):\\n        \"\"\"\\n        Return True if the socket is ready to send a command,\\n        otherwise False.\\n\\n        Automatically retries EINTR errors based on PEP 475.\\n        \"\"\"\\n        while True:\\n            try:\\n                return self.check_is_ready_for_command(timeout)\\n            except (select.error, IOError) as ex:\\n                if self.errno_from_exception(ex) == errno.EINTR:\\n                    continue\\n                return False', 'def errno_from_exception(self, ex):\\n        \"\"\"\\n        Get the error number from an exception\\n        \"\"\"\\n        if hasattr(ex, \\'errno\\'):\\n            return ex.errno\\n        elif ex.args:\\n            return ex.args[0]\\n        else:\\n            return None', 'def from_url(url, db=None, **kwargs):\\n    \"\"\"\\n    Returns an active Redis client generated from the given database URL.\\n\\n    Will attempt to extract the database id from the path url fragment, if\\n    none is provided.\\n    \"\"\"\\n    from redis.client import Redis\\n    return Redis.from_url(url, db, **kwargs)', 'def rotate_slaves(self):\\n        \"Round-robin slave balancer\"\\n        slaves = self.sentinel_manager.discover_slaves(self.service_name)\\n        if slaves:\\n            if self.slave_rr_counter is None:\\n                self.slave_rr_counter = random.randint(0, len(slaves) - 1)\\n            for _ in xrange(len(slaves)):\\n                self.slave_rr_counter = (\\n                    self.slave_rr_counter + 1) % len(slaves)\\n                slave = slaves[self.slave_rr_counter]\\n                yield slave\\n        # Fallback to the master connection\\n        try:\\n            yield self.get_master_address()\\n        except MasterNotFoundError:\\n            pass\\n        raise SlaveNotFoundError(\\'No slave found for %r\\' % (self.service_name))', 'def discover_master(self, service_name):\\n        \"\"\"\\n        Asks sentinel servers for the Redis master\\'s address corresponding\\n        to the service labeled ``service_name``.\\n\\n        Returns a pair (address, port) or raises MasterNotFoundError if no\\n        master is found.\\n        \"\"\"\\n        for sentinel_no, sentinel in enumerate(self.sentinels):\\n            try:\\n                masters = sentinel.sentinel_masters()\\n            except (ConnectionError, TimeoutError):\\n                continue\\n            state = masters.get(service_name)\\n            if state and self.check_master_state(state, service_name):\\n                # Put this sentinel at the top of the list\\n                self.sentinels[0], self.sentinels[sentinel_no] = (\\n                    sentinel, self.sentinels[0])\\n                return state[\\'ip\\'], state[\\'port\\']\\n        raise MasterNotFoundError(\"No master found for %r\" % (service_name,))', 'def filter_slaves(self, slaves):\\n        \"Remove slaves that are in an ODOWN or SDOWN state\"\\n        slaves_alive = []\\n        for slave in slaves:\\n            if slave[\\'is_odown\\'] or slave[\\'is_sdown\\']:\\n                continue\\n            slaves_alive.append((slave[\\'ip\\'], slave[\\'port\\']))\\n        return slaves_alive', 'def discover_slaves(self, service_name):\\n        \"Returns a list of alive slaves for service ``service_name``\"\\n        for sentinel in self.sentinels:\\n            try:\\n                slaves = sentinel.sentinel_slaves(service_name)\\n            except (ConnectionError, ResponseError, TimeoutError):\\n                continue\\n            slaves = self.filter_slaves(slaves)\\n            if slaves:\\n                return slaves\\n        return []', 'def explain_weights_lightning(estimator, vec=None, top=20, target_names=None,\\n                              targets=None, feature_names=None,\\n                              coef_scale=None):\\n    \"\"\" Return an explanation of a lightning estimator weights \"\"\"\\n    return explain_weights_lightning_not_supported(estimator)', 'def explain_prediction_lightning(estimator, doc, vec=None, top=None,\\n                                 target_names=None, targets=None,\\n                                 feature_names=None, vectorized=False,\\n                                 coef_scale=None):\\n    \"\"\" Return an explanation of a lightning estimator predictions \"\"\"\\n    return explain_weights_lightning_not_supported(estimator, doc)', 'def format_as_html(explanation,  # type: Explanation\\n                   include_styles=True,  # type: bool\\n                   force_weights=True,  # type: bool\\n                   show=fields.ALL,\\n                   preserve_density=None,  # type: Optional[bool]\\n                   highlight_spaces=None,  # type: Optional[bool]\\n                   horizontal_layout=True,  # type: bool\\n                   show_feature_values=False  # type: bool\\n                   ):\\n    # type: (...) -> str\\n    \"\"\" Format explanation as html.\\n    Most styles are inline, but some are included separately in <style> tag,\\n    you can omit them by passing ``include_styles=False`` and call\\n    ``format_html_styles`` to render them separately (or just omit them).\\n    With ``force_weights=False``, weights will not be displayed in a table for\\n    predictions where it is possible to show feature weights highlighted\\n    in the document.\\n    If ``highlight_spaces`` is None (default), spaces will be highlighted in\\n    feature names only if there are any spaces at the start or at the end of the\\n    feature. Setting it to True forces space highlighting, and setting it to\\n    False turns it off.\\n    If ``horizontal_layout`` is True (default), multiclass classifier\\n    weights are laid out horizontally.\\n    If ``show_feature_values`` is True, feature values are shown if present.\\n    Default is False.\\n    \"\"\"\\n    template = template_env.get_template(\\'explain.html\\')\\n    if highlight_spaces is None:\\n        highlight_spaces = should_highlight_spaces(explanation)\\n    targets = explanation.targets or []\\n    if len(targets) == 1:\\n        horizontal_layout = False\\n    explaining_prediction = has_any_values_for_weights(explanation)\\n    show_feature_values = show_feature_values and explaining_prediction\\n\\n    rendered_weighted_spans = render_targets_weighted_spans(\\n        targets, preserve_density)\\n    weighted_spans_others = [\\n        t.weighted_spans.other if t.weighted_spans else None for t in targets]\\n\\n    return template.render(\\n        include_styles=include_styles,\\n        force_weights=force_weights,\\n        target_table_styles=\\n        \\'border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\\',\\n        tr_styles=\\'border: none;\\',\\n        # Weight (th and td)\\n        td1_styles=\\'padding: 0 1em 0 0.5em; text-align: right; border: none;\\',\\n        # N more positive/negative\\n        tdm_styles=\\'padding: 0 0.5em 0 0.5em; text-align: center; border: none; \\'\\n                   \\'white-space: nowrap;\\',\\n        # Feature (th and td)\\n        td2_styles=\\'padding: 0 0.5em 0 0.5em; text-align: left; border: none;\\',\\n        # Value (th and td)\\n        td3_styles=\\'padding: 0 0.5em 0 1em; text-align: right; border: none;\\',\\n        horizontal_layout_table_styles=\\n        \\'border-collapse: collapse; border: none; margin-bottom: 1.5em;\\',\\n        horizontal_layout_td_styles=\\n        \\'padding: 0px; border: 1px solid black; vertical-align: top;\\',\\n        horizontal_layout_header_styles=\\n        \\'padding: 0.5em; border: 1px solid black; text-align: center;\\',\\n        show=show,\\n        expl=explanation,\\n        hl_spaces=highlight_spaces,\\n        horizontal_layout=horizontal_layout,\\n        any_weighted_spans=any(t.weighted_spans for t in targets),\\n        feat_imp_weight_range=max_or_0(\\n            abs(fw.weight) for fw in explanation.feature_importances.importances)\\n        if explanation.feature_importances else 0,\\n        target_weight_range=max_or_0(\\n            get_weight_range(t.feature_weights) for t in targets),\\n        other_weight_range=max_or_0(\\n            get_weight_range(other)\\n            for other in weighted_spans_others if other),\\n        targets_with_weighted_spans=list(\\n            zip(targets, rendered_weighted_spans, weighted_spans_others)),\\n        show_feature_values=show_feature_values,\\n        weights_table_span=3 if show_feature_values else 2,\\n        explaining_prediction=explaining_prediction,\\n        weight_help=html_escape(WEIGHT_HELP),\\n        contribution_help=html_escape(CONTRIBUTION_HELP),\\n    )', 'def render_targets_weighted_spans(\\n        targets,  # type: List[TargetExplanation]\\n        preserve_density,  # type: Optional[bool]\\n    ):\\n    # type: (...) -> List[Optional[str]]\\n    \"\"\" Return a list of rendered weighted spans for targets.\\n    Function must accept a list in order to select consistent weight\\n    ranges across all targets.\\n    \"\"\"\\n    prepared_weighted_spans = prepare_weighted_spans(\\n        targets, preserve_density)\\n\\n    def _fmt_pws(pws):\\n        # type: (PreparedWeightedSpans) -> str\\n        name = (\\'<b>{}:</b> \\'.format(pws.doc_weighted_spans.vec_name)\\n                if pws.doc_weighted_spans.vec_name else \\'\\')\\n        return \\'{}{}\\'.format(name, render_weighted_spans(pws))\\n\\n    def _fmt_pws_list(pws_lst):\\n        # type: (List[PreparedWeightedSpans]) -> str\\n        return \\'<br/>\\'.join(_fmt_pws(pws) for pws in pws_lst)\\n\\n    return [_fmt_pws_list(pws_lst) if pws_lst else None\\n            for pws_lst in prepared_weighted_spans]', 'def _colorize(token,  # type: str\\n              weight,  # type: float\\n              weight_range,  # type: float\\n              ):\\n    # type: (...) -> str\\n    \"\"\" Return token wrapped in a span with some styles\\n    (calculated from weight and weight_range) applied.\\n    \"\"\"\\n    token = html_escape(token)\\n    if np.isclose(weight, 0.):\\n        return (\\n            \\'<span \\'\\n            \\'style=\"opacity: {opacity}\"\\'\\n            \\'>{token}</span>\\'.format(\\n                opacity=_weight_opacity(weight, weight_range),\\n                token=token)\\n        )\\n    else:\\n        return (\\n            \\'<span \\'\\n            \\'style=\"background-color: {color}; opacity: {opacity}\" \\'\\n            \\'title=\"{weight:.3f}\"\\'\\n            \\'>{token}</span>\\'.format(\\n                color=format_hsl(\\n                    weight_color_hsl(weight, weight_range, min_lightness=0.6)),\\n                opacity=_weight_opacity(weight, weight_range),\\n                weight=weight,\\n                token=token)\\n        )', 'def _weight_opacity(weight, weight_range):\\n    # type: (float, float) -> str\\n    \"\"\" Return opacity value for given weight as a string.\\n    \"\"\"\\n    min_opacity = 0.8\\n    if np.isclose(weight, 0) and np.isclose(weight_range, 0):\\n        rel_weight = 0.0\\n    else:\\n        rel_weight = abs(weight) / weight_range\\n    return \\'{:.2f}\\'.format(min_opacity + (1 - min_opacity) * rel_weight)', 'def weight_color_hsl(weight, weight_range, min_lightness=0.8):\\n    # type: (float, float, float) -> _HSL_COLOR\\n    \"\"\" Return HSL color components for given weight,\\n    where the max absolute weight is given by weight_range.\\n    \"\"\"\\n    hue = _hue(weight)\\n    saturation = 1\\n    rel_weight = (abs(weight) / weight_range) ** 0.7\\n    lightness = 1.0 - (1 - min_lightness) * rel_weight\\n    return hue, saturation, lightness', 'def format_hsl(hsl_color):\\n    # type: (_HSL_COLOR) -> str\\n    \"\"\" Format hsl color as css color string.\\n    \"\"\"\\n    hue, saturation, lightness = hsl_color\\n    return \\'hsl({}, {:.2%}, {:.2%})\\'.format(hue, saturation, lightness)', 'def get_weight_range(weights):\\n    # type: (FeatureWeights) -> float\\n    \"\"\" Max absolute feature for pos and neg weights.\\n    \"\"\"\\n    return max_or_0(abs(fw.weight)\\n                    for lst in [weights.pos, weights.neg]\\n                    for fw in lst or [])', 'def remaining_weight_color_hsl(\\n        ws,  # type: List[FeatureWeight]\\n        weight_range,  # type: float\\n        pos_neg,  # type: str\\n    ):\\n    # type: (...) -> _HSL_COLOR\\n    \"\"\" Color for \"remaining\" row.\\n    Handles a number of edge cases: if there are no weights in ws or weight_range\\n    is zero, assume the worst (most intensive positive or negative color).\\n    \"\"\"\\n    sign = {\\'pos\\': 1.0, \\'neg\\': -1.0}[pos_neg]\\n    if not ws and not weight_range:\\n        weight = sign\\n        weight_range = 1.0\\n    elif not ws:\\n        weight = sign * weight_range\\n    else:\\n        weight = min((fw.weight for fw in ws), key=abs)\\n    return weight_color_hsl(weight, weight_range)', 'def _format_unhashed_feature(feature, weight, hl_spaces):\\n    # type: (...) -> str\\n    \"\"\" Format unhashed feature: show first (most probable) candidate,\\n    display other candidates in title attribute.\\n    \"\"\"\\n    if not feature:\\n        return \\'\\'\\n    else:\\n        first, rest = feature[0], feature[1:]\\n        html = format_signed(\\n            first, lambda x: _format_single_feature(x, weight, hl_spaces))\\n        if rest:\\n            html += \\' <span title=\"{}\">&hellip;</span>\\'.format(\\n                \\'\\\\n\\'.join(html_escape(format_signed(f)) for f in rest))\\n        return html', 'def _format_feature(feature, weight, hl_spaces):\\n    # type: (...) -> str\\n    \"\"\" Format any feature.\\n    \"\"\"\\n    if isinstance(feature, FormattedFeatureName):\\n        return feature.format()\\n    elif (isinstance(feature, list) and\\n            all(\\'name\\' in x and \\'sign\\' in x for x in feature)):\\n        return _format_unhashed_feature(feature, weight, hl_spaces=hl_spaces)\\n    else:\\n        return _format_single_feature(feature, weight, hl_spaces=hl_spaces)', 'def explain_weights_lightgbm(lgb,\\n                             vec=None,\\n                             top=20,\\n                             target_names=None,  # ignored\\n                             targets=None,  # ignored\\n                             feature_names=None,\\n                             feature_re=None,\\n                             feature_filter=None,\\n                             importance_type=\\'gain\\',\\n                             ):\\n    \"\"\"\\n    Return an explanation of an LightGBM estimator (via scikit-learn wrapper\\n    LGBMClassifier or LGBMRegressor) as feature importances.\\n\\n    See :func:`eli5.explain_weights` for description of\\n    ``top``, ``feature_names``,\\n    ``feature_re`` and ``feature_filter`` parameters.\\n\\n    ``target_names`` and ``targets`` parameters are ignored.\\n    \\n    Parameters\\n    ----------\\n    importance_type : str, optional\\n        A way to get feature importance. Possible values are:\\n\\n        - \\'gain\\' - the average gain of the feature when it is used in trees\\n          (default)\\n        - \\'split\\' - the number of times a feature is used to split the data\\n          across all trees\\n        - \\'weight\\' - the same as \\'split\\', for compatibility with xgboost\\n    \"\"\"\\n    coef = _get_lgb_feature_importances(lgb, importance_type)\\n    lgb_feature_names = lgb.booster_.feature_name()\\n    return get_feature_importance_explanation(lgb, vec, coef,\\n        feature_names=feature_names,\\n        estimator_feature_names=lgb_feature_names,\\n        feature_filter=feature_filter,\\n        feature_re=feature_re,\\n        top=top,\\n        description=DESCRIPTION_LIGHTGBM,\\n        num_features=coef.shape[-1],\\n        is_regression=isinstance(lgb, lightgbm.LGBMRegressor),\\n    )', 'def explain_prediction_lightgbm(\\n        lgb, doc,\\n        vec=None,\\n        top=None,\\n        top_targets=None,\\n        target_names=None,\\n        targets=None,\\n        feature_names=None,\\n        feature_re=None,\\n        feature_filter=None,\\n        vectorized=False,\\n        ):\\n    \"\"\" Return an explanation of LightGBM prediction (via scikit-learn wrapper\\n    LGBMClassifier or LGBMRegressor) as feature weights.\\n\\n    See :func:`eli5.explain_prediction` for description of\\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\\n\\n    ``vec`` is a vectorizer instance used to transform\\n    raw features to the input of the estimator ``xgb``\\n    (e.g. a fitted CountVectorizer instance); you can pass it\\n    instead of ``feature_names``.\\n\\n    ``vectorized`` is a flag which tells eli5 if ``doc`` should be\\n    passed through ``vec`` or not. By default it is False, meaning that\\n    if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\\n    estimator. Set it to True if you\\'re passing ``vec``,\\n    but ``doc`` is already vectorized.\\n\\n    Method for determining feature importances follows an idea from\\n    http://blog.datadive.net/interpreting-random-forests/.\\n    Feature weights are calculated by following decision paths in trees\\n    of an ensemble.\\n    Each leaf has an output score, and expected scores can also be assigned\\n    to parent nodes.\\n    Contribution of one feature on the decision path is how much expected score\\n    changes from parent to child.\\n    Weights of all features sum to the output score of the estimator.\\n    \"\"\"\\n\\n    vec, feature_names = handle_vec(lgb, doc, vec, vectorized, feature_names)\\n    if feature_names.bias_name is None:\\n        # LightGBM estimators do not have an intercept, but here we interpret\\n        # them as having an intercept\\n        feature_names.bias_name = \\'<BIAS>\\'\\n    X = get_X(doc, vec, vectorized=vectorized)\\n\\n    proba = predict_proba(lgb, X)\\n    weight_dicts = _get_prediction_feature_weights(lgb, X, _lgb_n_targets(lgb))\\n    x = get_X0(add_intercept(X))\\n\\n    is_regression = isinstance(lgb, lightgbm.LGBMRegressor)\\n    is_multiclass = _lgb_n_targets(lgb) > 2\\n    names = lgb.classes_ if not is_regression else [\\'y\\']\\n\\n    def get_score_weights(_label_id):\\n        _weights = _target_feature_weights(\\n            weight_dicts[_label_id],\\n            num_features=len(feature_names),\\n            bias_idx=feature_names.bias_idx,\\n        )\\n        _score = _get_score(weight_dicts[_label_id])\\n        return _score, _weights\\n\\n    return get_decision_path_explanation(\\n        lgb, doc, vec,\\n        x=x,\\n        feature_names=feature_names,\\n        feature_filter=feature_filter,\\n        feature_re=feature_re,\\n        top=top,\\n        vectorized=vectorized,\\n        original_display_names=names,\\n        target_names=target_names,\\n        targets=targets,\\n        top_targets=top_targets,\\n        is_regression=is_regression,\\n        is_multiclass=is_multiclass,\\n        proba=proba,\\n        get_score_weights=get_score_weights,\\n     )', 'def _compute_node_values(tree_info):\\n    \"\"\" Add node_value key with an expected value for non-leaf nodes \"\"\"\\n    def walk(tree):\\n        if \\'leaf_value\\' in tree:\\n            return tree[\\'leaf_value\\'], tree.get(\\'leaf_count\\', 0)\\n        left_value, left_count = walk(tree[\\'left_child\\'])\\n        right_value, right_count = walk(tree[\\'right_child\\'])\\n        count = left_count + right_count\\n        if tree[\\'split_gain\\'] <= 0:\\n            assert left_value == right_value\\n            tree[\\'_node_value\\'] = left_value\\n        else:\\n            tree[\\'_node_value\\'] = (left_value * left_count +\\n                                  right_value * right_count) / count\\n        return tree[\\'_node_value\\'], count\\n\\n    for tree in tree_info:\\n        walk(tree[\\'tree_structure\\'])', 'def _changes(path):\\n    \"\"\"\\n    >>> _changes([2, 3, 0, 5])\\n    [2, 1, -3, 5]\\n    >>> _changes([2])\\n    [2]\\n    \"\"\"\\n    res = [path[0]]\\n    res += [p - p_prev for p, p_prev in zip(path[1:], path)]\\n    return res', 'def _get_prediction_feature_weights(lgb, X, n_targets):\\n    \"\"\" \\n    Return a list of {feat_id: value} dicts with feature weights, \\n    following ideas from  http://blog.datadive.net/interpreting-random-forests/  \\n    \"\"\"\\n    if n_targets == 2:\\n        n_targets = 1\\n    dump = lgb.booster_.dump_model()\\n    tree_info = dump[\\'tree_info\\']\\n    _compute_node_values(tree_info)\\n    pred_leafs = lgb.booster_.predict(X, pred_leaf=True).reshape(-1, n_targets)\\n    tree_info = np.array(tree_info).reshape(-1, n_targets)\\n    assert pred_leafs.shape == tree_info.shape\\n\\n    res = []\\n    for target in range(n_targets):\\n        feature_weights = defaultdict(float)  # type: DefaultDict[Optional[str], float]\\n        for info, leaf_id in zip(tree_info[:, target], pred_leafs[:, target]):\\n            leaf_index, split_index = _get_leaf_split_indices(\\n                info[\\'tree_structure\\']\\n            )\\n            bias, path = _get_decision_path(leaf_index, split_index, leaf_id)\\n            feature_weights[None] += bias\\n            for feat, value in path:\\n                feature_weights[feat] += value\\n        res.append(dict(feature_weights))\\n    return res', 'def replace_spaces(s, replacer):\\n    # type: (str, Callable[[int, str], str]) -> str\\n    \"\"\"\\n    >>> replace_spaces(\\'ab\\', lambda n, l: \\'_\\' * n)\\n    \\'ab\\'\\n    >>> replace_spaces(\\'a b\\', lambda n, l: \\'_\\' * n)\\n    \\'a_b\\'\\n    >>> replace_spaces(\\' ab\\', lambda n, l: \\'_\\' * n)\\n    \\'_ab\\'\\n    >>> replace_spaces(\\'  a b \\', lambda n, s: s * n)\\n    \\'leftleftacenterbright\\'\\n    >>> replace_spaces(\\' a b  \\', lambda n, _: \\'0 0\\' * n)\\n    \\'0 0a0 0b0 00 0\\'\\n    \"\"\"\\n    def replace(m):\\n        # type: (Match[str]) -> str\\n        if m.start() == 0:\\n            side = \\'left\\'\\n        elif m.end() == len(s):\\n            side = \\'right\\'\\n        else:\\n            side = \\'center\\'\\n        return replacer(len(m.group()), side)\\n\\n    return re.sub(r\\'[ ]+\\', replace, s)', 'def format_signed(feature,  # type: Dict[str, Any]\\n                  formatter=None,  # type: Callable[..., str]\\n                  **kwargs\\n                  ):\\n    # type: (...) -> str\\n    \"\"\"\\n    Format unhashed feature with sign.\\n\\n    >>> format_signed({\\'name\\': \\'foo\\', \\'sign\\': 1})\\n    \\'foo\\'\\n    >>> format_signed({\\'name\\': \\'foo\\', \\'sign\\': -1})\\n    \\'(-)foo\\'\\n    >>> format_signed({\\'name\\': \\' foo\\', \\'sign\\': -1}, lambda x: \\'\"{}\"\\'.format(x))\\n    \\'(-)\" foo\"\\'\\n    \"\"\"\\n    txt = \\'\\' if feature[\\'sign\\'] > 0 else \\'(-)\\'\\n    name = feature[\\'name\\']  # type: str\\n    if formatter is not None:\\n        name = formatter(name, **kwargs)\\n    return \\'{}{}\\'.format(txt, name)', 'def tabulate(data,  # type: List[List[Any]]\\n             header=None,  # type: Optional[List[Any]]\\n             col_align=None,  # type: Union[str, List[str]]\\n             ):\\n    # type: (...) -> List[str]\\n    \"\"\" Format data as a table without any fancy features.\\n    col_align: l/r/c or a list/string of l/r/c. l = left, r = right, c = center\\n    Return a list of strings (lines of the table).\\n    \"\"\"\\n    if not data and not header:\\n        return []\\n    if data:\\n        n_cols = len(data[0])\\n    else:\\n        assert header is not None\\n        n_cols = len(header)\\n    if not all(len(row) == n_cols for row in data):\\n        raise ValueError(\\'data is not rectangular\\')\\n\\n    if col_align is None:\\n        col_align = [\\'l\\'] * n_cols\\n    elif isinstance(col_align, six.string_types) and len(col_align) == 1:\\n        col_align = [col_align] * n_cols\\n    else:\\n        col_align = list(col_align)\\n        if len(col_align) != n_cols:\\n            raise ValueError(\\'col_align length does not match number of columns\\')\\n\\n    if header and len(header) != n_cols:\\n        raise ValueError(\\'header length does not match number of columns\\')\\n\\n    if header:\\n        data = [header] + data\\n    data = [[six.text_type(x) for x in row] for row in data]\\n    col_width = [max(len(row[col_i]) for row in data) for col_i in range(n_cols)]\\n    if header:\\n        data.insert(1, [\\'-\\' * width for width in col_width])\\n\\n    line_tpl = u\\'  \\'.join(\\n        u\\'{:%s%s}\\' % ({\\'l\\': \\'\\', \\'r\\': \\'>\\', \\'c\\': \\'^\\'}[align], width)\\n        for align, width in zip(col_align, col_width))\\n    return [line_tpl.format(*row) for row in data]', 'def explain_prediction_sklearn(estimator, doc,\\n                               vec=None,\\n                               top=None,\\n                               top_targets=None,\\n                               target_names=None,\\n                               targets=None,\\n                               feature_names=None,\\n                               feature_re=None,\\n                               feature_filter=None,\\n                               vectorized=False):\\n    \"\"\" Return an explanation of a scikit-learn estimator \"\"\"\\n    return explain_prediction_sklearn_not_supported(estimator, doc)', 'def explain_prediction_linear_classifier(clf, doc,\\n                                         vec=None,\\n                                         top=None,\\n                                         top_targets=None,\\n                                         target_names=None,\\n                                         targets=None,\\n                                         feature_names=None,\\n                                         feature_re=None,\\n                                         feature_filter=None,\\n                                         vectorized=False,\\n                                         ):\\n    \"\"\"\\n    Explain prediction of a linear classifier.\\n\\n    See :func:`eli5.explain_prediction` for description of\\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\\n\\n    ``vec`` is a vectorizer instance used to transform\\n    raw features to the input of the classifier ``clf``\\n    (e.g. a fitted CountVectorizer instance); you can pass it\\n    instead of ``feature_names``.\\n\\n    ``vectorized`` is a flag which tells eli5 if ``doc`` should be\\n    passed through ``vec`` or not. By default it is False, meaning that\\n    if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\\n    classifier. Set it to True if you\\'re passing ``vec``, but ``doc``\\n    is already vectorized.\\n    \"\"\"\\n    vec, feature_names = handle_vec(clf, doc, vec, vectorized, feature_names)\\n    X = get_X(doc, vec=vec, vectorized=vectorized, to_dense=True)\\n\\n    proba = predict_proba(clf, X)\\n    score, = clf.decision_function(X)\\n\\n    if has_intercept(clf):\\n        X = add_intercept(X)\\n    x = get_X0(X)\\n\\n    feature_names, flt_indices = feature_names.handle_filter(\\n        feature_filter, feature_re, x)\\n\\n    res = Explanation(\\n        estimator=repr(clf),\\n        method=\\'linear model\\',\\n        targets=[],\\n    )\\n    assert res.targets is not None\\n\\n    _weights = _linear_weights(clf, x, top, feature_names, flt_indices)\\n    classes = getattr(clf, \"classes_\", [\"-1\", \"1\"])  # OneClassSVM support\\n    display_names = get_target_display_names(classes, target_names,\\n                                             targets, top_targets, score)\\n\\n    if is_multiclass_classifier(clf):\\n        for label_id, label in display_names:\\n            target_expl = TargetExplanation(\\n                target=label,\\n                feature_weights=_weights(label_id),\\n                score=score[label_id],\\n                proba=proba[label_id] if proba is not None else None,\\n            )\\n            add_weighted_spans(doc, vec, vectorized, target_expl)\\n            res.targets.append(target_expl)\\n    else:\\n        if len(display_names) == 1:  # target is passed explicitly\\n            label_id, target = display_names[0]\\n        else:\\n            label_id = 1 if score >= 0 else 0\\n            target = display_names[label_id][1]\\n        scale = -1 if label_id == 0 else 1\\n\\n        target_expl = TargetExplanation(\\n            target=target,\\n            feature_weights=_weights(0, scale=scale),\\n            score=score,\\n            proba=proba[label_id] if proba is not None else None,\\n        )\\n        add_weighted_spans(doc, vec, vectorized, target_expl)\\n        res.targets.append(target_expl)\\n\\n    return res', 'def explain_prediction_linear_regressor(reg, doc,\\n                                        vec=None,\\n                                        top=None,\\n                                        top_targets=None,\\n                                        target_names=None,\\n                                        targets=None,\\n                                        feature_names=None,\\n                                        feature_re=None,\\n                                        feature_filter=None,\\n                                        vectorized=False):\\n    \"\"\"\\n    Explain prediction of a linear regressor.\\n\\n    See :func:`eli5.explain_prediction` for description of\\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\\n\\n    ``vec`` is a vectorizer instance used to transform\\n    raw features to the input of the classifier ``clf``;\\n    you can pass it instead of ``feature_names``.\\n\\n    ``vectorized`` is a flag which tells eli5 if ``doc`` should be\\n    passed through ``vec`` or not. By default it is False, meaning that\\n    if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\\n    regressor ``reg``. Set it to True if you\\'re passing ``vec``,\\n    but ``doc`` is already vectorized.\\n    \"\"\"\\n    if isinstance(reg, (SVR, NuSVR)) and reg.kernel != \\'linear\\':\\n        return explain_prediction_sklearn_not_supported(reg, doc)\\n\\n    vec, feature_names = handle_vec(reg, doc, vec, vectorized, feature_names)\\n    X = get_X(doc, vec=vec, vectorized=vectorized, to_dense=True)\\n\\n    score, = reg.predict(X)\\n\\n    if has_intercept(reg):\\n        X = add_intercept(X)\\n    x = get_X0(X)\\n\\n    feature_names, flt_indices = feature_names.handle_filter(\\n        feature_filter, feature_re, x)\\n\\n    res = Explanation(\\n        estimator=repr(reg),\\n        method=\\'linear model\\',\\n        targets=[],\\n        is_regression=True,\\n    )\\n    assert res.targets is not None\\n\\n    _weights = _linear_weights(reg, x, top, feature_names, flt_indices)\\n    names = get_default_target_names(reg)\\n    display_names = get_target_display_names(names, target_names, targets,\\n                                             top_targets, score)\\n\\n    if is_multitarget_regressor(reg):\\n        for label_id, label in display_names:\\n            target_expl = TargetExplanation(\\n                target=label,\\n                feature_weights=_weights(label_id),\\n                score=score[label_id],\\n            )\\n            add_weighted_spans(doc, vec, vectorized, target_expl)\\n            res.targets.append(target_expl)\\n    else:\\n        target_expl = TargetExplanation(\\n            target=display_names[0][1],\\n            feature_weights=_weights(0),\\n            score=score,\\n        )\\n        add_weighted_spans(doc, vec, vectorized, target_expl)\\n        res.targets.append(target_expl)\\n\\n    return res', 'def explain_prediction_tree_classifier(\\n        clf, doc,\\n        vec=None,\\n        top=None,\\n        top_targets=None,\\n        target_names=None,\\n        targets=None,\\n        feature_names=None,\\n        feature_re=None,\\n        feature_filter=None,\\n        vectorized=False):\\n    \"\"\" Explain prediction of a tree classifier.\\n\\n    See :func:`eli5.explain_prediction` for description of\\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\\n\\n    ``vec`` is a vectorizer instance used to transform\\n    raw features to the input of the classifier ``clf``\\n    (e.g. a fitted CountVectorizer instance); you can pass it\\n    instead of ``feature_names``.\\n\\n    ``vectorized`` is a flag which tells eli5 if ``doc`` should be\\n    passed through ``vec`` or not. By default it is False, meaning that\\n    if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\\n    classifier. Set it to True if you\\'re passing ``vec``,\\n    but ``doc`` is already vectorized.\\n\\n    Method for determining feature importances follows an idea from\\n    http://blog.datadive.net/interpreting-random-forests/.\\n    Feature weights are calculated by following decision paths in trees\\n    of an ensemble (or a single tree for DecisionTreeClassifier).\\n    Each node of the tree has an output score, and contribution of a feature\\n    on the decision path is how much the score changes from parent to child.\\n    Weights of all features sum to the output score or proba of the estimator.\\n    \"\"\"\\n    vec, feature_names = handle_vec(clf, doc, vec, vectorized, feature_names)\\n    X = get_X(doc, vec=vec, vectorized=vectorized)\\n    if feature_names.bias_name is None:\\n        # Tree estimators do not have an intercept, but here we interpret\\n        # them as having an intercept\\n        feature_names.bias_name = \\'<BIAS>\\'\\n\\n    proba = predict_proba(clf, X)\\n    if hasattr(clf, \\'decision_function\\'):\\n        score, = clf.decision_function(X)\\n    else:\\n        score = None\\n\\n    is_multiclass = clf.n_classes_ > 2\\n    feature_weights = _trees_feature_weights(\\n        clf, X, feature_names, clf.n_classes_)\\n    x = get_X0(add_intercept(X))\\n    flt_feature_names, flt_indices = feature_names.handle_filter(\\n        feature_filter, feature_re, x)\\n\\n    def _weights(label_id, scale=1.0):\\n        weights = feature_weights[:, label_id]\\n        return get_top_features_filtered(x, flt_feature_names, flt_indices,\\n                                         weights, top, scale)\\n\\n    res = Explanation(\\n        estimator=repr(clf),\\n        method=\\'decision path\\',\\n        targets=[],\\n        description=(DESCRIPTION_TREE_CLF_MULTICLASS if is_multiclass\\n                     else DESCRIPTION_TREE_CLF_BINARY),\\n    )\\n    assert res.targets is not None\\n\\n    display_names = get_target_display_names(\\n        clf.classes_, target_names, targets, top_targets,\\n        score=score if score is not None else proba)\\n\\n    if is_multiclass:\\n        for label_id, label in display_names:\\n            target_expl = TargetExplanation(\\n                target=label,\\n                feature_weights=_weights(label_id),\\n                score=score[label_id] if score is not None else None,\\n                proba=proba[label_id] if proba is not None else None,\\n            )\\n            add_weighted_spans(doc, vec, vectorized, target_expl)\\n            res.targets.append(target_expl)\\n    else:\\n        target, scale, label_id = get_binary_target_scale_label_id(\\n            score, display_names, proba)\\n        target_expl = TargetExplanation(\\n            target=target,\\n            feature_weights=_weights(label_id, scale=scale),\\n            score=score if score is not None else None,\\n            proba=proba[label_id] if proba is not None else None,\\n        )\\n        add_weighted_spans(doc, vec, vectorized, target_expl)\\n        res.targets.append(target_expl)\\n\\n    return res', 'def explain_prediction_tree_regressor(\\n        reg, doc,\\n        vec=None,\\n        top=None,\\n        top_targets=None,\\n        target_names=None,\\n        targets=None,\\n        feature_names=None,\\n        feature_re=None,\\n        feature_filter=None,\\n        vectorized=False):\\n    \"\"\" Explain prediction of a tree regressor.\\n\\n    See :func:`eli5.explain_prediction` for description of\\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\\n\\n    ``vec`` is a vectorizer instance used to transform\\n    raw features to the input of the regressor ``reg``\\n    (e.g. a fitted CountVectorizer instance); you can pass it\\n    instead of ``feature_names``.\\n\\n    ``vectorized`` is a flag which tells eli5 if ``doc`` should be\\n    passed through ``vec`` or not. By default it is False, meaning that\\n    if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\\n    regressor. Set it to True if you\\'re passing ``vec``,\\n    but ``doc`` is already vectorized.\\n\\n    Method for determining feature importances follows an idea from\\n    http://blog.datadive.net/interpreting-random-forests/.\\n    Feature weights are calculated by following decision paths in trees\\n    of an ensemble (or a single tree for DecisionTreeRegressor).\\n    Each node of the tree has an output score, and contribution of a feature\\n    on the decision path is how much the score changes from parent to child.\\n    Weights of all features sum to the output score of the estimator.\\n    \"\"\"\\n    vec, feature_names = handle_vec(reg, doc, vec, vectorized, feature_names)\\n    X = get_X(doc, vec=vec, vectorized=vectorized)\\n    if feature_names.bias_name is None:\\n        # Tree estimators do not have an intercept, but here we interpret\\n        # them as having an intercept\\n        feature_names.bias_name = \\'<BIAS>\\'\\n\\n    score, = reg.predict(X)\\n    num_targets = getattr(reg, \\'n_outputs_\\', 1)\\n    is_multitarget = num_targets > 1\\n    feature_weights = _trees_feature_weights(reg, X, feature_names, num_targets)\\n    x = get_X0(add_intercept(X))\\n    flt_feature_names, flt_indices = feature_names.handle_filter(\\n        feature_filter, feature_re, x)\\n\\n    def _weights(label_id, scale=1.0):\\n        weights = feature_weights[:, label_id]\\n        return get_top_features_filtered(x, flt_feature_names, flt_indices,\\n                                         weights, top, scale)\\n\\n    res = Explanation(\\n        estimator=repr(reg),\\n        method=\\'decision path\\',\\n        description=(DESCRIPTION_TREE_REG_MULTITARGET if is_multitarget\\n                     else DESCRIPTION_TREE_REG),\\n        targets=[],\\n        is_regression=True,\\n    )\\n    assert res.targets is not None\\n\\n    names = get_default_target_names(reg, num_targets=num_targets)\\n    display_names = get_target_display_names(names, target_names, targets,\\n                                             top_targets, score)\\n\\n    if is_multitarget:\\n        for label_id, label in display_names:\\n            target_expl = TargetExplanation(\\n                target=label,\\n                feature_weights=_weights(label_id),\\n                score=score[label_id],\\n            )\\n            add_weighted_spans(doc, vec, vectorized, target_expl)\\n            res.targets.append(target_expl)\\n    else:\\n        target_expl = TargetExplanation(\\n            target=display_names[0][1],\\n            feature_weights=_weights(0),\\n            score=score,\\n        )\\n        add_weighted_spans(doc, vec, vectorized, target_expl)\\n        res.targets.append(target_expl)\\n\\n    return res', 'def _trees_feature_weights(clf, X, feature_names, num_targets):\\n    \"\"\" Return feature weights for a tree or a tree ensemble.\\n    \"\"\"\\n    feature_weights = np.zeros([len(feature_names), num_targets])\\n    if hasattr(clf, \\'tree_\\'):\\n        _update_tree_feature_weights(X, feature_names, clf, feature_weights)\\n    else:\\n        if isinstance(clf, (\\n                GradientBoostingClassifier, GradientBoostingRegressor)):\\n            weight = clf.learning_rate\\n        else:\\n            weight = 1. / len(clf.estimators_)\\n        for _clfs in clf.estimators_:\\n            _update = partial(_update_tree_feature_weights, X, feature_names)\\n            if isinstance(_clfs, np.ndarray):\\n                if len(_clfs) == 1:\\n                    _update(_clfs[0], feature_weights)\\n                else:\\n                    for idx, _clf in enumerate(_clfs):\\n                        _update(_clf, feature_weights[:, idx])\\n            else:\\n                _update(_clfs, feature_weights)\\n        feature_weights *= weight\\n        if hasattr(clf, \\'init_\\'):\\n            feature_weights[feature_names.bias_idx] += clf.init_.predict(X)[0]\\n    return feature_weights', 'def _update_tree_feature_weights(X, feature_names, clf, feature_weights):\\n    \"\"\" Update tree feature weights using decision path method.\\n    \"\"\"\\n    tree_value = clf.tree_.value\\n    if tree_value.shape[1] == 1:\\n        squeeze_axis = 1\\n    else:\\n        assert tree_value.shape[2] == 1\\n        squeeze_axis = 2\\n    tree_value = np.squeeze(tree_value, axis=squeeze_axis)\\n    tree_feature = clf.tree_.feature\\n    _, indices = clf.decision_path(X).nonzero()\\n    if isinstance(clf, DecisionTreeClassifier):\\n        norm = lambda x: x / x.sum()\\n    else:\\n        norm = lambda x: x\\n    feature_weights[feature_names.bias_idx] += norm(tree_value[0])\\n    for parent_idx, child_idx in zip(indices, indices[1:]):\\n        assert tree_feature[parent_idx] >= 0\\n        feature_idx = tree_feature[parent_idx]\\n        diff = norm(tree_value[child_idx]) - norm(tree_value[parent_idx])\\n        feature_weights[feature_idx] += diff', 'def _multiply(X, coef):\\n    \"\"\" Multiple X by coef element-wise, preserving sparsity. \"\"\"\\n    if sp.issparse(X):\\n        return X.multiply(sp.csr_matrix(coef))\\n    else:\\n        return np.multiply(X, coef)', 'def _linear_weights(clf, x, top, flt_feature_names, flt_indices):\\n    \"\"\" Return top weights getter for label_id.\\n    \"\"\"\\n    def _weights(label_id, scale=1.0):\\n        coef = get_coef(clf, label_id)\\n        scores = _multiply(x, coef)\\n        return get_top_features_filtered(x, flt_feature_names, flt_indices,\\n                                         scores, top, scale)\\n    return _weights', 'def attrs(class_):\\n    \"\"\" Like attr.s with slots=True,\\n    but with attributes extracted from __init__ method signature.\\n    slots=True ensures that signature matches what really happens\\n    (we can\\'t define different attributes on self).\\n    It is useful if we still want __init__ for proper type-checking and\\n    do not want to repeat attribute definitions in the class body.\\n    \"\"\"\\n    attrs_kwargs = {}\\n    for method, kw_name in [\\n            (\\'__repr__\\', \\'repr\\'),\\n            (\\'__eq__\\', \\'cmp\\'),\\n            (\\'__hash__\\', \\'hash\\'),\\n            ]:\\n        if method in class_.__dict__:\\n            # Allow to redefine a special method (or else attr.s will do it)\\n            attrs_kwargs[kw_name] = False\\n    init_args = inspect.getargspec(class_.__init__)\\n    defaults_shift = len(init_args.args) - len(init_args.defaults or []) - 1\\n    these = {}\\n    for idx, arg in enumerate(init_args.args[1:]):\\n        attrib_kwargs = {}\\n        if idx >= defaults_shift:\\n            attrib_kwargs[\\'default\\'] = init_args.defaults[idx - defaults_shift]\\n        these[arg] = attr.ib(**attrib_kwargs)\\n    return attr.s(class_, these=these, init=False, slots=True, **attrs_kwargs)', 'def is_probabilistic_classifier(clf):\\n    # type: (Any) -> bool\\n    \"\"\" Return True if a classifier can return probabilities \"\"\"\\n    if not hasattr(clf, \\'predict_proba\\'):\\n        return False\\n    if isinstance(clf, OneVsRestClassifier):\\n        # It currently has a predict_proba method, but does not check if\\n        # wrapped estimator has a predict_proba method.\\n        return hasattr(clf.estimator, \\'predict_proba\\')\\n    return True', 'def predict_proba(estimator, X):\\n    # type: (Any, Any) -> Optional[np.ndarray]\\n    \"\"\" Return result of predict_proba, if an estimator supports it, or None.\\n    \"\"\"\\n    if is_probabilistic_classifier(estimator):\\n        try:\\n            proba, = estimator.predict_proba(X)\\n            return proba\\n        except NotImplementedError:\\n            return None\\n    else:\\n        return None', 'def has_intercept(estimator):\\n    # type: (Any) -> bool\\n    \"\"\" Return True if an estimator has intercept fit. \"\"\"\\n    if hasattr(estimator, \\'fit_intercept\\'):\\n        return estimator.fit_intercept\\n    if hasattr(estimator, \\'intercept_\\'):\\n        if estimator.intercept_ is None:\\n            return False\\n        # scikit-learn sets intercept to zero vector if it is not fit\\n        return np.any(estimator.intercept_)\\n    return False', 'def get_feature_names(clf, vec=None, bias_name=\\'<BIAS>\\', feature_names=None,\\n                      num_features=None, estimator_feature_names=None):\\n    # type: (Any, Any, Optional[str], Any, int, Any) -> FeatureNames\\n    \"\"\"\\n    Return a FeatureNames instance that holds all feature names\\n    and a bias feature.\\n    If vec is None or doesn\\'t have get_feature_names() method,\\n    features are named x0, x1, x2, etc.\\n    \"\"\"\\n    if not has_intercept(clf):\\n        bias_name = None\\n\\n    if feature_names is None:\\n        if vec and hasattr(vec, \\'get_feature_names\\'):\\n            return FeatureNames(vec.get_feature_names(), bias_name=bias_name)\\n        else:\\n            if estimator_feature_names is None:\\n                num_features = num_features or get_num_features(clf)\\n                return FeatureNames(\\n                    n_features=num_features,\\n                    unkn_template=\\'x%d\\',\\n                    bias_name=bias_name\\n                )\\n            return FeatureNames(estimator_feature_names, bias_name=bias_name)\\n\\n    num_features = num_features or get_num_features(clf)\\n    if isinstance(feature_names, FeatureNames):\\n        if feature_names.n_features != num_features:\\n            raise ValueError(\"feature_names has a wrong n_features: \"\\n                             \"expected=%d, got=%d\" % (num_features,\\n                                                      feature_names.n_features))\\n        # Make a shallow copy setting proper bias_name\\n        return FeatureNames(\\n            feature_names.feature_names,\\n            n_features=num_features,\\n            bias_name=bias_name,\\n            unkn_template=feature_names.unkn_template)\\n    else:\\n        if len(feature_names) != num_features:\\n            raise ValueError(\"feature_names has a wrong length: \"\\n                             \"expected=%d, got=%d\" % (num_features,\\n                                                      len(feature_names)))\\n        return FeatureNames(feature_names, bias_name=bias_name)', 'def get_default_target_names(estimator, num_targets=None):\\n    \"\"\"\\n    Return a vector of target names: \"y\" if there is only one target,\\n    and \"y0\", \"y1\", ... if there are multiple targets.\\n    \"\"\"\\n    if num_targets is None:\\n        if len(estimator.coef_.shape) <= 1:\\n            num_targets = 1\\n        else:\\n            num_targets, _ = estimator.coef_.shape\\n    if num_targets == 1:\\n        target_names = [\\'y\\']\\n    else:\\n        target_names = [\\'y%d\\' % i for i in range(num_targets)]\\n    return np.array(target_names)', 'def get_coef(clf, label_id, scale=None):\\n    \"\"\"\\n    Return a vector of coefficients for a given label,\\n    including bias feature.\\n\\n    ``scale`` (optional) is a scaling vector; coef_[i] => coef[i] * scale[i] if\\n    scale[i] is not nan. Intercept is not scaled.\\n    \"\"\"\\n    if len(clf.coef_.shape) == 2:\\n        # Most classifiers (even in binary case) and regressors\\n        coef = _dense_1d(clf.coef_[label_id])\\n    elif len(clf.coef_.shape) == 1:\\n        # SGDRegressor stores coefficients in a 1D array\\n        if label_id != 0:\\n            raise ValueError(\\n                \\'Unexpected label_id %s for 1D coefficient\\' % label_id)\\n        coef = _dense_1d(clf.coef_)\\n    elif len(clf.coef_.shape) == 0:\\n        # Lasso with one feature: 0D array\\n        coef = np.array([clf.coef_])\\n    else:\\n        raise ValueError(\\'Unexpected clf.coef_ shape: %s\\' % clf.coef_.shape)\\n\\n    if scale is not None:\\n        if coef.shape != scale.shape:\\n            raise ValueError(\"scale shape is incorrect: expected %s, got %s\" % (\\n                coef.shape, scale.shape,\\n            ))\\n        # print(\"shape is ok\")\\n        not_nan = ~np.isnan(scale)\\n        coef = coef.copy()\\n        coef[not_nan] *= scale[not_nan]\\n\\n    if not has_intercept(clf):\\n        return coef\\n    if label_id == 0 and not isinstance(clf.intercept_, np.ndarray):\\n        bias = clf.intercept_\\n    else:\\n        bias = clf.intercept_[label_id]\\n    return np.hstack([coef, bias])', 'def get_num_features(estimator):\\n    \"\"\" Return size of a feature vector estimator expects as an input. \"\"\"\\n    if hasattr(estimator, \\'coef_\\'):  # linear models\\n        if len(estimator.coef_.shape) == 0:\\n            return 1\\n        return estimator.coef_.shape[-1]\\n    elif hasattr(estimator, \\'feature_importances_\\'):  # ensembles\\n        return estimator.feature_importances_.shape[-1]\\n    elif hasattr(estimator, \\'feature_count_\\'):  # naive bayes\\n        return estimator.feature_count_.shape[-1]\\n    elif hasattr(estimator, \\'theta_\\'):\\n        return estimator.theta_.shape[-1]\\n    elif hasattr(estimator, \\'estimators_\\') and len(estimator.estimators_):\\n        # OvR\\n        return get_num_features(estimator.estimators_[0])\\n    else:\\n        raise ValueError(\"Can\\'t figure out feature vector size for %s\" %\\n                         estimator)', 'def get_X0(X):\\n    \"\"\" Return zero-th element of a one-element data container.\\n    \"\"\"\\n    if pandas_available and isinstance(X, pd.DataFrame):\\n        assert len(X) == 1\\n        x = np.array(X.iloc[0])\\n    else:\\n        x, = X\\n    return x', 'def add_intercept(X):\\n    \"\"\" Add intercept column to X \"\"\"\\n    intercept = np.ones((X.shape[0], 1))\\n    if sp.issparse(X):\\n        return sp.hstack([X, intercept]).tocsr()\\n    else:\\n        return np.hstack([X, intercept])', 'def explain_weights_sklearn_crfsuite(crf,\\n                                     top=20,\\n                                     target_names=None,\\n                                     targets=None,\\n                                     feature_re=None,\\n                                     feature_filter=None):\\n    \"\"\" Explain sklearn_crfsuite.CRF weights.\\n\\n    See :func:`eli5.explain_weights` for description of\\n    ``top``, ``target_names``, ``targets``,\\n    ``feature_re`` and ``feature_filter`` parameters.\\n    \"\"\"\\n    feature_names = np.array(crf.attributes_)\\n    state_coef = crf_state_coef(crf).todense().A\\n    transition_coef = crf_transition_coef(crf)\\n\\n    if feature_filter is not None or feature_re is not None:\\n        state_feature_names, flt_indices = (\\n            FeatureNames(feature_names).handle_filter(feature_filter, feature_re))\\n        state_feature_names = np.array(state_feature_names.feature_names)\\n        state_coef = state_coef[:, flt_indices]\\n    else:\\n        state_feature_names = feature_names\\n\\n    def _features(label_id):\\n        return get_top_features(state_feature_names, state_coef[label_id], top)\\n\\n    if targets is None:\\n        targets = sorted_for_ner(crf.classes_)\\n\\n    display_names = get_target_display_names(crf.classes_, target_names,\\n                                             targets)\\n    indices, names = zip(*display_names)\\n    transition_coef = filter_transition_coefs(transition_coef, indices)\\n\\n    return Explanation(\\n        targets=[\\n            TargetExplanation(\\n                target=label,\\n                feature_weights=_features(label_id)\\n            )\\n            for label_id, label in zip(indices, names)\\n        ],\\n        transition_features=TransitionFeatureWeights(\\n            class_names=names,\\n            coef=transition_coef,\\n        ),\\n        estimator=repr(crf),\\n        method=\\'CRF\\',\\n    )', 'def filter_transition_coefs(transition_coef, indices):\\n    \"\"\"\\n    >>> coef = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\\n    >>> filter_transition_coefs(coef, [0])\\n    array([[0]])\\n    >>> filter_transition_coefs(coef, [1, 2])\\n    array([[4, 5],\\n           [7, 8]])\\n    >>> filter_transition_coefs(coef, [2, 0])\\n    array([[8, 6],\\n           [2, 0]])\\n    >>> filter_transition_coefs(coef, [0, 1, 2])\\n    array([[0, 1, 2],\\n           [3, 4, 5],\\n           [6, 7, 8]])\\n    \"\"\"\\n    indices = np.array(indices)\\n    rows = transition_coef[indices]\\n    return rows[:,indices]', 'def sorted_for_ner(crf_classes):\\n    \"\"\"\\n    Return labels sorted in a default order suitable for NER tasks:\\n\\n    >>> sorted_for_ner([\\'B-ORG\\', \\'B-PER\\', \\'O\\', \\'I-PER\\'])\\n    [\\'O\\', \\'B-ORG\\', \\'B-PER\\', \\'I-PER\\']\\n    \"\"\"\\n    def key(cls):\\n        if len(cls) > 2 and cls[1] == \\'-\\':\\n            # group names like B-ORG and I-ORG together\\n            return cls.split(\\'-\\', 1)[1], cls\\n        return \\'\\', cls\\n    return sorted(crf_classes, key=key)', 'def _numpy_to_python(obj):\\n    \"\"\" Convert an nested dict/list/tuple that might contain numpy objects\\n    to their python equivalents. Return converted object.\\n    \"\"\"\\n    if isinstance(obj, dict):\\n        return {k: _numpy_to_python(v) for k, v in obj.items()}\\n    elif isinstance(obj, (list, tuple, np.ndarray)):\\n        return [_numpy_to_python(x) for x in obj]\\n    elif isinstance(obj, FormattedFeatureName):\\n        return obj.value\\n    elif isinstance(obj, _numpy_string_types):\\n        return six.text_type(obj)\\n    elif hasattr(obj, \\'dtype\\') and np.isscalar(obj):\\n        if np.issubdtype(obj, np.floating):\\n            return float(obj)\\n        elif np.issubdtype(obj, np.integer):\\n            return int(obj)\\n        elif np.issubdtype(obj, np.bool_):\\n            return bool(obj)\\n    return obj', 'def _sampler_n_samples(self, n_samples):\\n        \"\"\" Return (sampler, n_samplers) tuples \"\"\"\\n        sampler_indices = self.rng_.choice(range(len(self.samplers)),\\n                                           size=n_samples,\\n                                           replace=True,\\n                                           p=self.weights)\\n        return [\\n            (self.samplers[idx], freq)\\n            for idx, freq in itemfreq(sampler_indices)\\n        ]', 'def sample_near(self, doc, n_samples=1):\\n        \"\"\"\\n        Sample near the document by replacing some of its features\\n        with values sampled from distribution found by KDE.\\n        \"\"\"\\n        doc = np.asarray(doc)\\n        num_features = len(self.kdes_)\\n        sizes = self.rng_.randint(low=1, high=num_features + 1, size=n_samples)\\n        samples = []\\n        for size in sizes:\\n            to_change = self.rng_.choice(num_features, size, replace=False)\\n            new_doc = doc.copy()\\n            for i in to_change:\\n                kde = self.kdes_[i]\\n                new_doc[i] = kde.sample(random_state=self.rng_).ravel()\\n            samples.append(new_doc)\\n        samples = np.asarray(samples)\\n        return samples, self._similarity(doc, samples)', 'def _all_feature_names(name):\\n    # type: (Union[str, bytes, List[Dict]]) -> List[str]\\n    \"\"\" All feature names for a feature: usually just the feature itself,\\n    but can be several features for unhashed features with collisions.\\n    \"\"\"\\n    if isinstance(name, bytes):\\n        return [name.decode(\\'utf8\\')]\\n    elif isinstance(name, list):\\n        return [x[\\'name\\'] for x in name]\\n    else:\\n        return [name]', 'def filtered(self, feature_filter, x=None):\\n        # type: (Callable, Any) -> Tuple[FeatureNames, List[int]]\\n        \"\"\" Return feature names filtered by a regular expression \\n        ``feature_re``, and indices of filtered elements.\\n        \"\"\"\\n        indices = []\\n        filtered_feature_names = []\\n        indexed_names = None  # type: Optional[Iterable[Tuple[int, Any]]]\\n        if isinstance(self.feature_names, (np.ndarray, list)):\\n            indexed_names = enumerate(self.feature_names)\\n        elif isinstance(self.feature_names, dict):\\n            indexed_names = six.iteritems(self.feature_names)\\n        elif self.feature_names is None:\\n            indexed_names = []\\n        assert indexed_names is not None\\n\\n        if x is not None:\\n            if sp.issparse(x) and len(x.shape) == 2:\\n                assert x.shape[0] == 1\\n                flt = lambda nm, i: feature_filter(nm, x[0, i])\\n            else:\\n                # FIXME: mypy warns about x[i] because it thinks x can be None\\n                flt = lambda nm, i: feature_filter(nm, x[i])  # type: ignore\\n        else:\\n            flt = lambda nm, i: feature_filter(nm)\\n\\n        for idx, name in indexed_names:\\n            if any(flt(nm, idx) for nm in _all_feature_names(name)):\\n                indices.append(idx)\\n                filtered_feature_names.append(name)\\n        if self.has_bias and flt(self.bias_name, self.bias_idx):\\n            assert self.bias_idx is not None  # for mypy\\n            bias_name = self.bias_name\\n            indices.append(self.bias_idx)\\n        else:\\n            bias_name = None\\n        return (\\n            FeatureNames(\\n                filtered_feature_names,\\n                bias_name=bias_name,\\n                unkn_template=self.unkn_template,\\n            ),\\n            indices)', 'def add_feature(self, feature):\\n        # type: (Any) -> int\\n        \"\"\" Add a new feature name, return it\\'s index.\\n        \"\"\"\\n        # A copy of self.feature_names is always made, because it might be\\n        # \"owned\" by someone else.\\n        # It\\'s possible to make the copy only at the first call to\\n        # self.add_feature to improve performance.\\n        idx = self.n_features\\n        if isinstance(self.feature_names, (list, np.ndarray)):\\n            self.feature_names = list(self.feature_names)\\n            self.feature_names.append(feature)\\n        elif isinstance(self.feature_names, dict):\\n            self.feature_names = dict(self.feature_names)\\n            self.feature_names[idx] = feature\\n        elif self.feature_names is None:\\n            self.feature_names = {idx: feature}\\n        self.n_features += 1\\n        return idx', 'def format_as_text(expl,  # type: Explanation\\n                   show=fields.ALL,\\n                   highlight_spaces=None,  # type: Optional[bool]\\n                   show_feature_values=False,  # type: bool\\n                   ):\\n    # type: (...) -> str\\n    \"\"\" Format explanation as text.\\n\\n    Parameters\\n    ----------\\n    expl : eli5.base.Explanation\\n        Explanation returned by ``eli5.explain_weights`` or\\n        ``eli5.explain_prediction`` functions.\\n\\n    highlight_spaces : bool or None, optional\\n        Whether to highlight spaces in feature names. This is useful if\\n        you work with text and have ngram features which may include spaces\\n        at left or right. Default is None, meaning that the value used\\n        is set automatically based on vectorizer and feature values.\\n\\n    show_feature_values : bool\\n        When True, feature values are shown along with feature contributions.\\n        Default is False.\\n\\n    show : List[str], optional\\n        List of sections to show. Allowed values:\\n\\n        * \\'targets\\' - per-target feature weights;\\n        * \\'transition_features\\' - transition features of a CRF model;\\n        * \\'feature_importances\\' - feature importances of a decision tree or\\n          an ensemble-based estimator;\\n        * \\'decision_tree\\' - decision tree in a graphical form;\\n        * \\'method\\' - a string with explanation method;\\n        * \\'description\\' - description of explanation method and its caveats.\\n\\n        ``eli5.formatters.fields`` provides constants that cover common cases:\\n        ``INFO`` (method and description), ``WEIGHTS`` (all the rest),\\n        and ``ALL`` (all).\\n    \"\"\"\\n    lines = []  # type: List[str]\\n\\n    if highlight_spaces is None:\\n        highlight_spaces = should_highlight_spaces(expl)\\n\\n    if expl.error:  # always shown\\n        lines.extend(_error_lines(expl))\\n\\n    explaining_prediction = has_any_values_for_weights(expl)\\n    show_feature_values = show_feature_values and explaining_prediction\\n\\n    for key in show:\\n        if not getattr(expl, key, None):\\n            continue\\n\\n        if key == \\'method\\':\\n            lines.extend(_method_lines(expl))\\n\\n        if key == \\'description\\':\\n            lines.extend(_description_lines(expl))\\n\\n        if key == \\'transition_features\\':\\n            lines.extend(_transition_features_lines(expl))\\n\\n        if key == \\'targets\\':\\n            lines.extend(_targets_lines(\\n                expl,\\n                hl_spaces=highlight_spaces,\\n                show_feature_values=show_feature_values,\\n                explaining_prediction=explaining_prediction,\\n            ))\\n\\n        if key == \\'feature_importances\\':\\n            lines.extend(_feature_importances_lines(\\n                expl, hl_spaces=highlight_spaces))\\n\\n        if key == \\'decision_tree\\':\\n            lines.extend(_decision_tree_lines(expl))\\n\\n    return \\'\\\\n\\'.join(lines)', 'def _format_unhashed_feature(name, hl_spaces, sep=\\' | \\'):\\n    # type: (List, bool, str) -> str\\n    \"\"\"\\n    Format feature name for hashed features.\\n    \"\"\"\\n    return sep.join(\\n        format_signed(n, _format_single_feature, hl_spaces=hl_spaces)\\n        for n in name)', 'def _get_top_features(feature_names, coef, top, x):\\n    \"\"\"\\n    Return a ``(pos, neg)`` tuple. ``pos`` and ``neg`` are lists of\\n    ``(name, value)`` tuples for features with positive and negative\\n    coefficients.\\n\\n    Parameters:\\n\\n    * ``feature_names`` - a vector of feature names;\\n    * ``coef`` - coefficient vector; coef.shape must be equal to\\n      feature_names.shape;\\n    * ``top`` can be either a number or a ``(num_pos, num_neg)`` tuple.\\n      If ``top`` is a number, ``top`` features with largest absolute\\n      coefficients are returned. If it is a ``(num_pos, num_neg)`` tuple,\\n      the function returns no more than ``num_pos`` positive features and\\n      no more than ``num_neg`` negative features. ``None`` value means\\n      \\'no limit\\'.\\n    * ``x`` is a vector of feature values, passed to FeatureWeight.value.\\n    \"\"\"\\n    if isinstance(top, (list, tuple)):\\n        num_pos, num_neg = list(top)  # \"list\" is just for mypy\\n        pos = _get_top_positive_features(feature_names, coef, num_pos, x)\\n        neg = _get_top_negative_features(feature_names, coef, num_neg, x)\\n    else:\\n        pos, neg = _get_top_abs_features(feature_names, coef, top, x)\\n    return pos, neg', 'def get_char_weights(doc_weighted_spans, preserve_density=None):\\n    # type: (DocWeightedSpans, Optional[bool]) -> np.ndarray\\n    \"\"\" Return character weights for a text document with highlighted features.\\n    If preserve_density is True, then color for longer fragments will be\\n    less intensive than for shorter fragments, so that \"sum\" of intensities\\n    will correspond to feature weight.\\n    If preserve_density is None, then it\\'s value is taken from\\n    the corresponding attribute of doc_weighted_spans.\\n    \"\"\"\\n    if preserve_density is None:\\n        preserve_density = doc_weighted_spans.preserve_density\\n    char_weights = np.zeros(len(doc_weighted_spans.document))\\n    feature_counts = Counter(f for f, _, __ in doc_weighted_spans.spans)\\n    for feature, spans, weight in doc_weighted_spans.spans:\\n        for start, end in spans:\\n            # start can be -1 for char_wb at the start of the document.\\n            start = max(0, start)\\n            if preserve_density:\\n                weight /= (end - start)\\n            weight /= feature_counts[feature]\\n            char_weights[start:end] += weight\\n    return char_weights', 'def prepare_weighted_spans(targets,  # type: List[TargetExplanation]\\n                           preserve_density=None,  # type: Optional[bool]\\n                           ):\\n    # type: (...) -> List[Optional[List[PreparedWeightedSpans]]]\\n    \"\"\" Return weighted spans prepared for rendering.\\n    Calculate a separate weight range for each different weighted\\n    span (for each different index): each target has the same number\\n    of weighted spans.\\n    \"\"\"\\n    targets_char_weights = [\\n        [get_char_weights(ws, preserve_density=preserve_density)\\n         for ws in t.weighted_spans.docs_weighted_spans]\\n         if t.weighted_spans else None\\n         for t in targets]  # type: List[Optional[List[np.ndarray]]]\\n    max_idx = max_or_0(len(ch_w or []) for ch_w in targets_char_weights)\\n\\n    targets_char_weights_not_None = [\\n        cw for cw in targets_char_weights\\n        if cw is not None]  # type: List[List[np.ndarray]]\\n\\n    spans_weight_ranges = [\\n        max_or_0(\\n            abs(x) for char_weights in targets_char_weights_not_None\\n            for x in char_weights[idx])\\n        for idx in range(max_idx)]\\n    return [\\n        [PreparedWeightedSpans(ws, char_weights, weight_range)\\n         for ws, char_weights, weight_range in zip(\\n            t.weighted_spans.docs_weighted_spans,  # type: ignore\\n            t_char_weights,\\n            spans_weight_ranges)]\\n        if t_char_weights is not None else None\\n        for t, t_char_weights in zip(targets, targets_char_weights)]', 'def build_span_analyzer(document, vec):\\n    \"\"\" Return an analyzer and the preprocessed doc.\\n    Analyzer will yield pairs of spans and feature, where spans are pairs\\n    of indices into the preprocessed doc. The idea here is to do minimal\\n    preprocessing so that we can still recover the same features as sklearn\\n    vectorizers, but with spans, that will allow us to highlight\\n    features in preprocessed documents.\\n    Analyzers are adapted from VectorizerMixin from sklearn.\\n    \"\"\"\\n    preprocessed_doc = vec.build_preprocessor()(vec.decode(document))\\n    analyzer = None\\n    if vec.analyzer == \\'word\\' and vec.tokenizer is None:\\n        stop_words = vec.get_stop_words()\\n        tokenize = _build_tokenizer(vec)\\n        analyzer = lambda doc: _word_ngrams(vec, tokenize(doc), stop_words)\\n    elif vec.analyzer == \\'char\\':\\n        preprocessed_doc = vec._white_spaces.sub(\\' \\', preprocessed_doc)\\n        analyzer = lambda doc: _char_ngrams(vec, doc)\\n    elif vec.analyzer == \\'char_wb\\':\\n        preprocessed_doc = vec._white_spaces.sub(\\' \\', preprocessed_doc)\\n        analyzer = lambda doc: _char_wb_ngrams(vec, doc)\\n    return analyzer, preprocessed_doc', 'def explain_weights_xgboost(xgb,\\n                            vec=None,\\n                            top=20,\\n                            target_names=None,  # ignored\\n                            targets=None,  # ignored\\n                            feature_names=None,\\n                            feature_re=None,  # type: Pattern[str]\\n                            feature_filter=None,\\n                            importance_type=\\'gain\\',\\n                            ):\\n    \"\"\"\\n    Return an explanation of an XGBoost estimator (via scikit-learn wrapper\\n    XGBClassifier or XGBRegressor, or via xgboost.Booster)\\n    as feature importances.\\n\\n    See :func:`eli5.explain_weights` for description of\\n    ``top``, ``feature_names``,\\n    ``feature_re`` and ``feature_filter`` parameters.\\n\\n    ``target_names`` and ``targets`` parameters are ignored.\\n\\n    Parameters\\n    ----------\\n    importance_type : str, optional\\n        A way to get feature importance. Possible values are:\\n\\n        - \\'gain\\' - the average gain of the feature when it is used in trees\\n          (default)\\n        - \\'weight\\' - the number of times a feature is used to split the data\\n          across all trees\\n        - \\'cover\\' - the average coverage of the feature when it is used in trees\\n    \"\"\"\\n    booster, is_regression = _check_booster_args(xgb)\\n    xgb_feature_names = booster.feature_names\\n    coef = _xgb_feature_importances(booster, importance_type=importance_type)\\n    return get_feature_importance_explanation(\\n        xgb, vec, coef,\\n        feature_names=feature_names,\\n        estimator_feature_names=xgb_feature_names,\\n        feature_filter=feature_filter,\\n        feature_re=feature_re,\\n        top=top,\\n        description=DESCRIPTION_XGBOOST,\\n        is_regression=is_regression,\\n        num_features=coef.shape[-1],\\n    )', 'def explain_prediction_xgboost(\\n        xgb, doc,\\n        vec=None,\\n        top=None,\\n        top_targets=None,\\n        target_names=None,\\n        targets=None,\\n        feature_names=None,\\n        feature_re=None,  # type: Pattern[str]\\n        feature_filter=None,\\n        vectorized=False,  # type: bool\\n        is_regression=None,  # type: bool\\n        missing=None,  # type: bool\\n        ):\\n    \"\"\" Return an explanation of XGBoost prediction (via scikit-learn wrapper\\n    XGBClassifier or XGBRegressor, or via xgboost.Booster) as feature weights.\\n\\n    See :func:`eli5.explain_prediction` for description of\\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\\n\\n    Parameters\\n    ----------\\n    vec : vectorizer, optional\\n        A vectorizer instance used to transform\\n        raw features to the input of the estimator ``xgb``\\n        (e.g. a fitted CountVectorizer instance); you can pass it\\n        instead of ``feature_names``.\\n\\n    vectorized : bool, optional\\n        A flag which tells eli5 if ``doc`` should be\\n        passed through ``vec`` or not. By default it is False, meaning that\\n        if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\\n        estimator. Set it to True if you\\'re passing ``vec``,\\n        but ``doc`` is already vectorized.\\n\\n    is_regression : bool, optional\\n        Pass if an ``xgboost.Booster`` is passed as the first argument.\\n        True if solving a regression problem (\"objective\" starts with \"reg\")\\n        and False for a classification problem.\\n        If not set, regression is assumed for a single target estimator\\n        and proba will not be shown.\\n\\n    missing : optional\\n        Pass if an ``xgboost.Booster`` is passed as the first argument.\\n        Set it to the same value as the ``missing`` argument to\\n        ``xgboost.DMatrix``.\\n        Matters only if sparse values are used. Default is ``np.nan``.\\n\\n    Method for determining feature importances follows an idea from\\n    http://blog.datadive.net/interpreting-random-forests/.\\n    Feature weights are calculated by following decision paths in trees\\n    of an ensemble.\\n    Each leaf has an output score, and expected scores can also be assigned\\n    to parent nodes.\\n    Contribution of one feature on the decision path is how much expected score\\n    changes from parent to child.\\n    Weights of all features sum to the output score of the estimator.\\n    \"\"\"\\n    booster, is_regression = _check_booster_args(xgb, is_regression)\\n    xgb_feature_names = booster.feature_names\\n    vec, feature_names = handle_vec(\\n        xgb, doc, vec, vectorized, feature_names,\\n        num_features=len(xgb_feature_names))\\n    if feature_names.bias_name is None:\\n        # XGBoost estimators do not have an intercept, but here we interpret\\n        # them as having an intercept\\n        feature_names.bias_name = \\'<BIAS>\\'\\n\\n    X = get_X(doc, vec, vectorized=vectorized)\\n    if sp.issparse(X):\\n        # Work around XGBoost issue:\\n        # https://github.com/dmlc/xgboost/issues/1238#issuecomment-243872543\\n        X = X.tocsc()\\n\\n    if missing is None:\\n        missing = np.nan if isinstance(xgb, Booster) else xgb.missing\\n    dmatrix = DMatrix(X, missing=missing)\\n\\n    if isinstance(xgb, Booster):\\n        prediction = xgb.predict(dmatrix)\\n        n_targets = prediction.shape[-1]  # type: int\\n        if is_regression is None:\\n            # When n_targets is 1, this can be classification too,\\n            # but it\\'s safer to assume regression.\\n            # If n_targets > 1, it must be classification.\\n            is_regression = n_targets == 1\\n        if is_regression:\\n            proba = None\\n        else:\\n            if n_targets == 1:\\n                p, = prediction\\n                proba = np.array([1 - p, p])\\n            else:\\n                proba, = prediction\\n    else:\\n        proba = predict_proba(xgb, X)\\n        n_targets = _xgb_n_targets(xgb)\\n\\n    if is_regression:\\n        names = [\\'y\\']\\n    elif isinstance(xgb, Booster):\\n        names = np.arange(max(2, n_targets))\\n    else:\\n        names = xgb.classes_\\n\\n    scores_weights = _prediction_feature_weights(\\n        booster, dmatrix, n_targets, feature_names, xgb_feature_names)\\n\\n    x = get_X0(add_intercept(X))\\n    x = _missing_values_set_to_nan(x, missing, sparse_missing=True)\\n\\n    return get_decision_path_explanation(\\n        xgb, doc, vec,\\n        x=x,\\n        feature_names=feature_names,\\n        feature_filter=feature_filter,\\n        feature_re=feature_re,\\n        top=top,\\n        vectorized=vectorized,\\n        original_display_names=names,\\n        target_names=target_names,\\n        targets=targets,\\n        top_targets=top_targets,\\n        is_regression=is_regression,\\n        is_multiclass=n_targets > 1,\\n        proba=proba,\\n        get_score_weights=lambda label_id: scores_weights[label_id],\\n     )', 'def _prediction_feature_weights(booster, dmatrix, n_targets,\\n                                feature_names, xgb_feature_names):\\n    \"\"\" For each target, return score and numpy array with feature weights\\n    on this prediction, following an idea from\\n    http://blog.datadive.net/interpreting-random-forests/\\n    \"\"\"\\n    # XGBClassifier does not have pred_leaf argument, so use booster\\n    leaf_ids, = booster.predict(dmatrix, pred_leaf=True)\\n    xgb_feature_names = {f: i for i, f in enumerate(xgb_feature_names)}\\n    tree_dumps = booster.get_dump(with_stats=True)\\n    assert len(tree_dumps) == len(leaf_ids)\\n\\n    target_feature_weights = partial(\\n        _target_feature_weights,\\n        feature_names=feature_names, xgb_feature_names=xgb_feature_names)\\n    if n_targets > 1:\\n        # For multiclass, XGBoost stores dumps and leaf_ids in a 1d array,\\n        # so we need to split them.\\n        scores_weights = [\\n            target_feature_weights(\\n                leaf_ids[target_idx::n_targets],\\n                tree_dumps[target_idx::n_targets],\\n            ) for target_idx in range(n_targets)]\\n    else:\\n        scores_weights = [target_feature_weights(leaf_ids, tree_dumps)]\\n    return scores_weights', 'def _indexed_leafs(parent):\\n    \"\"\" Return a leaf nodeid -> node dictionary with\\n    \"parent\" and \"leaf\" (average child \"leaf\" value) added to all nodes.\\n    \"\"\"\\n    if not parent.get(\\'children\\'):\\n        return {parent[\\'nodeid\\']: parent}\\n    indexed = {}\\n    for child in parent[\\'children\\']:\\n        child[\\'parent\\'] = parent\\n        if \\'leaf\\' in child:\\n            indexed[child[\\'nodeid\\']] = child\\n        else:\\n            indexed.update(_indexed_leafs(child))\\n    parent[\\'leaf\\'] = _parent_value(parent[\\'children\\'])\\n    return indexed', 'def _parent_value(children):\\n    # type: (...) -> int\\n    \"\"\" Value of the parent node: a weighted sum of child values.\\n    \"\"\"\\n    covers = np.array([child[\\'cover\\'] for child in children])\\n    covers /= np.sum(covers)\\n    leafs = np.array([child[\\'leaf\\'] for child in children])\\n    return np.sum(leafs * covers)', 'def _parse_tree_dump(text_dump):\\n    # type: (str) -> Optional[Dict[str, Any]]\\n    \"\"\" Parse text tree dump (one item of a list returned by Booster.get_dump())\\n    into json format that will be used by next XGBoost release.\\n    \"\"\"\\n    result = None\\n    stack = []  # type: List[Dict]\\n    for line in text_dump.split(\\'\\\\n\\'):\\n        if line:\\n            depth, node = _parse_dump_line(line)\\n            if depth == 0:\\n                assert not stack\\n                result = node\\n                stack.append(node)\\n            elif depth > len(stack):\\n                raise ValueError(\\'Unexpected dump structure\\')\\n            else:\\n                if depth < len(stack):\\n                    stack = stack[:depth]\\n                stack[-1].setdefault(\\'children\\', []).append(node)\\n                stack.append(node)\\n    return result', 'def _missing_values_set_to_nan(values, missing_value, sparse_missing):\\n    \"\"\" Return a copy of values where missing values (equal to missing_value)\\n    are replaced to nan according. If sparse_missing is True,\\n    entries missing in a sparse matrix will also be set to nan.\\n    Sparse matrices will be converted to dense format.\\n    \"\"\"\\n    if sp.issparse(values):\\n        assert values.shape[0] == 1\\n    if sparse_missing and sp.issparse(values) and missing_value != 0:\\n        # Nothing special needs to be done for missing.value == 0 because\\n        # missing values are assumed to be zero in sparse matrices.\\n        values_coo = values.tocoo()\\n        values = values.toarray()[0]\\n        missing_mask = values == 0\\n        # fix for possible zero values\\n        missing_mask[values_coo.col] = False\\n        values[missing_mask] = np.nan\\n    elif is_sparse_vector(values):\\n        values = values.toarray()[0]\\n    else:\\n        values = values.copy()\\n    if not np.isnan(missing_value):\\n        values[values == missing_value] = np.nan\\n    return values', 'def argsort_k_smallest(x, k):\\n    \"\"\" Return no more than ``k`` indices of smallest values. \"\"\"\\n    if k == 0:\\n        return np.array([], dtype=np.intp)\\n    if k is None or k >= len(x):\\n        return np.argsort(x)\\n    indices = np.argpartition(x, k)[:k]\\n    values = x[indices]\\n    return indices[np.argsort(values)]', 'def mask(x, indices):\\n    \"\"\"\\n    The same as x[indices], but return an empty array if indices are empty,\\n    instead of returning all x elements,\\n    and handles sparse \"vectors\".\\n    \"\"\"\\n    indices_shape = (\\n        [len(indices)] if isinstance(indices, list) else indices.shape)\\n    if not indices_shape[0]:\\n        return np.array([])\\n    elif is_sparse_vector(x) and len(indices_shape) == 1:\\n        return x[0, indices].toarray()[0]\\n    else:\\n        return x[indices]', 'def is_sparse_vector(x):\\n    \"\"\" x is a 2D sparse matrix with it\\'s first shape equal to 1.\\n    \"\"\"\\n    return sp.issparse(x) and len(x.shape) == 2 and x.shape[0] == 1', 'def indices_to_bool_mask(indices, size):\\n    \"\"\" Convert indices to a boolean (integer) mask.\\n\\n    >>> list(indices_to_bool_mask(np.array([2, 3]), 4))\\n    [False, False, True, True]\\n\\n    >>> list(indices_to_bool_mask([2, 3], 4))\\n    [False, False, True, True]\\n\\n    >>> indices_to_bool_mask(np.array([5]), 2)\\n    Traceback (most recent call last):\\n    ...\\n    IndexError: index 5 is out of bounds ...\\n    \"\"\"\\n    mask = np.zeros(size, dtype=bool)\\n    mask[indices] = 1\\n    return mask', 'def get_target_display_names(original_names=None, target_names=None,\\n                             targets=None, top_targets=None, score=None):\\n    \"\"\"\\n    Return a list of (target_id, display_name) tuples.\\n\\n    By default original names are passed as-is, only indices are added:\\n    >>> get_target_display_names([\\'x\\', \\'y\\'])\\n    [(0, \\'x\\'), (1, \\'y\\')]\\n\\n    ``targets`` can be written using both names from ``target_names` and\\n    from ``original_names``:\\n    >>> get_target_display_names([\\'x\\', \\'y\\'], targets=[\\'y\\', \\'X\\'],\\n    ...                   target_names={\\'x\\': \\'X\\'})\\n    [(1, \\'y\\'), (0, \\'X\\')]\\n\\n    Provide display names:\\n    >>> get_target_display_names([0, 2], target_names=[\\'foo\\', \\'bar\\'])\\n    [(0, \\'foo\\'), (1, \\'bar\\')]\\n\\n    Change order of labels:\\n    >>> get_target_display_names([\\'x\\', \\'y\\'], targets=[\\'y\\', \\'x\\'])\\n    [(1, \\'y\\'), (0, \\'x\\')]\\n\\n    Provide display names, choose only a subset of labels:\\n    >>> get_target_display_names([0, 2], target_names=[\\'foo\\', \\'bar\\'], targets=[2])\\n    [(1, \\'bar\\')]\\n\\n    >>> get_target_display_names([False, True], targets=[True])\\n    [(1, True)]\\n\\n    >>> get_target_display_names([False, True], targets=[False])\\n    [(0, False)]\\n\\n    target_names can be a dictionary with {old_name: new_name} labels:\\n    >>> get_target_display_names([\\'x\\', \\'y\\'], targets=[\\'y\\', \\'x\\'],\\n    ...                   target_names={\\'x\\': \\'X\\'})\\n    [(1, \\'y\\'), (0, \\'X\\')]\\n\\n    Error is raised when target_names format is invalid:\\n    >>> get_target_display_names([\\'x\\', \\'y\\'], target_names=[\\'foo\\'])\\n    Traceback (most recent call last):\\n    ...\\n    ValueError: target_names must have the same length as original names (expected 2, got 1)\\n\\n    Top target selection by score:\\n    >>> get_target_display_names([\\'x\\', \\'y\\', \\'z\\'], score=[1, 2, 1.5], top_targets=2)\\n    [(1, \\'y\\'), (2, \\'z\\')]\\n\\n    Top target selection by score, negative:\\n    >>> get_target_display_names([\\'x\\', \\'y\\', \\'z\\'], score=[1, 2, 1.5], top_targets=-3)\\n    [(0, \\'x\\'), (2, \\'z\\'), (1, \\'y\\')]\\n\\n    Error is raised if both top_targets and targets are passed:\\n    >>> get_target_display_names([\\'x\\', \\'y\\'], targets=[\\'x\\'], score=[1, 2], top_targets=1)\\n    Traceback (most recent call last):\\n    ...\\n    ValueError: Pass either \"targets\" or \"top_targets\", not both\\n    \"\"\"\\n    if isinstance(target_names, (list, tuple, np.ndarray)):\\n        if original_names is not None:\\n            if len(target_names) != len(original_names):\\n                raise ValueError(\"target_names must have the same length as \"\\n                                 \"original names (expected {}, got {})\".format(\\n                                     len(original_names), len(target_names)\\n                                 ))\\n        display_names = target_names\\n    elif isinstance(target_names, dict):\\n        display_names = [target_names.get(name, name)\\n                         for name in original_names]\\n    else:\\n        display_names = original_names\\n\\n    if targets is None:\\n        if top_targets is not None:\\n            assert len(score) == len(original_names)\\n            if top_targets < 0:\\n                reverse = False\\n                top_targets = -top_targets\\n            else:\\n                reverse = True\\n            targets = [\\n                target for _, target in sorted(\\n                    enumerate(original_names),\\n                    key=lambda x: score[x[0]],\\n                    reverse=reverse,\\n                )][:top_targets]\\n        else:\\n            targets = original_names\\n    elif top_targets is not None:\\n        raise ValueError(\\'Pass either \"targets\" or \"top_targets\", not both\\')\\n\\n    target_indices = _get_value_indices(original_names, display_names, targets)\\n    names = [display_names[i] for i in target_indices]\\n    return list(zip(target_indices, names))', 'def get_binary_target_scale_label_id(score, display_names, proba=None):\\n    \"\"\"\\n    Return (target_name, scale, label_id) tuple for a binary classifier.\\n\\n    >>> get_binary_target_scale_label_id(+5.0, get_target_display_names([False, True]))\\n    (True, 1, 1)\\n    >>> get_binary_target_scale_label_id(-5.0, get_target_display_names([False, True]))\\n    (False, -1, 0)\\n    >>> get_binary_target_scale_label_id(-5.0, get_target_display_names([False, True], targets=[True]))\\n    (True, 1, 1)\\n    >>> get_binary_target_scale_label_id(-5.0, get_target_display_names([False, True], targets=[False]))\\n    (False, -1, 0)\\n    \"\"\"\\n    if score is not None:\\n        label_id = 1 if score >= 0 else 0\\n        scale = -1 if label_id == 0 else 1\\n    else:\\n        # Only probability is available - this is the case for\\n        # DecisionTreeClassifier. As contributions sum to the probability\\n        # (not to the score), they shouldn\\'t be inverted.\\n        label_id = 1 if proba[1] >= 0.5 else 0\\n        scale = 1\\n\\n    if len(display_names) == 1:  # target is passed explicitly\\n        predicted_label_id = label_id\\n        label_id, target = display_names[0]\\n        scale *= -1 if label_id != predicted_label_id else 1\\n    else:\\n        target = display_names[label_id][1]\\n\\n    return target, scale, label_id', 'def _get_value_indices(names1, names2, lookups):\\n    \"\"\"\\n    >>> _get_value_indices([\\'foo\\', \\'bar\\', \\'baz\\'], [\\'foo\\', \\'bar\\', \\'baz\\'],\\n    ...                    [\\'bar\\', \\'foo\\'])\\n    [1, 0]\\n    >>> _get_value_indices([\\'foo\\', \\'bar\\', \\'baz\\'], [\\'FOO\\', \\'bar\\', \\'baz\\'],\\n    ...                    [\\'bar\\', \\'FOO\\'])\\n    [1, 0]\\n    >>> _get_value_indices([\\'foo\\', \\'bar\\', \\'BAZ\\'], [\\'foo\\', \\'BAZ\\', \\'baz\\'],\\n    ...                    [\\'BAZ\\', \\'foo\\'])\\n    [2, 0]\\n    >>> _get_value_indices([\\'foo\\', \\'bar\\', \\'baz\\'], [\\'foo\\', \\'bar\\', \\'baz\\'],\\n    ...                    [\\'spam\\'])\\n    Traceback (most recent call last):\\n    ...\\n    KeyError: \\'spam\\'\\n    \"\"\"\\n    positions = {name: idx for idx, name in enumerate(names2)}\\n    positions.update({name: idx for idx, name in enumerate(names1)})\\n    return [positions[name] for name in lookups]', 'def dot2svg(dot):\\n    # type: (str) -> str\\n    \"\"\" Render Graphviz data to SVG \"\"\"\\n    svg = graphviz.Source(dot).pipe(format=\\'svg\\').decode(\\'utf8\\')  # type: str\\n    # strip doctype and xml declaration\\n    svg = svg[svg.index(\\'<svg\\'):]\\n    return svg', 'def explain_weights_sklearn(estimator, vec=None, top=_TOP,\\n                            target_names=None,\\n                            targets=None,\\n                            feature_names=None, coef_scale=None,\\n                            feature_re=None, feature_filter=None):\\n    \"\"\" Return an explanation of an estimator \"\"\"\\n    return explain_weights_sklearn_not_supported(estimator)', 'def explain_linear_classifier_weights(clf,\\n                                      vec=None,\\n                                      top=_TOP,\\n                                      target_names=None,\\n                                      targets=None,\\n                                      feature_names=None,\\n                                      coef_scale=None,\\n                                      feature_re=None,\\n                                      feature_filter=None,\\n                                      ):\\n    \"\"\"\\n    Return an explanation of a linear classifier weights.\\n\\n    See :func:`eli5.explain_weights` for description of\\n    ``top``, ``target_names``, ``targets``, ``feature_names``,\\n    ``feature_re`` and ``feature_filter`` parameters.\\n\\n    ``vec`` is a vectorizer instance used to transform\\n    raw features to the input of the classifier ``clf``\\n    (e.g. a fitted CountVectorizer instance); you can pass it\\n    instead of ``feature_names``.\\n\\n    ``coef_scale`` is a 1D np.ndarray with a scaling coefficient\\n    for each feature; coef[i] = coef[i] * coef_scale[i] if\\n    coef_scale[i] is not nan. Use it if you want to scale coefficients\\n    before displaying them, to take input feature sign or scale in account.\\n    \"\"\"\\n    feature_names, coef_scale = handle_hashing_vec(vec, feature_names,\\n                                                   coef_scale)\\n    feature_names, flt_indices = get_feature_names_filtered(\\n        clf, vec,\\n        feature_names=feature_names,\\n        feature_filter=feature_filter,\\n        feature_re=feature_re,\\n    )\\n\\n    _extra_caveats = \"\\\\n\" + HASHING_CAVEATS if is_invhashing(vec) else \\'\\'\\n\\n    def _features(label_id):\\n        coef = get_coef(clf, label_id, scale=coef_scale)\\n        if flt_indices is not None:\\n            coef = coef[flt_indices]\\n        return get_top_features(feature_names, coef, top)\\n\\n    classes = getattr(clf, \"classes_\", [\"-1\", \"1\"])  # OneClassSVM support\\n    display_names = get_target_display_names(classes, target_names, targets)\\n    if is_multiclass_classifier(clf):\\n        return Explanation(\\n            targets=[\\n                TargetExplanation(\\n                    target=label,\\n                    feature_weights=_features(label_id)\\n                )\\n                for label_id, label in display_names\\n                ],\\n            description=DESCRIPTION_CLF_MULTICLASS + _extra_caveats,\\n            estimator=repr(clf),\\n            method=\\'linear model\\',\\n        )\\n    else:\\n        # for binary classifiers scikit-learn stores a single coefficient\\n        # vector, which corresponds to clf.classes_[1].\\n        return Explanation(\\n            targets=[\\n                TargetExplanation(\\n                    target=display_names[1][1],\\n                    feature_weights=_features(0),\\n                )\\n            ],\\n            description=DESCRIPTION_CLF_BINARY + _extra_caveats,\\n            estimator=repr(clf),\\n            method=\\'linear model\\',\\n        )', 'def explain_rf_feature_importance(estimator,\\n                                  vec=None,\\n                                  top=_TOP,\\n                                  target_names=None,  # ignored\\n                                  targets=None,  # ignored\\n                                  feature_names=None,\\n                                  feature_re=None,\\n                                  feature_filter=None,\\n                                  ):\\n    \"\"\"\\n    Return an explanation of a tree-based ensemble estimator.\\n\\n    See :func:`eli5.explain_weights` for description of\\n    ``top``, ``feature_names``, ``feature_re`` and ``feature_filter``\\n    parameters.\\n\\n    ``target_names`` and ``targets`` parameters are ignored.\\n\\n    ``vec`` is a vectorizer instance used to transform\\n    raw features to the input of the estimator (e.g. a fitted\\n    CountVectorizer instance); you can pass it instead of ``feature_names``.\\n    \"\"\"\\n    coef = estimator.feature_importances_\\n    trees = np.array(estimator.estimators_).ravel()\\n    coef_std = np.std([tree.feature_importances_ for tree in trees], axis=0)\\n    return get_feature_importance_explanation(estimator, vec, coef,\\n        coef_std=coef_std,\\n        feature_names=feature_names,\\n        feature_filter=feature_filter,\\n        feature_re=feature_re,\\n        top=top,\\n        description=DESCRIPTION_RANDOM_FOREST,\\n        is_regression=isinstance(estimator, RegressorMixin),\\n    )', 'def explain_decision_tree(estimator,\\n                          vec=None,\\n                          top=_TOP,\\n                          target_names=None,\\n                          targets=None,  # ignored\\n                          feature_names=None,\\n                          feature_re=None,\\n                          feature_filter=None,\\n                          **export_graphviz_kwargs):\\n    \"\"\"\\n    Return an explanation of a decision tree.\\n\\n    See :func:`eli5.explain_weights` for description of\\n    ``top``, ``target_names``, ``feature_names``,\\n    ``feature_re`` and ``feature_filter`` parameters.\\n\\n    ``targets`` parameter is ignored.\\n\\n    ``vec`` is a vectorizer instance used to transform\\n    raw features to the input of the estimator (e.g. a fitted\\n    CountVectorizer instance); you can pass it instead of ``feature_names``.\\n\\n    All other keyword arguments are passed to\\n    `sklearn.tree.export_graphviz`_ function.\\n\\n    .. _sklearn.tree.export_graphviz: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\n    \"\"\"\\n    feature_names = get_feature_names(estimator, vec,\\n                                      feature_names=feature_names)\\n    tree_feature_names = feature_names\\n    feature_names, flt_indices = feature_names.handle_filter(\\n        feature_filter, feature_re)\\n    feature_importances = get_feature_importances_filtered(\\n        estimator.feature_importances_, feature_names, flt_indices, top)\\n\\n    export_graphviz_kwargs.setdefault(\"proportion\", True)\\n    tree_info = get_tree_info(\\n        estimator,\\n        feature_names=tree_feature_names,\\n        class_names=target_names,\\n        **export_graphviz_kwargs)\\n\\n    return Explanation(\\n        feature_importances=feature_importances,\\n        decision_tree=tree_info,\\n        description=DESCRIPTION_DECISION_TREE,\\n        estimator=repr(estimator),\\n        method=\\'decision tree\\',\\n    )', 'def explain_linear_regressor_weights(reg,\\n                                     vec=None,\\n                                     top=_TOP,\\n                                     target_names=None,\\n                                     targets=None,\\n                                     feature_names=None,\\n                                     coef_scale=None,\\n                                     feature_re=None,\\n                                     feature_filter=None,\\n                                     ):\\n    \"\"\"\\n    Return an explanation of a linear regressor weights.\\n\\n    See :func:`eli5.explain_weights` for description of\\n    ``top``, ``target_names``, ``targets``, ``feature_names``,\\n    ``feature_re`` and ``feature_filter`` parameters.\\n\\n    ``vec`` is a vectorizer instance used to transform\\n    raw features to the input of the regressor ``reg``; you can\\n    pass it instead of ``feature_names``.\\n\\n    ``coef_scale`` is a 1D np.ndarray with a scaling coefficient\\n    for each feature; coef[i] = coef[i] * coef_scale[i] if\\n    coef_scale[i] is not nan. Use it if you want to scale coefficients\\n    before displaying them, to take input feature sign or scale in account.\\n    \"\"\"\\n    if isinstance(reg, (SVR, NuSVR)) and reg.kernel != \\'linear\\':\\n        return explain_weights_sklearn_not_supported(reg)\\n\\n    feature_names, coef_scale = handle_hashing_vec(vec, feature_names,\\n                                                   coef_scale)\\n    feature_names, flt_indices = get_feature_names_filtered(\\n        reg, vec,\\n        feature_names=feature_names,\\n        feature_filter=feature_filter,\\n        feature_re=feature_re,\\n    )\\n    _extra_caveats = \"\\\\n\" + HASHING_CAVEATS if is_invhashing(vec) else \\'\\'\\n\\n    def _features(target_id):\\n        coef = get_coef(reg, target_id, scale=coef_scale)\\n        if flt_indices is not None:\\n            coef = coef[flt_indices]\\n        return get_top_features(feature_names, coef, top)\\n\\n    display_names = get_target_display_names(get_default_target_names(reg),\\n                                             target_names, targets)\\n    if is_multitarget_regressor(reg):\\n        return Explanation(\\n            targets=[\\n                TargetExplanation(\\n                    target=target_name,\\n                    feature_weights=_features(target_id)\\n                )\\n                for target_id, target_name in display_names\\n                ],\\n            description=DESCRIPTION_REGRESSION_MULTITARGET + _extra_caveats,\\n            estimator=repr(reg),\\n            method=\\'linear model\\',\\n            is_regression=True,\\n        )\\n    else:\\n        return Explanation(\\n            targets=[TargetExplanation(\\n                target=display_names[0][1],\\n                feature_weights=_features(0),\\n            )],\\n            description=DESCRIPTION_REGRESSION + _extra_caveats,\\n            estimator=repr(reg),\\n            method=\\'linear model\\',\\n            is_regression=True,\\n        )', 'def explain_permutation_importance(estimator,\\n                                   vec=None,\\n                                   top=_TOP,\\n                                   target_names=None,  # ignored\\n                                   targets=None,  # ignored\\n                                   feature_names=None,\\n                                   feature_re=None,\\n                                   feature_filter=None,\\n                                   ):\\n    \"\"\"\\n    Return an explanation of PermutationImportance.\\n\\n    See :func:`eli5.explain_weights` for description of\\n    ``top``, ``feature_names``, ``feature_re`` and ``feature_filter``\\n    parameters.\\n\\n    ``target_names`` and ``targets`` parameters are ignored.\\n\\n    ``vec`` is a vectorizer instance used to transform\\n    raw features to the input of the estimator (e.g. a fitted\\n    CountVectorizer instance); you can pass it instead of ``feature_names``.\\n    \"\"\"\\n    coef = estimator.feature_importances_\\n    coef_std = estimator.feature_importances_std_\\n    return get_feature_importance_explanation(estimator, vec, coef,\\n        coef_std=coef_std,\\n        feature_names=feature_names,\\n        feature_filter=feature_filter,\\n        feature_re=feature_re,\\n        top=top,\\n        description=DESCRIPTION_SCORE_DECREASE + estimator.caveats_,\\n        is_regression=isinstance(estimator.wrapped_estimator_, RegressorMixin),\\n    )', 'def _get_collisions(indices):\\n    # type: (...) -> Dict[int, List[int]]\\n    \"\"\"\\n    Return a dict ``{column_id: [possible term ids]}``\\n    with collision information.\\n    \"\"\"\\n    collisions = defaultdict(list)  # type: Dict[int, List[int]]\\n    for term_id, hash_id in enumerate(indices):\\n        collisions[hash_id].append(term_id)\\n    return dict(collisions)', 'def _get_indices_and_signs(hasher, terms):\\n    \"\"\"\\n    For each term from ``terms`` return its column index and sign,\\n    as assigned by FeatureHasher ``hasher``.\\n    \"\"\"\\n    X = _transform_terms(hasher, terms)\\n    indices = X.nonzero()[1]\\n    signs = X.sum(axis=1).A.ravel()\\n    return indices, signs', 'def handle_hashing_vec(vec, feature_names, coef_scale, with_coef_scale=True):\\n    \"\"\" Return feature_names and coef_scale (if with_coef_scale is True),\\n    calling .get_feature_names for invhashing vectorizers.\\n    \"\"\"\\n    needs_coef_scale = with_coef_scale and coef_scale is None\\n    if is_invhashing(vec):\\n        if feature_names is None:\\n            feature_names = vec.get_feature_names(always_signed=False)\\n        if needs_coef_scale:\\n            coef_scale = vec.column_signs_\\n    elif (isinstance(vec, FeatureUnion) and\\n              any(is_invhashing(v) for _, v in vec.transformer_list) and\\n              (needs_coef_scale or feature_names is None)):\\n        _feature_names, _coef_scale = _invhashing_union_feature_names_scale(vec)\\n        if feature_names is None:\\n            feature_names = _feature_names\\n        if needs_coef_scale:\\n            coef_scale = _coef_scale\\n    return (feature_names, coef_scale) if with_coef_scale else feature_names', 'def invert_hashing_and_fit(\\n        vec,  # type: Union[FeatureUnion, HashingVectorizer]\\n        docs\\n    ):\\n    # type: (...) -> Union[FeatureUnion, InvertableHashingVectorizer]\\n    \"\"\" Create an :class:`~.InvertableHashingVectorizer` from hashing\\n    vectorizer vec and fit it on docs. If vec is a FeatureUnion, do it for all\\n    hashing vectorizers in the union.\\n    Return an :class:`~.InvertableHashingVectorizer`, or a FeatureUnion,\\n    or an unchanged vectorizer.\\n    \"\"\"\\n    if isinstance(vec, HashingVectorizer):\\n        vec = InvertableHashingVectorizer(vec)\\n        vec.fit(docs)\\n    elif (isinstance(vec, FeatureUnion) and\\n              any(isinstance(v, HashingVectorizer)\\n                  for _, v in vec.transformer_list)):\\n        vec = _fit_invhashing_union(vec, docs)\\n    return vec', 'def _fit_invhashing_union(vec_union, docs):\\n    # type: (FeatureUnion, Any) -> FeatureUnion\\n    \"\"\" Fit InvertableHashingVectorizer on doc inside a FeatureUnion.\\n    \"\"\"\\n    return FeatureUnion(\\n        [(name, invert_hashing_and_fit(v, docs))\\n         for name, v in vec_union.transformer_list],\\n        transformer_weights=vec_union.transformer_weights,\\n        n_jobs=vec_union.n_jobs)', 'def fit(self, X, y=None):\\n        \"\"\" Extract possible terms from documents \"\"\"\\n        self.unhasher.fit(self._get_terms_iter(X))\\n        return self', 'def get_feature_names(self, always_signed=True):\\n        # type: (bool) -> FeatureNames\\n        \"\"\"\\n        Return feature names.\\n        This is a best-effort function which tries to reconstruct feature\\n        names based on what it has seen so far.\\n\\n        HashingVectorizer uses a signed hash function. If always_signed is True,\\n        each term in feature names is prepended with its sign. If it is False,\\n        signs are only shown in case of possible collisions of different sign.\\n\\n        You probably want always_signed=True if you\\'re checking\\n        unprocessed classifier coefficients, and always_signed=False\\n        if you\\'ve taken care of :attr:`column_signs_`.\\n        \"\"\"\\n        return self.unhasher.get_feature_names(\\n            always_signed=always_signed,\\n            always_positive=self._always_positive(),\\n        )', 'def column_signs_(self):\\n        \"\"\"\\n        Return a numpy array with expected signs of features.\\n        Values are\\n\\n        * +1 when all known terms which map to the column have positive sign;\\n        * -1 when all known terms which map to the column have negative sign;\\n        * ``nan`` when there are both positive and negative known terms\\n          for this column, or when there is no known term which maps to this\\n          column.\\n        \"\"\"\\n        if self._always_positive():\\n            return np.ones(self.n_features)\\n        self.unhasher.recalculate_attributes()\\n        return self.unhasher.column_signs_', 'def recalculate_attributes(self, force=False):\\n        # type: (bool) -> None\\n        \"\"\"\\n        Update all computed attributes. It is only needed if you need to access\\n        computed attributes after :meth:`patrial_fit` was called.\\n        \"\"\"\\n        if not self._attributes_dirty and not force:\\n            return\\n        terms = [term for term, _ in self._term_counts.most_common()]\\n        if six.PY2:\\n            terms = np.array(terms, dtype=np.object)\\n        else:\\n            terms = np.array(terms)\\n        if len(terms):\\n            indices, signs = _get_indices_and_signs(self.hasher, terms)\\n        else:\\n            indices, signs = np.array([]), np.array([])\\n        self.terms_ = terms  # type: np.ndarray\\n        self.term_columns_ = indices\\n        self.term_signs_ = signs\\n        self.collisions_ = _get_collisions(indices)\\n        self.column_signs_ = self._get_column_signs()\\n        self._attributes_dirty = False', 'def tree2text(tree_obj, indent=4):\\n    # type: (TreeInfo, int) -> str\\n    \"\"\"\\n    Return text representation of a decision tree.\\n    \"\"\"\\n    parts = []\\n\\n    def _format_node(node, depth=0):\\n        # type: (NodeInfo, int) -> None\\n        def p(*args):\\n            # type: (*str) -> None\\n            parts.append(\" \" * depth * indent)\\n            parts.extend(args)\\n\\n        if node.is_leaf:\\n            value_repr = _format_leaf_value(tree_obj, node)\\n            parts.append(\"  ---> {}\".format(value_repr))\\n        else:\\n            assert node.left is not None\\n            assert node.right is not None\\n            feat_name = node.feature_name\\n\\n            if depth > 0:\\n                parts.append(\"\\\\n\")\\n            left_samples = node.left.sample_ratio\\n            p(\"{feat_name} <= {threshold:0.3f}  ({left_samples:0.1%})\".format(\\n                left_samples=left_samples,\\n                feat_name=feat_name,\\n                threshold=node.threshold,\\n            ))\\n            _format_node(node.left, depth=depth + 1)\\n\\n            parts.append(\"\\\\n\")\\n            right_samples = node.right.sample_ratio\\n            p(\"{feat_name} > {threshold:0.3f}  ({right_samples:0.1%})\".format(\\n                right_samples=right_samples,\\n                feat_name=feat_name,\\n                threshold=node.threshold,\\n                ))\\n            _format_node(node.right, depth=depth + 1)\\n\\n    _format_node(tree_obj.tree)\\n    return \"\".join(parts)', 'def _format_array(x, fmt):\\n    # type: (Any, str) -> str\\n    \"\"\"\\n    >>> _format_array([0, 1.0], \"{:0.3f}\")\\n    \\'[0.000, 1.000]\\'\\n    \"\"\"\\n    value_repr = \", \".join(fmt.format(v) for v in x)\\n    return \"[{}]\".format(value_repr)', 'def explain_weights_df(estimator, **kwargs):\\n    # type: (...) -> pd.DataFrame\\n    \"\"\" Explain weights and export them to ``pandas.DataFrame``.\\n    All keyword arguments are passed to :func:`eli5.explain_weights`.\\n    Weights of all features are exported by default.\\n    \"\"\"\\n    kwargs = _set_defaults(kwargs)\\n    return format_as_dataframe(\\n        eli5.explain_weights(estimator, **kwargs))', 'def explain_weights_dfs(estimator, **kwargs):\\n    # type: (...) -> Dict[str, pd.DataFrame]\\n    \"\"\" Explain weights and export them to a dict with ``pandas.DataFrame``\\n    values (as :func:`eli5.formatters.as_dataframe.format_as_dataframes` does).\\n    All keyword arguments are passed to :func:`eli5.explain_weights`.\\n    Weights of all features are exported by default.\\n    \"\"\"\\n    kwargs = _set_defaults(kwargs)\\n    return format_as_dataframes(\\n        eli5.explain_weights(estimator, **kwargs))', 'def explain_prediction_df(estimator, doc, **kwargs):\\n    # type: (...) -> pd.DataFrame\\n    \"\"\" Explain prediction and export explanation to ``pandas.DataFrame``\\n    All keyword arguments are passed to :func:`eli5.explain_prediction`.\\n    Weights of all features are exported by default.\\n    \"\"\"\\n    kwargs = _set_defaults(kwargs)\\n    return format_as_dataframe(\\n        eli5.explain_prediction(estimator, doc, **kwargs))', 'def explain_prediction_dfs(estimator, doc, **kwargs):\\n    # type: (...) -> Dict[str, pd.DataFrame]\\n    \"\"\" Explain prediction and export explanation\\n    to a dict with ``pandas.DataFrame`` values\\n    (as :func:`eli5.formatters.as_dataframe.format_as_dataframes` does).\\n    All keyword arguments are passed to :func:`eli5.explain_prediction`.\\n    Weights of all features are exported by default.\\n    \"\"\"\\n    kwargs = _set_defaults(kwargs)\\n    return format_as_dataframes(\\n        eli5.explain_prediction(estimator, doc, **kwargs))', 'def format_as_dataframes(explanation):\\n    # type: (Explanation) -> Dict[str, pd.DataFrame]\\n    \"\"\" Export an explanation to a dictionary with ``pandas.DataFrame`` values\\n    and string keys that correspond to explanation attributes.\\n    Use this method if several dataframes can be exported from a single\\n    explanation (e.g. for CRF explanation with has both feature weights\\n    and transition matrix).\\n    Note that :func:`eli5.explain_weights` limits number of features\\n    by default. If you need all features, pass ``top=None`` to\\n    :func:`eli5.explain_weights`, or use\\n    :func:`explain_weights_dfs`.\\n    \"\"\"\\n    result = {}\\n    for attr in _EXPORTED_ATTRIBUTES:\\n        value = getattr(explanation, attr)\\n        if value:\\n            result[attr] = format_as_dataframe(value)\\n    return result', 'def format_as_dataframe(explanation):\\n    # type: (Explanation) -> Optional[pd.DataFrame]\\n    \"\"\" Export an explanation to a single ``pandas.DataFrame``.\\n    In case several dataframes could be exported by\\n    :func:`eli5.formatters.as_dataframe.format_as_dataframes`,\\n    a warning is raised. If no dataframe can be exported, ``None`` is returned.\\n    This function also accepts some components of the explanation as arguments:\\n    feature importances, targets, transition features.\\n    Note that :func:`eli5.explain_weights` limits number of features\\n    by default. If you need all features, pass ``top=None`` to\\n    :func:`eli5.explain_weights`, or use\\n    :func:`explain_weights_df`.\\n    \"\"\"\\n    for attr in _EXPORTED_ATTRIBUTES:\\n        value = getattr(explanation, attr)\\n        if value:\\n            other_attrs = [a for a in _EXPORTED_ATTRIBUTES\\n                           if getattr(explanation, a) and a != attr]\\n            if other_attrs:\\n                warnings.warn(\\'Exporting {} to DataFrame, but also {} could be \\'\\n                              \\'exported. Consider using eli5.format_as_dataframes.\\'\\n                              .format(attr, \\', \\'.join(other_attrs)))\\n            return format_as_dataframe(value)\\n    return None', 'def iter_shuffled(X, columns_to_shuffle=None, pre_shuffle=False,\\n                  random_state=None):\\n    \"\"\"\\n    Return an iterator of X matrices which have one or more columns shuffled.\\n    After each iteration yielded matrix is mutated inplace, so\\n    if you want to use multiple of them at the same time, make copies.\\n\\n    ``columns_to_shuffle`` is a sequence of column numbers to shuffle.\\n    By default, all columns are shuffled once, i.e. columns_to_shuffle\\n    is ``range(X.shape[1])``.\\n\\n    If ``pre_shuffle`` is True, a copy of ``X`` is shuffled once, and then\\n    result takes shuffled columns from this copy. If it is False,\\n    columns are shuffled on fly. ``pre_shuffle = True`` can be faster\\n    if there is a lot of columns, or if columns are used multiple times.\\n    \"\"\"\\n    rng = check_random_state(random_state)\\n\\n    if columns_to_shuffle is None:\\n        columns_to_shuffle = range(X.shape[1])\\n\\n    if pre_shuffle:\\n        X_shuffled = X.copy()\\n        rng.shuffle(X_shuffled)\\n\\n    X_res = X.copy()\\n    for columns in columns_to_shuffle:\\n        if pre_shuffle:\\n            X_res[:, columns] = X_shuffled[:, columns]\\n        else:\\n            rng.shuffle(X_res[:, columns])\\n        yield X_res\\n        X_res[:, columns] = X[:, columns]', 'def get_score_importances(\\n        score_func,  # type: Callable[[Any, Any], float]\\n        X,\\n        y,\\n        n_iter=5,  # type: int\\n        columns_to_shuffle=None,\\n        random_state=None\\n    ):\\n    # type: (...) -> Tuple[float, List[np.ndarray]]\\n    \"\"\"\\n    Return ``(base_score, score_decreases)`` tuple with the base score and\\n    score decreases when a feature is not available.\\n\\n    ``base_score`` is ``score_func(X, y)``; ``score_decreases``\\n    is a list of length ``n_iter`` with feature importance arrays\\n    (each array is of shape ``n_features``); feature importances are computed\\n    as score decrease when a feature is not available.\\n\\n    ``n_iter`` iterations of the basic algorithm is done, each iteration\\n    starting from a different random seed.\\n\\n    If you just want feature importances, you can take a mean of the result::\\n\\n        import numpy as np\\n        from eli5.permutation_importance import get_score_importances\\n\\n        base_score, score_decreases = get_score_importances(score_func, X, y)\\n        feature_importances = np.mean(score_decreases, axis=0)\\n\\n    \"\"\"\\n    rng = check_random_state(random_state)\\n    base_score = score_func(X, y)\\n    scores_decreases = []\\n    for i in range(n_iter):\\n        scores_shuffled = _get_scores_shufled(\\n            score_func, X, y, columns_to_shuffle=columns_to_shuffle,\\n            random_state=rng\\n        )\\n        scores_decreases.append(-scores_shuffled + base_score)\\n    return base_score, scores_decreases', 'def show_weights(estimator, **kwargs):\\n    \"\"\" Return an explanation of estimator parameters (weights)\\n    as an IPython.display.HTML object. Use this function\\n    to show classifier weights in IPython.\\n\\n    :func:`show_weights` accepts all\\n    :func:`eli5.explain_weights` arguments and all\\n    :func:`eli5.formatters.html.format_as_html`\\n    keyword arguments, so it is possible to get explanation and\\n    customize formatting in a single call.\\n\\n    Parameters\\n    ----------\\n    estimator : object\\n        Estimator instance. This argument must be positional.\\n\\n    top : int or (int, int) tuple, optional\\n        Number of features to show. When ``top`` is int, ``top`` features with\\n        a highest absolute values are shown. When it is (pos, neg) tuple,\\n        no more than ``pos`` positive features and no more than ``neg``\\n        negative features is shown. ``None`` value means no limit.\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    target_names : list[str] or {\\'old_name\\': \\'new_name\\'} dict, optional\\n        Names of targets or classes. This argument can be used to provide\\n        human-readable class/target names for estimators which don\\'t expose\\n        clss names themselves. It can be also used to rename estimator-provided\\n        classes before displaying them.\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    targets : list, optional\\n        Order of class/target names to show. This argument can be also used\\n        to show information only for a subset of classes. It should be a list\\n        of class / target names which match either names provided by\\n        an estimator or names defined in ``target_names`` parameter.\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    feature_names : list, optional\\n        A list of feature names. It allows to specify feature\\n        names when they are not provided by an estimator object.\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    feature_re : str, optional\\n        Only feature names which match ``feature_re`` regex are shown\\n        (more precisely, ``re.search(feature_re, x)`` is checked).\\n\\n    feature_filter : Callable[[str], bool], optional\\n        Only feature names for which ``feature_filter`` function returns True\\n        are shown.\\n\\n    show : List[str], optional\\n        List of sections to show. Allowed values:\\n\\n        * \\'targets\\' - per-target feature weights;\\n        * \\'transition_features\\' - transition features of a CRF model;\\n        * \\'feature_importances\\' - feature importances of a decision tree or\\n          an ensemble-based estimator;\\n        * \\'decision_tree\\' - decision tree in a graphical form;\\n        * \\'method\\' - a string with explanation method;\\n        * \\'description\\' - description of explanation method and its caveats.\\n\\n        ``eli5.formatters.fields`` provides constants that cover common cases:\\n        ``INFO`` (method and description), ``WEIGHTS`` (all the rest),\\n        and ``ALL`` (all).\\n\\n    horizontal_layout : bool\\n        When True, feature weight tables are printed horizontally\\n        (left to right); when False, feature weight tables are printed\\n        vertically (top to down). Default is True.\\n\\n    highlight_spaces : bool or None, optional\\n        Whether to highlight spaces in feature names. This is useful if\\n        you work with text and have ngram features which may include spaces\\n        at left or right. Default is None, meaning that the value used\\n        is set automatically based on vectorizer and feature values.\\n\\n    include_styles : bool\\n        Most styles are inline, but some are included separately in <style> tag;\\n        you can omit them by passing ``include_styles=False``. Default is True.\\n\\n    **kwargs: dict\\n        Keyword arguments. All keyword arguments are passed to\\n        concrete explain_weights... implementations.\\n\\n    Returns\\n    -------\\n    IPython.display.HTML\\n        The result is printed in IPython notebook as an HTML widget.\\n        If you need to display several explanations as an output of a single\\n        cell, or if you want to display it from a function then use\\n        IPython.display.display::\\n\\n            from IPython.display import display\\n            display(eli5.show_weights(clf1))\\n            display(eli5.show_weights(clf2))\\n\\n    \"\"\"\\n    format_kwargs, explain_kwargs = _split_kwargs(kwargs)\\n    expl = explain_weights(estimator, **explain_kwargs)\\n    html = format_as_html(expl, **format_kwargs)\\n    return HTML(html)', 'def show_prediction(estimator, doc, **kwargs):\\n    \"\"\" Return an explanation of estimator prediction\\n    as an IPython.display.HTML object. Use this function\\n    to show information about classifier prediction in IPython.\\n\\n    :func:`show_prediction` accepts all\\n    :func:`eli5.explain_prediction` arguments and all\\n    :func:`eli5.formatters.html.format_as_html`\\n    keyword arguments, so it is possible to get explanation and\\n    customize formatting in a single call.\\n\\n    Parameters\\n    ----------\\n    estimator : object\\n        Estimator instance. This argument must be positional.\\n\\n    doc : object\\n        Example to run estimator on. Estimator makes a prediction for this\\n        example, and :func:`show_prediction` tries to show information\\n        about this prediction. Pass a single element, not a one-element array:\\n        if you fitted your estimator on ``X``, that would be ``X[i]`` for\\n        most containers, and ``X.iloc[i]`` for ``pandas.DataFrame``.\\n\\n    top : int or (int, int) tuple, optional\\n        Number of features to show. When ``top`` is int, ``top`` features with\\n        a highest absolute values are shown. When it is (pos, neg) tuple,\\n        no more than ``pos`` positive features and no more than ``neg``\\n        negative features is shown. ``None`` value means no limit (default).\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    top_targets : int, optional\\n        Number of targets to show. When ``top_targets`` is provided,\\n        only specified number of targets with highest scores are shown.\\n        Negative value means targets with lowest scores are shown.\\n        Must not be given with ``targets`` argument.\\n        ``None`` value means no limit: all targets are shown (default).\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    target_names : list[str] or {\\'old_name\\': \\'new_name\\'} dict, optional\\n        Names of targets or classes. This argument can be used to provide\\n        human-readable class/target names for estimators which don\\'t expose\\n        clss names themselves. It can be also used to rename estimator-provided\\n        classes before displaying them.\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    targets : list, optional\\n        Order of class/target names to show. This argument can be also used\\n        to show information only for a subset of classes. It should be a list\\n        of class / target names which match either names provided by\\n        an estimator or names defined in ``target_names`` parameter.\\n\\n        In case of binary classification you can use this argument to\\n        set the class which probability or score should be displayed, with\\n        an appropriate explanation. By default a result for predicted class\\n        is shown. For example, you can use ``targets=[True]`` to always show\\n        result for a positive class, even if the predicted label is False.\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    feature_names : list, optional\\n        A list of feature names. It allows to specify feature\\n        names when they are not provided by an estimator object.\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    feature_re : str, optional\\n        Only feature names which match ``feature_re`` regex are shown\\n        (more precisely, ``re.search(feature_re, x)`` is checked).\\n\\n    feature_filter : Callable[[str, float], bool], optional\\n        Only feature names for which ``feature_filter`` function returns True\\n        are shown. It must accept feature name and feature value.\\n        Missing features always have a NaN value.\\n\\n    show : List[str], optional\\n        List of sections to show. Allowed values:\\n\\n        * \\'targets\\' - per-target feature weights;\\n        * \\'transition_features\\' - transition features of a CRF model;\\n        * \\'feature_importances\\' - feature importances of a decision tree or\\n          an ensemble-based estimator;\\n        * \\'decision_tree\\' - decision tree in a graphical form;\\n        * \\'method\\' - a string with explanation method;\\n        * \\'description\\' - description of explanation method and its caveats.\\n\\n        ``eli5.formatters.fields`` provides constants that cover common cases:\\n        ``INFO`` (method and description), ``WEIGHTS`` (all the rest),\\n        and ``ALL`` (all).\\n\\n    horizontal_layout : bool\\n        When True, feature weight tables are printed horizontally\\n        (left to right); when False, feature weight tables are printed\\n        vertically (top to down). Default is True.\\n\\n    highlight_spaces : bool or None, optional\\n        Whether to highlight spaces in feature names. This is useful if\\n        you work with text and have ngram features which may include spaces\\n        at left or right. Default is None, meaning that the value used\\n        is set automatically based on vectorizer and feature values.\\n\\n    include_styles : bool\\n        Most styles are inline, but some are included separately in <style> tag;\\n        you can omit them by passing ``include_styles=False``. Default is True.\\n\\n    force_weights : bool\\n        When True, a table with feature weights is displayed even if all\\n        features are already highlighted in text. Default is False.\\n\\n    preserve_density: bool or None\\n        This argument currently only makes sense when used with text data\\n        and vectorizers from scikit-learn.\\n\\n        If preserve_density is True, then color for longer fragments will be\\n        less intensive than for shorter fragments, so that \"sum\" of intensities\\n        will correspond to feature weight.\\n\\n        If preserve_density is None, then it\\'s value is chosen depending on\\n        analyzer kind: it is preserved for \"char\" and \"char_wb\" analyzers,\\n        and not preserved for \"word\" analyzers.\\n\\n        Default is None.\\n\\n    show_feature_values : bool\\n        When True, feature values are shown along with feature contributions.\\n        Default is False.\\n\\n    **kwargs: dict\\n        Keyword arguments. All keyword arguments are passed to\\n        concrete explain_prediction... implementations.\\n\\n    Returns\\n    -------\\n    IPython.display.HTML\\n        The result is printed in IPython notebook as an HTML widget.\\n        If you need to display several explanations as an output of a single\\n        cell, or if you want to display it from a function then use\\n        IPython.display.display::\\n\\n            from IPython.display import display\\n            display(eli5.show_weights(clf1))\\n            display(eli5.show_weights(clf2))\\n    \"\"\"\\n    format_kwargs, explain_kwargs = _split_kwargs(kwargs)\\n    expl = explain_prediction(estimator, doc, **explain_kwargs)\\n    html = format_as_html(expl, **format_kwargs)\\n    return HTML(html)', 'def get_tree_info(decision_tree,\\n                  feature_names=None,\\n                  **export_graphviz_kwargs):\\n    # type: (...) -> TreeInfo\\n    \"\"\"\\n    Convert DecisionTreeClassifier or DecisionTreeRegressor\\n    to an inspectable object.\\n    \"\"\"\\n    return TreeInfo(\\n        criterion=decision_tree.criterion,\\n        tree=_get_root_node_info(decision_tree, feature_names),\\n        graphviz=tree2dot(decision_tree,\\n                          feature_names=feature_names,\\n                          **export_graphviz_kwargs),\\n        is_classification=isinstance(decision_tree, ClassifierMixin),\\n    )', 'def generate_samples(text,                # type: TokenizedText\\n                     n_samples=500,       # type: int\\n                     bow=True,            # type: bool\\n                     random_state=None,\\n                     replacement=\\'\\',      # type: str\\n                     min_replace=1,       # type: Union[int, float]\\n                     max_replace=1.0,     # type: Union[int, float]\\n                     group_size=1,        # type: int\\n                     ):\\n    # type: (...) -> Tuple[List[str], np.ndarray, np.ndarray]\\n    \"\"\"\\n    Return ``n_samples`` changed versions of text (with some words removed),\\n    along with distances between the original text and a generated\\n    examples. If ``bow=False``, all tokens are considered unique\\n    (i.e. token position matters).\\n    \"\"\"\\n    kwargs = dict(\\n        n_samples=n_samples,\\n        replacement=replacement,\\n        random_state=random_state,\\n        min_replace=min_replace,\\n        max_replace=max_replace,\\n    )\\n    if bow:\\n        num_tokens = len(text.vocab)\\n        res = text.replace_random_tokens_bow(**kwargs)\\n    else:\\n        num_tokens = len(text.tokens)\\n        res = text.replace_random_tokens(group_size=group_size, **kwargs)\\n\\n    texts, num_removed_vec, masks = zip(*res)\\n    similarity = cosine_similarity_vec(num_tokens, num_removed_vec)\\n    return texts, similarity, vstack(masks)', 'def cosine_similarity_vec(num_tokens, num_removed_vec):\\n    \"\"\"\\n    Return cosine similarity between a binary vector with all ones\\n    of length ``num_tokens`` and vectors of the same length with\\n    ``num_removed_vec`` elements set to zero.\\n    \"\"\"\\n    remaining = -np.array(num_removed_vec) + num_tokens\\n    return remaining / (np.sqrt(num_tokens + 1e-6) * np.sqrt(remaining + 1e-6))', 'def replace_random_tokens(self,\\n                              n_samples,  # type: int\\n                              replacement=\\'\\',  # type: str\\n                              random_state=None,\\n                              min_replace=1,  # type: Union[int, float]\\n                              max_replace=1.0,  # type: Union[int, float]\\n                              group_size=1  # type: int\\n                              ):\\n        # type: (...) -> List[Tuple[str, int, np.ndarray]]\\n        \"\"\" \\n        Return a list of ``(text, replaced_count, mask)``\\n        tuples with n_samples versions of text with some words replaced.\\n        By default words are replaced with \\'\\', i.e. removed.\\n        \"\"\"\\n        n_tokens = len(self.tokens)\\n        indices = np.arange(n_tokens)\\n        if not n_tokens:\\n            nomask = np.array([], dtype=int)\\n            return [(\\'\\', 0, nomask)] * n_samples\\n\\n        min_replace, max_replace = self._get_min_max(min_replace, max_replace,\\n                                                     n_tokens)\\n        rng = check_random_state(random_state)\\n        replace_sizes = rng.randint(low=min_replace, high=max_replace + 1,\\n                                    size=n_samples)\\n        res = []\\n        for num_to_replace in replace_sizes:\\n            idx_to_replace = rng.choice(indices, num_to_replace, replace=False)\\n            idx_to_replace = np.array([idx_to_replace] + [\\n                idx_to_replace + shift for shift in range(1, group_size)\\n            ]).ravel()\\n            padded_size = n_tokens + group_size - 1\\n            mask = indices_to_bool_mask(idx_to_replace, padded_size)[:n_tokens]\\n            s = self.split.masked(mask, replacement)\\n            res.append((s.text, num_to_replace, mask))\\n        return res', 'def replace_random_tokens_bow(self,\\n                                  n_samples,  # type: int\\n                                  replacement=\\'\\',  # type: str\\n                                  random_state=None,\\n                                  min_replace=1,  # type: Union[int, float]\\n                                  max_replace=1.0, # type: Union[int, float]\\n                                  ):\\n        # type: (...) -> List[Tuple[str, int, np.ndarray]]\\n        \"\"\"\\n        Return a list of ``(text, replaced_words_count, mask)`` tuples with\\n        n_samples versions of text with some words replaced.\\n        If a word is replaced, all duplicate words are also replaced\\n        from the text. By default words are replaced with \\'\\', i.e. removed.\\n        \"\"\"\\n        if not self.vocab:\\n            nomask = np.array([], dtype=int)\\n            return [(\\'\\', 0, nomask)] * n_samples\\n\\n        min_replace, max_replace = self._get_min_max(min_replace, max_replace,\\n                                                     len(self.vocab))\\n        rng = check_random_state(random_state)\\n        replace_sizes = rng.randint(low=min_replace, high=max_replace + 1,\\n                                    size=n_samples)\\n        res = []\\n        for num_to_replace in replace_sizes:\\n            tokens_to_replace = set(rng.choice(self.vocab, num_to_replace,\\n                                               replace=False))\\n            idx_to_replace = [idx for idx, token in enumerate(self.tokens)\\n                              if token in tokens_to_replace]\\n            mask = indices_to_bool_mask(idx_to_replace, len(self.tokens))\\n            s = self.split.masked(idx_to_replace, replacement)\\n            res.append((s.text, num_to_replace, mask))\\n        return res', 'def fit(self,\\n            doc,             # type: str\\n            predict_proba,   # type: Callable[[Any], Any]\\n            ):\\n        # type: (...) -> TextExplainer\\n        \"\"\"\\n        Explain ``predict_proba`` probabilistic classification function\\n        for the ``doc`` example. This method fits a local classification\\n        pipeline following LIME approach.\\n\\n        To get the explanation use :meth:`show_prediction`,\\n        :meth:`show_weights`, :meth:`explain_prediction` or\\n        :meth:`explain_weights`.\\n\\n        Parameters\\n        ----------\\n        doc : str\\n            Text to explain\\n        predict_proba : callable\\n            Black-box classification pipeline. ``predict_proba``\\n            should be a function which takes a list of strings (documents)\\n            and return a matrix of shape ``(n_samples, n_classes)`` with\\n            probability values - a row per document and a column per output\\n            label.\\n        \"\"\"\\n        self.doc_ = doc\\n\\n        if self.position_dependent:\\n            samples, sims, mask, text = self.sampler.sample_near_with_mask(\\n                doc=doc,\\n                n_samples=self.n_samples\\n            )\\n            self.vec_ = SingleDocumentVectorizer(\\n                token_pattern=self.token_pattern\\n            ).fit([doc])\\n            X = ~mask\\n        else:\\n            self.vec_ = clone(self.vec).fit([doc])\\n            samples, sims = self.sampler.sample_near(\\n                doc=doc,\\n                n_samples=self.n_samples\\n            )\\n            X = self.vec_.transform(samples)\\n\\n        if self.rbf_sigma is not None:\\n            sims = rbf(1-sims, sigma=self.rbf_sigma)\\n\\n        self.samples_ = samples\\n        self.similarity_ = sims\\n        self.X_ = X\\n        self.y_proba_ = predict_proba(samples)\\n        self.clf_ = clone(self.clf)\\n\\n        self.metrics_ = _train_local_classifier(\\n            estimator=self.clf_,\\n            samples=X,\\n            similarity=sims,\\n            y_proba=self.y_proba_,\\n            expand_factor=self.expand_factor,\\n            random_state=self.rng_\\n        )\\n        return self', 'def show_prediction(self, **kwargs):\\n        \"\"\"\\n        Call :func:`eli5.show_prediction` for the locally-fit\\n        classification pipeline. Keyword arguments are passed\\n        to :func:`eli5.show_prediction`.\\n\\n        :func:`fit` must be called before using this method.\\n        \"\"\"\\n        self._fix_target_names(kwargs)\\n        return eli5.show_prediction(self.clf_, self.doc_, vec=self.vec_,\\n                                    **kwargs)', 'def show_weights(self, **kwargs):\\n        \"\"\"\\n        Call :func:`eli5.show_weights` for the locally-fit\\n        classification pipeline. Keyword arguments are passed\\n        to :func:`eli5.show_weights`.\\n\\n        :func:`fit` must be called before using this method.\\n        \"\"\"\\n        self._fix_target_names(kwargs)\\n        return eli5.show_weights(self.clf_, vec=self.vec_, **kwargs)', 'def explain_weights(self, **kwargs):\\n        \"\"\"\\n        Call :func:`eli5.show_weights` for the locally-fit\\n        classification pipeline. Keyword arguments are passed\\n        to :func:`eli5.show_weights`.\\n\\n        :func:`fit` must be called before using this method.\\n        \"\"\"\\n        self._fix_target_names(kwargs)\\n        return eli5.explain_weights(self.clf_, vec=self.vec_, **kwargs)', 'def fit_proba(clf, X, y_proba, expand_factor=10, sample_weight=None,\\n              shuffle=True, random_state=None,\\n              **fit_params):\\n    \"\"\"\\n    Fit classifier ``clf`` to return probabilities close to ``y_proba``.\\n\\n    scikit-learn can\\'t optimize cross-entropy directly if target\\n    probability values are not indicator vectors. As a workaround this function\\n    expands the dataset according to target probabilities.\\n    Use expand_factor=None to turn it off\\n    (e.g. if probability scores are 0/1 in a first place).\\n    \"\"\"\\n    X, y, sample_weight = expanded_X_y_sample_weights(X, y_proba,\\n        expand_factor=expand_factor,\\n        sample_weight=sample_weight,\\n        shuffle=shuffle,\\n        random_state=random_state,\\n    )\\n    fit_params = with_sample_weight(clf, sample_weight, fit_params)\\n    clf.fit(X, y, **fit_params)\\n    return clf', 'def with_sample_weight(clf, sample_weight, fit_params):\\n    \"\"\"\\n    Return fit_params with added \"sample_weight\" argument.\\n    Unlike `fit_params[\\'sample_weight\\'] = sample_weight` it\\n    handles a case where ``clf`` is a pipeline.\\n    \"\"\"\\n    param_name = _get_classifier_prefix(clf) + \"sample_weight\"\\n    params = {param_name: sample_weight}\\n    params.update(fit_params)\\n    return params', 'def fix_multiclass_predict_proba(y_proba,          # type: np.ndarray\\n                                 seen_classes,\\n                                 complete_classes\\n                                 ):\\n    # type: (...) -> np.ndarray\\n    \"\"\"\\n    Add missing columns to predict_proba result.\\n\\n    When a multiclass classifier is fit on a dataset which only contains\\n    a subset of possible classes its predict_proba result only has columns\\n    corresponding to seen classes. This function adds missing columns.\\n    \"\"\"\\n    assert set(complete_classes) >= set(seen_classes)\\n    y_proba_fixed = np.zeros(\\n        shape=(y_proba.shape[0], len(complete_classes)),\\n        dtype=y_proba.dtype,\\n    )\\n    class_mapping = np.searchsorted(complete_classes, seen_classes)\\n    y_proba_fixed[:, class_mapping] = y_proba\\n    return y_proba_fixed', 'def expanded_X_y_sample_weights(X, y_proba, expand_factor=10,\\n                                sample_weight=None, shuffle=True,\\n                                random_state=None):\\n    \"\"\"\\n    scikit-learn can\\'t optimize cross-entropy directly if target\\n    probability values are not indicator vectors.\\n    As a workaround this function expands the dataset according to\\n    target probabilities. ``expand_factor=None`` means no dataset\\n    expansion.\\n    \"\"\"\\n    rng = check_random_state(random_state)\\n    if expand_factor:\\n        if sample_weight is not None:\\n            X, y, sample_weight = zip(*expand_dataset(X, y_proba,\\n                                                      factor=expand_factor,\\n                                                      random_state=rng,\\n                                                      extra_arrays=[\\n                                                          sample_weight\\n                                                      ]))\\n        else:\\n            X, y = zip(*expand_dataset(X, y_proba,\\n                                       factor=expand_factor,\\n                                       random_state=rng))\\n    else:\\n        y = y_proba.argmax(axis=1)\\n\\n    if isinstance(X, (list, tuple)) and len(X) and issparse(X[0]):\\n        X = vstack(X)\\n\\n    if shuffle:\\n        if sample_weight is not None:\\n            X, y, sample_weight = _shuffle(X, y, sample_weight,\\n                                           random_state=rng)\\n        else:\\n            X, y = _shuffle(X, y, random_state=rng)\\n    return X, y, sample_weight', 'def expand_dataset(X, y_proba, factor=10, random_state=None, extra_arrays=None):\\n    \"\"\"\\n    Convert a dataset with float multiclass probabilities to a dataset\\n    with indicator probabilities by duplicating X rows and sampling\\n    true labels.\\n    \"\"\"\\n    rng = check_random_state(random_state)\\n    extra_arrays = extra_arrays or []\\n    n_classes = y_proba.shape[1]\\n    classes = np.arange(n_classes, dtype=int)\\n    for el in zip(X, y_proba, *extra_arrays):\\n        x, probs = el[0:2]\\n        rest = el[2:]\\n        for label in rng.choice(classes, size=factor, p=probs):\\n            yield (x, label) + rest', 'def transform_feature_names(transformer, in_names=None):\\n    \"\"\"Get feature names for transformer output as a function of input names.\\n\\n    Used by :func:`explain_weights` when applied to a scikit-learn Pipeline,\\n    this ``singledispatch`` should be registered with custom name\\n    transformations for each class of transformer.\\n    \\n    If there is no ``singledispatch`` handler registered for a transformer \\n    class, ``transformer.get_feature_names()`` method is called; if there is\\n    no such method then feature names are not supported and \\n    this function raises an exception.\\n\\n    Parameters\\n    ----------\\n    transformer : scikit-learn-compatible transformer\\n    in_names : list of str, optional\\n        Names for features input to transformer.transform().\\n        If not provided, the implementation may generate default feature names\\n        if the number of input features is known.\\n\\n    Returns\\n    -------\\n    feature_names : list of str\\n    \"\"\"\\n    if hasattr(transformer, \\'get_feature_names\\'):\\n        return transformer.get_feature_names()\\n    raise NotImplementedError(\\'transform_feature_names not available for \\'\\n                              \\'{}\\'.format(transformer))', 'def get_weighted_spans(doc, vec, feature_weights):\\n    # type: (Any, Any, FeatureWeights) -> Optional[WeightedSpans]\\n    \"\"\" If possible, return a dict with preprocessed document and a list\\n    of spans with weights, corresponding to features in the document.\\n    \"\"\"\\n    if isinstance(vec, FeatureUnion):\\n        return _get_weighted_spans_from_union(doc, vec, feature_weights)\\n    else:\\n        result = _get_doc_weighted_spans(doc, vec, feature_weights)\\n        if result is not None:\\n            found_features, doc_weighted_spans = result\\n            return WeightedSpans(\\n                [doc_weighted_spans],\\n                other=_get_other(feature_weights, [(\\'\\', found_features)]),\\n            )\\n    return None', 'def add_weighted_spans(doc, vec, vectorized, target_expl):\\n    # type: (Any, Any, bool, TargetExplanation) -> None\\n    \"\"\"\\n    Compute and set ``target_expl.weighted_spans`` attribute, when possible.\\n    \"\"\"\\n    if vec is None or vectorized:\\n        return\\n\\n    weighted_spans = get_weighted_spans(doc, vec, target_expl.feature_weights)\\n    if weighted_spans:\\n        target_expl.weighted_spans = weighted_spans', 'def _get_feature_weights_dict(feature_weights,  # type: FeatureWeights\\n                              feature_fn        # type: Optional[Callable[[str], str]]\\n                              ):\\n    # type: (...) -> Dict[str, Tuple[float, Tuple[str, int]]]\\n    \"\"\" Return {feat_name: (weight, (group, idx))} mapping. \"\"\"\\n    return {\\n        # (group, idx) is an unique feature identifier, e.g. (\\'pos\\', 2)\\n        feat_name: (fw.weight, (group, idx))\\n        for group in [\\'pos\\', \\'neg\\']\\n        for idx, fw in enumerate(getattr(feature_weights, group))\\n        for feat_name in _get_features(fw.feature, feature_fn)\\n    }', 'def fit(self, X, y, groups=None, **fit_params):\\n        # type: (...) -> PermutationImportance\\n        \"\"\"Compute ``feature_importances_`` attribute and optionally\\n        fit the base estimator.\\n\\n        Parameters\\n        ----------\\n        X : array-like of shape (n_samples, n_features)\\n            The training input samples.\\n\\n        y : array-like, shape (n_samples,)\\n            The target values (integers that correspond to classes in\\n            classification, real numbers in regression).\\n\\n        groups : array-like, with shape (n_samples,), optional\\n            Group labels for the samples used while splitting the dataset into\\n            train/test set.\\n\\n        **fit_params : Other estimator specific parameters\\n\\n        Returns\\n        -------\\n        self : object\\n            Returns self.\\n        \"\"\"\\n        self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)\\n\\n        if pandas_available and isinstance(X, pd.DataFrame):\\n            self.scorer_ = self._wrap_scorer(self.scorer_, X.columns)\\n\\n        if self.cv != \"prefit\" and self.refit:\\n            self.estimator_ = clone(self.estimator)\\n            self.estimator_.fit(X, y, **fit_params)\\n\\n        X = check_array(X)\\n\\n        if self.cv not in (None, \"prefit\"):\\n            si = self._cv_scores_importances(X, y, groups=groups, **fit_params)\\n        else:\\n            si = self._non_cv_scores_importances(X, y)\\n        scores, results = si\\n        self.scores_ = np.array(scores)\\n        self.results_ = results\\n        self.feature_importances_ = np.mean(results, axis=0)\\n        self.feature_importances_std_ = np.std(results, axis=0)\\n        return self', 'def explain_prediction(estimator, doc, **kwargs):\\n    \"\"\"\\n    Return an explanation of an estimator prediction.\\n\\n    :func:`explain_prediction` is not doing any work itself, it dispatches\\n    to a concrete implementation based on estimator type.\\n\\n    Parameters\\n    ----------\\n    estimator : object\\n        Estimator instance. This argument must be positional.\\n\\n    doc : object\\n        Example to run estimator on. Estimator makes a prediction for this\\n        example, and :func:`explain_prediction` tries to show information\\n        about this prediction. Pass a single element, not a one-element array:\\n        if you fitted your estimator on ``X``, that would be ``X[i]`` for\\n        most containers, and ``X.iloc[i]`` for ``pandas.DataFrame``.\\n\\n    top : int or (int, int) tuple, optional\\n        Number of features to show. When ``top`` is int, ``top`` features with\\n        a highest absolute values are shown. When it is (pos, neg) tuple,\\n        no more than ``pos`` positive features and no more than ``neg``\\n        negative features is shown. ``None`` value means no limit (default).\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    top_targets : int, optional\\n        Number of targets to show. When ``top_targets`` is provided,\\n        only specified number of targets with highest scores are shown.\\n        Negative value means targets with lowest scores are shown.\\n        Must not be given with ``targets`` argument.\\n        ``None`` value means no limit: all targets are shown (default).\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    target_names : list[str] or {\\'old_name\\': \\'new_name\\'} dict, optional\\n        Names of targets or classes. This argument can be used to provide\\n        human-readable class/target names for estimators which don\\'t expose\\n        clss names themselves. It can be also used to rename estimator-provided\\n        classes before displaying them.\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    targets : list, optional\\n        Order of class/target names to show. This argument can be also used\\n        to show information only for a subset of classes. It should be a list\\n        of class / target names which match either names provided by\\n        an estimator or names defined in ``target_names`` parameter.\\n        Must not be given with ``top_targets`` argument.\\n\\n        In case of binary classification you can use this argument to\\n        set the class which probability or score should be displayed, with\\n        an appropriate explanation. By default a result for predicted class\\n        is shown. For example, you can use ``targets=[True]`` to always show\\n        result for a positive class, even if the predicted label is False.\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    feature_names : list, optional\\n        A list of feature names. It allows to specify feature\\n        names when they are not provided by an estimator object.\\n\\n        This argument may be supported or not, depending on estimator type.\\n\\n    feature_re : str, optional\\n        Only feature names which match ``feature_re`` regex are returned\\n        (more precisely, ``re.search(feature_re, x)`` is checked).\\n\\n    feature_filter : Callable[[str, float], bool], optional\\n        Only feature names for which ``feature_filter`` function returns True\\n        are returned. It must accept feature name and feature value.\\n        Missing features always have a NaN value.\\n\\n    **kwargs: dict\\n        Keyword arguments. All keyword arguments are passed to\\n        concrete explain_prediction... implementations.\\n\\n    Returns\\n    -------\\n    Explanation\\n        :class:`~.Explanation` result. Use one of the formatting functions from\\n        :mod:`eli5.formatters` to print it in a human-readable form.\\n\\n        Explanation instances have repr which works well with\\n        IPython notebook, but it can be a better idea to use\\n        :func:`eli5.show_prediction` instead of :func:`eli5.explain_prediction`\\n        if you work with IPython: :func:`eli5.show_prediction` allows to\\n        customize formatting without a need to import :mod:`eli5.formatters`\\n        functions.\\n    \"\"\"\\n    return Explanation(\\n        estimator=repr(estimator),\\n        error=\"estimator %r is not supported\" % estimator,\\n    )', 'def directions(client, origin, destination,\\n               mode=None, waypoints=None, alternatives=False, avoid=None,\\n               language=None, units=None, region=None, departure_time=None,\\n               arrival_time=None, optimize_waypoints=False, transit_mode=None,\\n               transit_routing_preference=None, traffic_model=None):\\n    \"\"\"Get directions between an origin point and a destination point.\\n\\n    :param origin: The address or latitude/longitude value from which you wish\\n        to calculate directions.\\n    :type origin: string, dict, list, or tuple\\n\\n    :param destination: The address or latitude/longitude value from which\\n        you wish to calculate directions. You can use a place_id as destination\\n        by putting \\'place_id:\\' as a preffix in the passing parameter.\\n    :type destination: string, dict, list, or tuple\\n\\n    :param mode: Specifies the mode of transport to use when calculating\\n        directions. One of \"driving\", \"walking\", \"bicycling\" or \"transit\"\\n    :type mode: string\\n\\n    :param waypoints: Specifies an array of waypoints. Waypoints alter a\\n        route by routing it through the specified location(s).\\n    :type waypoints: a single location, or a list of locations, where a\\n        location is a string, dict, list, or tuple\\n\\n    :param alternatives: If True, more than one route may be returned in the\\n        response.\\n    :type alternatives: bool\\n\\n    :param avoid: Indicates that the calculated route(s) should avoid the\\n        indicated features.\\n    :type avoid: list or string\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :param units: Specifies the unit system to use when displaying results.\\n        \"metric\" or \"imperial\"\\n    :type units: string\\n\\n    :param region: The region code, specified as a ccTLD (\"top-level domain\"\\n        two-character value.\\n    :type region: string\\n\\n    :param departure_time: Specifies the desired time of departure.\\n    :type departure_time: int or datetime.datetime\\n\\n    :param arrival_time: Specifies the desired time of arrival for transit\\n        directions. Note: you can\\'t specify both departure_time and\\n        arrival_time.\\n    :type arrival_time: int or datetime.datetime\\n\\n    :param optimize_waypoints: Optimize the provided route by rearranging the\\n        waypoints in a more efficient order.\\n    :type optimize_waypoints: bool\\n\\n    :param transit_mode: Specifies one or more preferred modes of transit.\\n        This parameter may only be specified for requests where the mode is\\n        transit. Valid values are \"bus\", \"subway\", \"train\", \"tram\", \"rail\".\\n        \"rail\" is equivalent to [\"train\", \"tram\", \"subway\"].\\n    :type transit_mode: string or list of strings\\n\\n    :param transit_routing_preference: Specifies preferences for transit\\n        requests. Valid values are \"less_walking\" or \"fewer_transfers\"\\n    :type transit_routing_preference: string\\n\\n    :param traffic_model: Specifies the predictive travel time model to use.\\n        Valid values are \"best_guess\" or \"optimistic\" or \"pessimistic\".\\n        The traffic_model parameter may only be specified for requests where\\n        the travel mode is driving, and where the request includes a\\n        departure_time.\\n    :type units: string\\n\\n    :rtype: list of routes\\n    \"\"\"\\n\\n    params = {\\n        \"origin\": convert.latlng(origin),\\n        \"destination\": convert.latlng(destination)\\n    }\\n\\n    if mode:\\n        # NOTE(broady): the mode parameter is not validated by the Maps API\\n        # server. Check here to prevent silent failures.\\n        if mode not in [\"driving\", \"walking\", \"bicycling\", \"transit\"]:\\n            raise ValueError(\"Invalid travel mode.\")\\n        params[\"mode\"] = mode\\n\\n    if waypoints:\\n        waypoints = convert.location_list(waypoints)\\n        if optimize_waypoints:\\n            waypoints = \"optimize:true|\" + waypoints\\n        params[\"waypoints\"] = waypoints\\n\\n    if alternatives:\\n        params[\"alternatives\"] = \"true\"\\n\\n    if avoid:\\n        params[\"avoid\"] = convert.join_list(\"|\", avoid)\\n\\n    if language:\\n        params[\"language\"] = language\\n\\n    if units:\\n        params[\"units\"] = units\\n\\n    if region:\\n        params[\"region\"] = region\\n\\n    if departure_time:\\n        params[\"departure_time\"] = convert.time(departure_time)\\n\\n    if arrival_time:\\n        params[\"arrival_time\"] = convert.time(arrival_time)\\n\\n    if departure_time and arrival_time:\\n        raise ValueError(\"Should not specify both departure_time and\"\\n                         \"arrival_time.\")\\n\\n    if transit_mode:\\n        params[\"transit_mode\"] = convert.join_list(\"|\", transit_mode)\\n\\n    if transit_routing_preference:\\n        params[\"transit_routing_preference\"] = transit_routing_preference\\n\\n    if traffic_model:\\n        params[\"traffic_model\"] = traffic_model\\n\\n    return client._request(\"/maps/api/directions/json\", params).get(\"routes\", [])', 'def geocode(client, address=None, components=None, bounds=None, region=None,\\n            language=None):\\n    \"\"\"\\n    Geocoding is the process of converting addresses\\n    (like ``\"1600 Amphitheatre Parkway, Mountain View, CA\"``) into geographic\\n    coordinates (like latitude 37.423021 and longitude -122.083739), which you\\n    can use to place markers or position the map.\\n\\n    :param address: The address to geocode.\\n    :type address: string\\n\\n    :param components: A component filter for which you wish to obtain a\\n        geocode, for example: ``{\\'administrative_area\\': \\'TX\\',\\'country\\': \\'US\\'}``\\n    :type components: dict\\n\\n    :param bounds: The bounding box of the viewport within which to bias geocode\\n        results more prominently.\\n    :type bounds: string or dict with northeast and southwest keys.\\n\\n    :param region: The region code, specified as a ccTLD (\"top-level domain\")\\n        two-character value.\\n    :type region: string\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :rtype: list of geocoding results.\\n    \"\"\"\\n\\n    params = {}\\n\\n    if address:\\n        params[\"address\"] = address\\n\\n    if components:\\n        params[\"components\"] = convert.components(components)\\n\\n    if bounds:\\n        params[\"bounds\"] = convert.bounds(bounds)\\n\\n    if region:\\n        params[\"region\"] = region\\n\\n    if language:\\n        params[\"language\"] = language\\n\\n    return client._request(\"/maps/api/geocode/json\", params).get(\"results\", [])', 'def reverse_geocode(client, latlng, result_type=None, location_type=None,\\n                    language=None):\\n    \"\"\"\\n    Reverse geocoding is the process of converting geographic coordinates into a\\n    human-readable address.\\n\\n    :param latlng: The latitude/longitude value or place_id for which you wish\\n        to obtain the closest, human-readable address.\\n    :type latlng: string, dict, list, or tuple\\n\\n    :param result_type: One or more address types to restrict results to.\\n    :type result_type: string or list of strings\\n\\n    :param location_type: One or more location types to restrict results to.\\n    :type location_type: list of strings\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :rtype: list of reverse geocoding results.\\n    \"\"\"\\n\\n    # Check if latlng param is a place_id string.\\n    #  place_id strings do not contain commas; latlng strings do.\\n    if convert.is_string(latlng) and \\',\\' not in latlng:\\n        params = {\"place_id\": latlng}\\n    else:\\n        params = {\"latlng\": convert.latlng(latlng)}\\n\\n    if result_type:\\n        params[\"result_type\"] = convert.join_list(\"|\", result_type)\\n\\n    if location_type:\\n        params[\"location_type\"] = convert.join_list(\"|\", location_type)\\n\\n    if language:\\n        params[\"language\"] = language\\n\\n    return client._request(\"/maps/api/geocode/json\", params).get(\"results\", [])', 'def elevation(client, locations):\\n    \"\"\"\\n    Provides elevation data for locations provided on the surface of the\\n    earth, including depth locations on the ocean floor (which return negative\\n    values)\\n\\n    :param locations: List of latitude/longitude values from which you wish\\n        to calculate elevation data.\\n    :type locations: a single location, or a list of locations, where a\\n        location is a string, dict, list, or tuple\\n\\n    :rtype: list of elevation data responses\\n    \"\"\"\\n    params = {\"locations\": convert.shortest_path(locations)}\\n    return client._request(\"/maps/api/elevation/json\", params).get(\"results\", [])', 'def elevation_along_path(client, path, samples):\\n    \"\"\"\\n    Provides elevation data sampled along a path on the surface of the earth.\\n\\n    :param path: An encoded polyline string, or a list of latitude/longitude\\n        values from which you wish to calculate elevation data.\\n    :type path: string, dict, list, or tuple\\n\\n    :param samples: The number of sample points along a path for which to\\n        return elevation data.\\n    :type samples: int\\n\\n    :rtype: list of elevation data responses\\n    \"\"\"\\n\\n    if type(path) is str:\\n        path = \"enc:%s\" % path\\n    else:\\n        path = convert.shortest_path(path)\\n\\n    params = {\\n        \"path\": path,\\n        \"samples\": samples\\n    }\\n\\n    return client._request(\"/maps/api/elevation/json\", params).get(\"results\", [])', 'def latlng(arg):\\n    \"\"\"Converts a lat/lon pair to a comma-separated string.\\n\\n    For example:\\n\\n    sydney = {\\n        \"lat\" : -33.8674869,\\n        \"lng\" : 151.2069902\\n    }\\n\\n    convert.latlng(sydney)\\n    # \\'-33.8674869,151.2069902\\'\\n\\n    For convenience, also accepts lat/lon pair as a string, in\\n    which case it\\'s returned unchanged.\\n\\n    :param arg: The lat/lon pair.\\n    :type arg: string or dict or list or tuple\\n    \"\"\"\\n    if is_string(arg):\\n        return arg\\n\\n    normalized = normalize_lat_lng(arg)\\n    return \"%s,%s\" % (format_float(normalized[0]), format_float(normalized[1]))', 'def normalize_lat_lng(arg):\\n    \"\"\"Take the various lat/lng representations and return a tuple.\\n\\n    Accepts various representations:\\n    1) dict with two entries - \"lat\" and \"lng\"\\n    2) list or tuple - e.g. (-33, 151) or [-33, 151]\\n\\n    :param arg: The lat/lng pair.\\n    :type arg: dict or list or tuple\\n\\n    :rtype: tuple (lat, lng)\\n    \"\"\"\\n    if isinstance(arg, dict):\\n        if \"lat\" in arg and \"lng\" in arg:\\n            return arg[\"lat\"], arg[\"lng\"]\\n        if \"latitude\" in arg and \"longitude\" in arg:\\n            return arg[\"latitude\"], arg[\"longitude\"]\\n\\n    # List or tuple.\\n    if _is_list(arg):\\n        return arg[0], arg[1]\\n\\n    raise TypeError(\\n        \"Expected a lat/lng dict or tuple, \"\\n        \"but got %s\" % type(arg).__name__)', 'def location_list(arg):\\n    \"\"\"Joins a list of locations into a pipe separated string, handling\\n    the various formats supported for lat/lng values.\\n\\n    For example:\\n    p = [{\"lat\" : -33.867486, \"lng\" : 151.206990}, \"Sydney\"]\\n    convert.waypoint(p)\\n    # \\'-33.867486,151.206990|Sydney\\'\\n\\n    :param arg: The lat/lng list.\\n    :type arg: list\\n\\n    :rtype: string\\n    \"\"\"\\n    if isinstance(arg, tuple):\\n        # Handle the single-tuple lat/lng case.\\n        return latlng(arg)\\n    else:\\n        return \"|\".join([latlng(location) for location in as_list(arg)])', 'def _is_list(arg):\\n    \"\"\"Checks if arg is list-like. This excludes strings and dicts.\"\"\"\\n    if isinstance(arg, dict):\\n        return False\\n    if isinstance(arg, str): # Python 3-only, as str has __iter__\\n        return False\\n    return (not _has_method(arg, \"strip\")\\n            and _has_method(arg, \"__getitem__\")\\n            or _has_method(arg, \"__iter__\"))', 'def is_string(val):\\n    \"\"\"Determines whether the passed value is a string, safe for 2/3.\"\"\"\\n    try:\\n        basestring\\n    except NameError:\\n        return isinstance(val, str)\\n    return isinstance(val, basestring)', 'def time(arg):\\n    \"\"\"Converts the value into a unix time (seconds since unix epoch).\\n\\n    For example:\\n        convert.time(datetime.now())\\n        # \\'1409810596\\'\\n\\n    :param arg: The time.\\n    :type arg: datetime.datetime or int\\n    \"\"\"\\n    # handle datetime instances.\\n    if _has_method(arg, \"timetuple\"):\\n        arg = _time.mktime(arg.timetuple())\\n\\n    if isinstance(arg, float):\\n        arg = int(arg)\\n\\n    return str(arg)', 'def _has_method(arg, method):\\n    \"\"\"Returns true if the given object has a method with the given name.\\n\\n    :param arg: the object\\n\\n    :param method: the method name\\n    :type method: string\\n\\n    :rtype: bool\\n    \"\"\"\\n    return hasattr(arg, method) and callable(getattr(arg, method))', 'def components(arg):\\n    \"\"\"Converts a dict of components to the format expected by the Google Maps\\n    server.\\n\\n    For example:\\n    c = {\"country\": \"US\", \"postal_code\": \"94043\"}\\n    convert.components(c)\\n    # \\'country:US|postal_code:94043\\'\\n\\n    :param arg: The component filter.\\n    :type arg: dict\\n\\n    :rtype: basestring\\n    \"\"\"\\n\\n    # Components may have multiple values per type, here we\\n    # expand them into individual key/value items, eg:\\n    # {\"country\": [\"US\", \"AU\"], \"foo\": 1} -> \"country:AU\", \"country:US\", \"foo:1\"\\n    def expand(arg):\\n        for k, v in arg.items():\\n            for item in as_list(v):\\n                yield \"%s:%s\" % (k, item)\\n\\n    if isinstance(arg, dict):\\n        return \"|\".join(sorted(expand(arg)))\\n\\n    raise TypeError(\\n        \"Expected a dict for components, \"\\n        \"but got %s\" % type(arg).__name__)', 'def bounds(arg):\\n    \"\"\"Converts a lat/lon bounds to a comma- and pipe-separated string.\\n\\n    Accepts two representations:\\n    1) string: pipe-separated pair of comma-separated lat/lon pairs.\\n    2) dict with two entries - \"southwest\" and \"northeast\". See convert.latlng\\n    for information on how these can be represented.\\n\\n    For example:\\n\\n    sydney_bounds = {\\n        \"northeast\" : {\\n            \"lat\" : -33.4245981,\\n            \"lng\" : 151.3426361\\n        },\\n        \"southwest\" : {\\n            \"lat\" : -34.1692489,\\n            \"lng\" : 150.502229\\n        }\\n    }\\n\\n    convert.bounds(sydney_bounds)\\n    # \\'-34.169249,150.502229|-33.424598,151.342636\\'\\n\\n    :param arg: The bounds.\\n    :type arg: dict\\n    \"\"\"\\n\\n    if is_string(arg) and arg.count(\"|\") == 1 and arg.count(\",\") == 2:\\n        return arg\\n    elif isinstance(arg, dict):\\n        if \"southwest\" in arg and \"northeast\" in arg:\\n            return \"%s|%s\" % (latlng(arg[\"southwest\"]),\\n                              latlng(arg[\"northeast\"]))\\n\\n    raise TypeError(\\n        \"Expected a bounds (southwest/northeast) dict, \"\\n        \"but got %s\" % type(arg).__name__)', 'def decode_polyline(polyline):\\n    \"\"\"Decodes a Polyline string into a list of lat/lng dicts.\\n\\n    See the developer docs for a detailed description of this encoding:\\n    https://developers.google.com/maps/documentation/utilities/polylinealgorithm\\n\\n    :param polyline: An encoded polyline\\n    :type polyline: string\\n\\n    :rtype: list of dicts with lat/lng keys\\n    \"\"\"\\n    points = []\\n    index = lat = lng = 0\\n\\n    while index < len(polyline):\\n        result = 1\\n        shift = 0\\n        while True:\\n            b = ord(polyline[index]) - 63 - 1\\n            index += 1\\n            result += b << shift\\n            shift += 5\\n            if b < 0x1f:\\n                break\\n        lat += (~result >> 1) if (result & 1) != 0 else (result >> 1)\\n\\n        result = 1\\n        shift = 0\\n        while True:\\n            b = ord(polyline[index]) - 63 - 1\\n            index += 1\\n            result += b << shift\\n            shift += 5\\n            if b < 0x1f:\\n                break\\n        lng += ~(result >> 1) if (result & 1) != 0 else (result >> 1)\\n\\n        points.append({\"lat\": lat * 1e-5, \"lng\": lng * 1e-5})\\n\\n    return points', 'def encode_polyline(points):\\n    \"\"\"Encodes a list of points into a polyline string.\\n\\n    See the developer docs for a detailed description of this encoding:\\n    https://developers.google.com/maps/documentation/utilities/polylinealgorithm\\n\\n    :param points: a list of lat/lng pairs\\n    :type points: list of dicts or tuples\\n\\n    :rtype: string\\n    \"\"\"\\n    last_lat = last_lng = 0\\n    result = \"\"\\n\\n    for point in points:\\n        ll = normalize_lat_lng(point)\\n        lat = int(round(ll[0] * 1e5))\\n        lng = int(round(ll[1] * 1e5))\\n        d_lat = lat - last_lat\\n        d_lng = lng - last_lng\\n\\n        for v in [d_lat, d_lng]:\\n            v = ~(v << 1) if v < 0 else v << 1\\n            while v >= 0x20:\\n                result += (chr((0x20 | (v & 0x1f)) + 63))\\n                v >>= 5\\n            result += (chr(v + 63))\\n\\n        last_lat = lat\\n        last_lng = lng\\n\\n    return result', 'def timezone(client, location, timestamp=None, language=None):\\n    \"\"\"Get time zone for a location on the earth, as well as that location\\'s\\n    time offset from UTC.\\n\\n    :param location: The latitude/longitude value representing the location to\\n        look up.\\n    :type location: string, dict, list, or tuple\\n\\n    :param timestamp: Timestamp specifies the desired time as seconds since\\n        midnight, January 1, 1970 UTC. The Time Zone API uses the timestamp to\\n        determine whether or not Daylight Savings should be applied. Times\\n        before 1970 can be expressed as negative values. Optional. Defaults to\\n        ``datetime.utcnow()``.\\n    :type timestamp: int or datetime.datetime\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :rtype: dict\\n    \"\"\"\\n\\n    params = {\\n        \"location\": convert.latlng(location),\\n        \"timestamp\": convert.time(timestamp or datetime.utcnow())\\n    }\\n\\n    if language:\\n        params[\"language\"] = language\\n\\n    return client._request( \"/maps/api/timezone/json\", params)', 'def snap_to_roads(client, path, interpolate=False):\\n    \"\"\"Snaps a path to the most likely roads travelled.\\n\\n    Takes up to 100 GPS points collected along a route, and returns a similar\\n    set of data with the points snapped to the most likely roads the vehicle\\n    was traveling along.\\n\\n    :param path: The path to be snapped.\\n    :type path: a single location, or a list of locations, where a\\n        location is a string, dict, list, or tuple\\n\\n    :param interpolate: Whether to interpolate a path to include all points\\n        forming the full road-geometry. When true, additional interpolated\\n        points will also be returned, resulting in a path that smoothly follows\\n        the geometry of the road, even around corners and through tunnels.\\n        Interpolated paths may contain more points than the original path.\\n    :type interpolate: bool\\n\\n    :rtype: A list of snapped points.\\n    \"\"\"\\n\\n    params = {\"path\": convert.location_list(path)}\\n\\n    if interpolate:\\n        params[\"interpolate\"] = \"true\"\\n\\n    return client._request(\"/v1/snapToRoads\", params,\\n                       base_url=_ROADS_BASE_URL,\\n                       accepts_clientid=False,\\n                       extract_body=_roads_extract).get(\"snappedPoints\", [])', 'def nearest_roads(client, points):\\n    \"\"\"Find the closest road segments for each point\\n\\n    Takes up to 100 independent coordinates, and returns the closest road\\n    segment for each point. The points passed do not need to be part of a\\n    continuous path.\\n\\n    :param points: The points for which the nearest road segments are to be\\n        located.\\n    :type points: a single location, or a list of locations, where a\\n        location is a string, dict, list, or tuple\\n\\n    :rtype: A list of snapped points.\\n    \"\"\"\\n\\n    params = {\"points\": convert.location_list(points)}\\n\\n    return client._request(\"/v1/nearestRoads\", params,\\n                       base_url=_ROADS_BASE_URL,\\n                       accepts_clientid=False,\\n                       extract_body=_roads_extract).get(\"snappedPoints\", [])', 'def speed_limits(client, place_ids):\\n    \"\"\"Returns the posted speed limit (in km/h) for given road segments.\\n\\n    :param place_ids: The Place ID of the road segment. Place IDs are returned\\n        by the snap_to_roads function. You can pass up to 100 Place IDs.\\n    :type place_ids: str or list\\n\\n    :rtype: list of speed limits.\\n    \"\"\"\\n\\n    params = [(\"placeId\", place_id) for place_id in convert.as_list(place_ids)]\\n\\n    return client._request(\"/v1/speedLimits\", params,\\n                       base_url=_ROADS_BASE_URL,\\n                       accepts_clientid=False,\\n                       extract_body=_roads_extract).get(\"speedLimits\", [])', 'def snapped_speed_limits(client, path):\\n    \"\"\"Returns the posted speed limit (in km/h) for given road segments.\\n\\n    The provided points will first be snapped to the most likely roads the\\n    vehicle was traveling along.\\n\\n    :param path: The path of points to be snapped.\\n    :type path: a single location, or a list of locations, where a\\n        location is a string, dict, list, or tuple\\n\\n    :rtype: dict with a list of speed limits and a list of the snapped points.\\n    \"\"\"\\n\\n    params = {\"path\": convert.location_list(path)}\\n\\n    return client._request(\"/v1/speedLimits\", params,\\n                       base_url=_ROADS_BASE_URL,\\n                       accepts_clientid=False,\\n                       extract_body=_roads_extract)', 'def _roads_extract(resp):\\n    \"\"\"Extracts a result from a Roads API HTTP response.\"\"\"\\n\\n    try:\\n        j = resp.json()\\n    except:\\n        if resp.status_code != 200:\\n            raise googlemaps.exceptions.HTTPError(resp.status_code)\\n\\n        raise googlemaps.exceptions.ApiError(\"UNKNOWN_ERROR\",\\n                                             \"Received a malformed response.\")\\n\\n    if \"error\" in j:\\n        error = j[\"error\"]\\n        status = error[\"status\"]\\n\\n        if status == \"RESOURCE_EXHAUSTED\":\\n            raise googlemaps.exceptions._OverQueryLimit(status,\\n                                                        error.get(\"message\"))\\n\\n        raise googlemaps.exceptions.ApiError(status, error.get(\"message\"))\\n\\n    if resp.status_code != 200:\\n        raise googlemaps.exceptions.HTTPError(resp.status_code)\\n\\n    return j', 'def find_place(client, input, input_type, fields=None, location_bias=None,\\n                language=None):\\n    \"\"\"\\n    A Find Place request takes a text input, and returns a place.\\n    The text input can be any kind of Places data, for example,\\n    a name, address, or phone number.\\n\\n    :param input: The text input specifying which place to search for (for\\n                  example, a name, address, or phone number).\\n    :type input: string\\n\\n    :param input_type: The type of input. This can be one of either \\'textquery\\'\\n                  or \\'phonenumber\\'.\\n    :type input_type: string\\n\\n    :param fields: The fields specifying the types of place data to return,\\n                   separated by a comma. For full details see:\\n                   https://developers.google.com/places/web-service/search#FindPlaceRequests\\n    :type input: list\\n\\n    :param location_bias: Prefer results in a specified area, by specifying\\n                          either a radius plus lat/lng, or two lat/lng pairs\\n                          representing the points of a rectangle. See:\\n                          https://developers.google.com/places/web-service/search#FindPlaceRequests\\n    :type location_bias: string\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :rtype: result dict with the following keys:\\n            status: status code\\n            candidates: list of places\\n    \"\"\"\\n    params = {\"input\": input, \"inputtype\": input_type}\\n\\n    if input_type != \"textquery\" and input_type != \"phonenumber\":\\n        raise ValueError(\"Valid values for the `input_type` param for \"\\n                         \"`find_place` are \\'textquery\\' or \\'phonenumber\\', \"\\n                         \"the given value is invalid: \\'%s\\'\" % input_type)\\n\\n    if fields:\\n        invalid_fields = set(fields) - PLACES_FIND_FIELDS\\n        if invalid_fields:\\n            raise ValueError(\"Valid values for the `fields` param for \"\\n                             \"`find_place` are \\'%s\\', these given field(s) \"\\n                             \"are invalid: \\'%s\\'\" % (\\n                                \"\\', \\'\".join(PLACES_FIND_FIELDS),\\n                                \"\\', \\'\".join(invalid_fields)))\\n        params[\"fields\"] = convert.join_list(\",\", fields)\\n\\n    if location_bias:\\n        valid = [\"ipbias\", \"point\", \"circle\", \"rectangle\"]\\n        if location_bias.split(\":\")[0] not in valid:\\n            raise ValueError(\"location_bias should be prefixed with one of: %s\"\\n                             % valid)\\n        params[\"locationbias\"] = location_bias\\n    if language:\\n        params[\"language\"] = language\\n\\n    return client._request(\"/maps/api/place/findplacefromtext/json\", params)', 'def places(client, query, location=None, radius=None, language=None,\\n           min_price=None, max_price=None, open_now=False, type=None, region=None,\\n           page_token=None):\\n    \"\"\"\\n    Places search.\\n\\n    :param query: The text string on which to search, for example: \"restaurant\".\\n    :type query: string\\n\\n    :param location: The latitude/longitude value for which you wish to obtain the\\n        closest, human-readable address.\\n    :type location: string, dict, list, or tuple\\n\\n    :param radius: Distance in meters within which to bias results.\\n    :type radius: int\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :param min_price: Restricts results to only those places with no less than\\n        this price level. Valid values are in the range from 0 (most affordable)\\n        to 4 (most expensive).\\n    :type min_price: int\\n\\n    :param max_price: Restricts results to only those places with no greater\\n        than this price level. Valid values are in the range from 0 (most\\n        affordable) to 4 (most expensive).\\n    :type max_price: int\\n\\n    :param open_now: Return only those places that are open for business at\\n        the time the query is sent.\\n    :type open_now: bool\\n\\n    :param type: Restricts the results to places matching the specified type.\\n        The full list of supported types is available here:\\n        https://developers.google.com/places/supported_types\\n    :type type: string\\n\\n    :param region: The region code, optional parameter.\\n        See more @ https://developers.google.com/places/web-service/search\\n    :type region: string\\n\\n    :param page_token: Token from a previous search that when provided will\\n        returns the next page of results for the same search.\\n    :type page_token: string\\n\\n    :rtype: result dict with the following keys:\\n        results: list of places\\n        html_attributions: set of attributions which must be displayed\\n        next_page_token: token for retrieving the next page of results\\n    \"\"\"\\n    return _places(client, \"text\", query=query, location=location,\\n                   radius=radius, language=language, min_price=min_price,\\n                   max_price=max_price, open_now=open_now, type=type, region=region,\\n                   page_token=page_token)', 'def places_nearby(client, location=None, radius=None, keyword=None,\\n                  language=None, min_price=None, max_price=None, name=None,\\n                  open_now=False, rank_by=None, type=None, page_token=None):\\n    \"\"\"\\n    Performs nearby search for places.\\n\\n    :param location: The latitude/longitude value for which you wish to obtain the\\n                     closest, human-readable address.\\n    :type location: string, dict, list, or tuple\\n\\n    :param radius: Distance in meters within which to bias results.\\n    :type radius: int\\n\\n    :param region: The region code, optional parameter.\\n        See more @ https://developers.google.com/places/web-service/search\\n    :type region: string\\n\\n    :param keyword: A term to be matched against all content that Google has\\n                    indexed for this place.\\n    :type keyword: string\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :param min_price: Restricts results to only those places with no less than\\n                      this price level. Valid values are in the range from 0\\n                      (most affordable) to 4 (most expensive).\\n    :type min_price: int\\n\\n    :param max_price: Restricts results to only those places with no greater\\n                      than this price level. Valid values are in the range\\n                      from 0 (most affordable) to 4 (most expensive).\\n    :type max_price: int\\n\\n    :param name: One or more terms to be matched against the names of places.\\n    :type name: string or list of strings\\n\\n    :param open_now: Return only those places that are open for business at\\n                     the time the query is sent.\\n    :type open_now: bool\\n\\n    :param rank_by: Specifies the order in which results are listed.\\n                    Possible values are: prominence (default), distance\\n    :type rank_by: string\\n\\n    :param type: Restricts the results to places matching the specified type.\\n        The full list of supported types is available here:\\n        https://developers.google.com/places/supported_types\\n    :type type: string\\n\\n    :param page_token: Token from a previous search that when provided will\\n                       returns the next page of results for the same search.\\n    :type page_token: string\\n\\n    :rtype: result dict with the following keys:\\n            status: status code\\n            results: list of places\\n            html_attributions: set of attributions which must be displayed\\n            next_page_token: token for retrieving the next page of results\\n\\n    \"\"\"\\n    if not location and not page_token:\\n        raise ValueError(\"either a location or page_token arg is required\")\\n    if rank_by == \"distance\":\\n        if not (keyword or name or type):\\n            raise ValueError(\"either a keyword, name, or type arg is required \"\\n                             \"when rank_by is set to distance\")\\n        elif radius is not None:\\n            raise ValueError(\"radius cannot be specified when rank_by is set to \"\\n                             \"distance\")\\n\\n    return _places(client, \"nearby\", location=location, radius=radius,\\n                   keyword=keyword, language=language, min_price=min_price,\\n                   max_price=max_price, name=name, open_now=open_now,\\n                   rank_by=rank_by, type=type, page_token=page_token)', 'def places_radar(client, location, radius, keyword=None, min_price=None,\\n                 max_price=None, name=None, open_now=False, type=None):\\n    \"\"\"\\n    Performs radar search for places.\\n\\n    :param location: The latitude/longitude value for which you wish to obtain the\\n                     closest, human-readable address.\\n    :type location: string, dict, list, or tuple\\n\\n    :param radius: Distance in meters within which to bias results.\\n    :type radius: int\\n\\n    :param keyword: A term to be matched against all content that Google has\\n                    indexed for this place.\\n    :type keyword: string\\n\\n    :param min_price: Restricts results to only those places with no less than\\n                      this price level. Valid values are in the range from 0\\n                      (most affordable) to 4 (most expensive).\\n    :type min_price: int\\n\\n    :param max_price: Restricts results to only those places with no greater\\n                      than this price level. Valid values are in the range\\n                      from 0 (most affordable) to 4 (most expensive).\\n    :type max_price: int\\n\\n    :param name: One or more terms to be matched against the names of places.\\n    :type name: string or list of strings\\n\\n    :param open_now: Return only those places that are open for business at\\n                     the time the query is sent.\\n    :type open_now: bool\\n\\n    :param type: Restricts the results to places matching the specified type.\\n        The full list of supported types is available here:\\n        https://developers.google.com/places/supported_types\\n    :type type: string\\n\\n    :rtype: result dict with the following keys:\\n            status: status code\\n            results: list of places\\n            html_attributions: set of attributions which must be displayed\\n\\n    \"\"\"\\n    if not (keyword or name or type):\\n        raise ValueError(\"either a keyword, name, or type arg is required\")\\n\\n    from warnings import warn\\n    warn(\"places_radar is deprecated, see http://goo.gl/BGiumE\",\\n         DeprecationWarning)\\n\\n    return _places(client, \"radar\", location=location, radius=radius,\\n                   keyword=keyword, min_price=min_price, max_price=max_price,\\n                   name=name, open_now=open_now, type=type)', 'def _places(client, url_part, query=None, location=None, radius=None,\\n            keyword=None, language=None, min_price=0, max_price=4, name=None,\\n            open_now=False, rank_by=None, type=None, region=None, page_token=None):\\n    \"\"\"\\n    Internal handler for ``places``, ``places_nearby``, and ``places_radar``.\\n    See each method\\'s docs for arg details.\\n    \"\"\"\\n\\n    params = {\"minprice\": min_price, \"maxprice\": max_price}\\n\\n    if query:\\n        params[\"query\"] = query\\n    if location:\\n        params[\"location\"] = convert.latlng(location)\\n    if radius:\\n        params[\"radius\"] = radius\\n    if keyword:\\n        params[\"keyword\"] = keyword\\n    if language:\\n        params[\"language\"] = language\\n    if name:\\n        params[\"name\"] = convert.join_list(\" \", name)\\n    if open_now:\\n        params[\"opennow\"] = \"true\"\\n    if rank_by:\\n        params[\"rankby\"] = rank_by\\n    if type:\\n        params[\"type\"] = type\\n    if region:\\n        params[\"region\"] = region\\n    if page_token:\\n        params[\"pagetoken\"] = page_token\\n\\n    url = \"/maps/api/place/%ssearch/json\" % url_part\\n    return client._request(url, params)', 'def place(client, place_id, session_token=None, fields=None, language=None):\\n    \"\"\"\\n    Comprehensive details for an individual place.\\n\\n    :param place_id: A textual identifier that uniquely identifies a place,\\n        returned from a Places search.\\n    :type place_id: string\\n\\n    :param session_token: A random string which identifies an autocomplete\\n                          session for billing purposes.\\n    :type session_token: string\\n\\n    :param fields: The fields specifying the types of place data to return,\\n                   separated by a comma. For full details see:\\n                   https://cloud.google.com/maps-platform/user-guide/product-changes/#places\\n    :type input: list\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :rtype: result dict with the following keys:\\n        result: dict containing place details\\n        html_attributions: set of attributions which must be displayed\\n    \"\"\"\\n    params = {\"placeid\": place_id}\\n\\n    if fields:\\n        invalid_fields = set(fields) - PLACES_DETAIL_FIELDS\\n        if invalid_fields:\\n            raise ValueError(\"Valid values for the `fields` param for \"\\n                             \"`place` are \\'%s\\', these given field(s) \"\\n                             \"are invalid: \\'%s\\'\" % (\\n                                \"\\', \\'\".join(PLACES_DETAIL_FIELDS),\\n                                \"\\', \\'\".join(invalid_fields)))\\n        params[\"fields\"] = convert.join_list(\",\", fields)\\n\\n    if language:\\n        params[\"language\"] = language\\n    if session_token:\\n        params[\"sessiontoken\"] = session_token\\n\\n    return client._request(\"/maps/api/place/details/json\", params)', 'def places_photo(client, photo_reference, max_width=None, max_height=None):\\n    \"\"\"\\n    Downloads a photo from the Places API.\\n\\n    :param photo_reference: A string identifier that uniquely identifies a\\n        photo, as provided by either a Places search or Places detail request.\\n    :type photo_reference: string\\n\\n    :param max_width: Specifies the maximum desired width, in pixels.\\n    :type max_width: int\\n\\n    :param max_height: Specifies the maximum desired height, in pixels.\\n    :type max_height: int\\n\\n    :rtype: iterator containing the raw image data, which typically can be\\n        used to save an image file locally. For example:\\n\\n        ```\\n        f = open(local_filename, \\'wb\\')\\n        for chunk in client.places_photo(photo_reference, max_width=100):\\n            if chunk:\\n                f.write(chunk)\\n        f.close()\\n        ```\\n    \"\"\"\\n\\n    if not (max_width or max_height):\\n        raise ValueError(\"a max_width or max_height arg is required\")\\n\\n    params = {\"photoreference\": photo_reference}\\n\\n    if max_width:\\n        params[\"maxwidth\"] = max_width\\n    if max_height:\\n        params[\"maxheight\"] = max_height\\n\\n    # \"extract_body\" and \"stream\" args here are used to return an iterable\\n    # response containing the image file data, rather than converting from\\n    # json.\\n    response = client._request(\"/maps/api/place/photo\", params,\\n                           extract_body=lambda response: response,\\n                           requests_kwargs={\"stream\": True})\\n    return response.iter_content()', 'def places_autocomplete(client, input_text, session_token, offset=None,\\n                        location=None, radius=None, language=None, types=None,\\n                        components=None, strict_bounds=False):\\n    \"\"\"\\n    Returns Place predictions given a textual search string and optional\\n    geographic bounds.\\n\\n    :param input_text: The text string on which to search.\\n    :type input_text: string\\n\\n    :param session_token: A random string which identifies an autocomplete\\n                          session for billing purposes.\\n    :type session_token: string\\n\\n    :param offset: The position, in the input term, of the last character\\n                   that the service uses to match predictions. For example,\\n                   if the input is \\'Google\\' and the offset is 3, the\\n                   service will match on \\'Goo\\'.\\n    :type offset: int\\n\\n    :param location: The latitude/longitude value for which you wish to obtain the\\n                     closest, human-readable address.\\n    :type location: string, dict, list, or tuple\\n\\n    :param radius: Distance in meters within which to bias results.\\n    :type radius: int\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :param types: Restricts the results to places matching the specified type.\\n        The full list of supported types is available here:\\n        https://developers.google.com/places/web-service/autocomplete#place_types\\n    :type types: string\\n\\n    :param components: A component filter for which you wish to obtain a geocode.\\n        Currently, you can use components to filter by up to 5 countries for\\n        example: ``{\\'country\\': [\\'US\\', \\'AU\\']}``\\n    :type components: dict\\n\\n    :param strict_bounds: Returns only those places that are strictly within\\n        the region defined by location and radius.\\n    :type strict_bounds: bool\\n\\n    :rtype: list of predictions\\n\\n    \"\"\"\\n    return _autocomplete(client, \"\", input_text, session_token=session_token,\\n                         offset=offset, location=location, radius=radius,\\n                         language=language, types=types, components=components,\\n                         strict_bounds=strict_bounds)', 'def places_autocomplete_query(client, input_text, offset=None, location=None,\\n                              radius=None, language=None):\\n    \"\"\"\\n    Returns Place predictions given a textual search query, such as\\n    \"pizza near New York\", and optional geographic bounds.\\n\\n    :param input_text: The text query on which to search.\\n    :type input_text: string\\n\\n    :param offset: The position, in the input term, of the last character\\n        that the service uses to match predictions. For example, if the input\\n        is \\'Google\\' and the offset is 3, the service will match on \\'Goo\\'.\\n    :type offset: int\\n\\n    :param location: The latitude/longitude value for which you wish to obtain the\\n        closest, human-readable address.\\n    :type location: string, dict, list, or tuple\\n\\n    :param radius: Distance in meters within which to bias results.\\n    :type radius: number\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :rtype: list of predictions\\n    \"\"\"\\n    return _autocomplete(client, \"query\", input_text, offset=offset,\\n                         location=location, radius=radius, language=language)', 'def _autocomplete(client, url_part, input_text, session_token=None,\\n                  offset=None, location=None, radius=None, language=None,\\n                  types=None, components=None, strict_bounds=False):\\n    \"\"\"\\n    Internal handler for ``autocomplete`` and ``autocomplete_query``.\\n    See each method\\'s docs for arg details.\\n    \"\"\"\\n\\n    params = {\"input\": input_text}\\n\\n    if session_token:\\n        params[\"sessiontoken\"] = session_token\\n    if offset:\\n        params[\"offset\"] = offset\\n    if location:\\n        params[\"location\"] = convert.latlng(location)\\n    if radius:\\n        params[\"radius\"] = radius\\n    if language:\\n        params[\"language\"] = language\\n    if types:\\n        params[\"types\"] = types\\n    if components:\\n        if len(components) != 1 or list(components.keys())[0] != \"country\":\\n            raise ValueError(\"Only country components are supported\")\\n        params[\"components\"] = convert.components(components)\\n    if strict_bounds:\\n        params[\"strictbounds\"] = \"true\"\\n\\n    url = \"/maps/api/place/%sautocomplete/json\" % url_part\\n    return client._request(url, params).get(\"predictions\", [])', 'def _geolocation_extract(response):\\n    \"\"\"\\n    Mimics the exception handling logic in ``client._get_body``, but\\n    for geolocation which uses a different response format.\\n    \"\"\"\\n    body = response.json()\\n    if response.status_code in (200, 404):\\n        return body\\n\\n    try:\\n        error = body[\"error\"][\"errors\"][0][\"reason\"]\\n    except KeyError:\\n        error = None\\n\\n    if response.status_code == 403:\\n        raise exceptions._OverQueryLimit(response.status_code, error)\\n    else:\\n        raise exceptions.ApiError(response.status_code, error)', 'def geolocate(client, home_mobile_country_code=None,\\n              home_mobile_network_code=None, radio_type=None, carrier=None,\\n              consider_ip=None, cell_towers=None, wifi_access_points=None):\\n    \"\"\"\\n    The Google Maps Geolocation API returns a location and accuracy\\n    radius based on information about cell towers and WiFi nodes given.\\n\\n    See https://developers.google.com/maps/documentation/geolocation/intro\\n    for more info, including more detail for each parameter below.\\n\\n    :param home_mobile_country_code: The mobile country code (MCC) for\\n        the device\\'s home network.\\n    :type home_mobile_country_code: string\\n\\n    :param home_mobile_network_code: The mobile network code (MCC) for\\n        the device\\'s home network.\\n    :type home_mobile_network_code: string\\n\\n    :param radio_type: The mobile radio type. Supported values are\\n        lte, gsm, cdma, and wcdma. While this field is optional, it\\n        should be included if a value is available, for more accurate\\n        results.\\n    :type radio_type: string\\n\\n    :param carrier: The carrier name.\\n    :type carrier: string\\n\\n    :param consider_ip: Specifies whether to fall back to IP geolocation\\n        if wifi and cell tower signals are not available. Note that the\\n        IP address in the request header may not be the IP of the device.\\n    :type consider_ip: bool\\n\\n    :param cell_towers: A list of cell tower dicts. See\\n        https://developers.google.com/maps/documentation/geolocation/intro#cell_tower_object\\n        for more detail.\\n    :type cell_towers: list of dicts\\n\\n    :param wifi_access_points: A list of WiFi access point dicts. See\\n        https://developers.google.com/maps/documentation/geolocation/intro#wifi_access_point_object\\n        for more detail.\\n    :type wifi_access_points: list of dicts\\n    \"\"\"\\n\\n    params = {}\\n    if home_mobile_country_code is not None:\\n        params[\"homeMobileCountryCode\"] = home_mobile_country_code\\n    if home_mobile_network_code is not None:\\n        params[\"homeMobileNetworkCode\"] = home_mobile_network_code\\n    if radio_type is not None:\\n        params[\"radioType\"] = radio_type\\n    if carrier is not None:\\n        params[\"carrier\"] = carrier\\n    if consider_ip is not None:\\n        params[\"considerIp\"] = consider_ip\\n    if cell_towers is not None:\\n        params[\"cellTowers\"] = cell_towers\\n    if wifi_access_points is not None:\\n        params[\"wifiAccessPoints\"] = wifi_access_points\\n\\n    return client._request(\"/geolocation/v1/geolocate\", {},  # No GET params\\n                           base_url=_GEOLOCATION_BASE_URL,\\n                           extract_body=_geolocation_extract,\\n                           post_json=params)', 'def distance_matrix(client, origins, destinations,\\n                    mode=None, language=None, avoid=None, units=None,\\n                    departure_time=None, arrival_time=None, transit_mode=None,\\n                    transit_routing_preference=None, traffic_model=None, region=None):\\n    \"\"\" Gets travel distance and time for a matrix of origins and destinations.\\n\\n    :param origins: One or more locations and/or latitude/longitude values,\\n        from which to calculate distance and time. If you pass an address as\\n        a string, the service will geocode the string and convert it to a\\n        latitude/longitude coordinate to calculate directions.\\n    :type origins: a single location, or a list of locations, where a\\n        location is a string, dict, list, or tuple\\n\\n    :param destinations: One or more addresses and/or lat/lng values, to\\n        which to calculate distance and time. If you pass an address as a\\n        string, the service will geocode the string and convert it to a\\n        latitude/longitude coordinate to calculate directions.\\n    :type destinations: a single location, or a list of locations, where a\\n        location is a string, dict, list, or tuple\\n\\n    :param mode: Specifies the mode of transport to use when calculating\\n        directions. Valid values are \"driving\", \"walking\", \"transit\" or\\n        \"bicycling\".\\n    :type mode: string\\n\\n    :param language: The language in which to return results.\\n    :type language: string\\n\\n    :param avoid: Indicates that the calculated route(s) should avoid the\\n        indicated features. Valid values are \"tolls\", \"highways\" or \"ferries\".\\n    :type avoid: string\\n\\n    :param units: Specifies the unit system to use when displaying results.\\n        Valid values are \"metric\" or \"imperial\".\\n    :type units: string\\n\\n    :param departure_time: Specifies the desired time of departure.\\n    :type departure_time: int or datetime.datetime\\n\\n    :param arrival_time: Specifies the desired time of arrival for transit\\n        directions. Note: you can\\'t specify both departure_time and\\n        arrival_time.\\n    :type arrival_time: int or datetime.datetime\\n\\n    :param transit_mode: Specifies one or more preferred modes of transit.\\n        This parameter may only be specified for requests where the mode is\\n        transit. Valid values are \"bus\", \"subway\", \"train\", \"tram\", \"rail\".\\n        \"rail\" is equivalent to [\"train\", \"tram\", \"subway\"].\\n    :type transit_mode: string or list of strings\\n\\n    :param transit_routing_preference: Specifies preferences for transit\\n        requests. Valid values are \"less_walking\" or \"fewer_transfers\".\\n    :type transit_routing_preference: string\\n\\n    :param traffic_model: Specifies the predictive travel time model to use.\\n        Valid values are \"best_guess\" or \"optimistic\" or \"pessimistic\".\\n        The traffic_model parameter may only be specified for requests where\\n        the travel mode is driving, and where the request includes a\\n        departure_time.\\n\\n    :param region: Specifies the prefered region the geocoder should search\\n        first, but it will not restrict the results to only this region. Valid\\n        values are a ccTLD code.\\n    :type region: string\\n\\n    :rtype: matrix of distances. Results are returned in rows, each row\\n        containing one origin paired with each destination.\\n    \"\"\"\\n\\n    params = {\\n        \"origins\": convert.location_list(origins),\\n        \"destinations\": convert.location_list(destinations)\\n    }\\n\\n    if mode:\\n        # NOTE(broady): the mode parameter is not validated by the Maps API\\n        # server. Check here to prevent silent failures.\\n        if mode not in [\"driving\", \"walking\", \"bicycling\", \"transit\"]:\\n            raise ValueError(\"Invalid travel mode.\")\\n        params[\"mode\"] = mode\\n\\n    if language:\\n        params[\"language\"] = language\\n\\n    if avoid:\\n        if avoid not in [\"tolls\", \"highways\", \"ferries\"]:\\n            raise ValueError(\"Invalid route restriction.\")\\n        params[\"avoid\"] = avoid\\n\\n    if units:\\n        params[\"units\"] = units\\n\\n    if departure_time:\\n        params[\"departure_time\"] = convert.time(departure_time)\\n\\n    if arrival_time:\\n        params[\"arrival_time\"] = convert.time(arrival_time)\\n\\n    if departure_time and arrival_time:\\n        raise ValueError(\"Should not specify both departure_time and\"\\n                         \"arrival_time.\")\\n\\n    if transit_mode:\\n        params[\"transit_mode\"] = convert.join_list(\"|\", transit_mode)\\n\\n    if transit_routing_preference:\\n        params[\"transit_routing_preference\"] = transit_routing_preference\\n\\n    if traffic_model:\\n        params[\"traffic_model\"] = traffic_model\\n\\n    if region:\\n        params[\"region\"] = region\\n\\n    return client._request(\"/maps/api/distancematrix/json\", params)', 'def make_api_method(func):\\n    \"\"\"\\n    Provides a single entry point for modifying all API methods.\\n    For now this is limited to allowing the client object to be modified\\n    with an `extra_params` keyword arg to each method, that is then used\\n    as the params for each web service request.\\n\\n    Please note that this is an unsupported feature for advanced use only.\\n    It\\'s also currently incompatibile with multiple threads, see GH #160.\\n    \"\"\"\\n    @functools.wraps(func)\\n    def wrapper(*args, **kwargs):\\n        args[0]._extra_params = kwargs.pop(\"extra_params\", None)\\n        result = func(*args, **kwargs)\\n        try:\\n            del args[0]._extra_params\\n        except AttributeError:\\n            pass\\n        return result\\n    return wrapper', 'def sign_hmac(secret, payload):\\n    \"\"\"Returns a base64-encoded HMAC-SHA1 signature of a given string.\\n\\n    :param secret: The key used for the signature, base64 encoded.\\n    :type secret: string\\n\\n    :param payload: The payload to sign.\\n    :type payload: string\\n\\n    :rtype: string\\n    \"\"\"\\n    payload = payload.encode(\\'ascii\\', \\'strict\\')\\n    secret = secret.encode(\\'ascii\\', \\'strict\\')\\n    sig = hmac.new(base64.urlsafe_b64decode(secret), payload, hashlib.sha1)\\n    out = base64.urlsafe_b64encode(sig.digest())\\n    return out.decode(\\'utf-8\\')', 'def urlencode_params(params):\\n    \"\"\"URL encodes the parameters.\\n\\n    :param params: The parameters\\n    :type params: list of key/value tuples.\\n\\n    :rtype: string\\n    \"\"\"\\n    # urlencode does not handle unicode strings in Python 2.\\n    # Firstly, normalize the values so they get encoded correctly.\\n    params = [(key, normalize_for_urlencode(val)) for key, val in params]\\n    # Secondly, unquote unreserved chars which are incorrectly quoted\\n    # by urllib.urlencode, causing invalid auth signatures. See GH #72\\n    # for more info.\\n    return requests.utils.unquote_unreserved(urlencode(params))', 'def _request(self, url, params, first_request_time=None, retry_counter=0,\\n             base_url=_DEFAULT_BASE_URL, accepts_clientid=True,\\n             extract_body=None, requests_kwargs=None, post_json=None):\\n        \"\"\"Performs HTTP GET/POST with credentials, returning the body as\\n        JSON.\\n\\n        :param url: URL path for the request. Should begin with a slash.\\n        :type url: string\\n\\n        :param params: HTTP GET parameters.\\n        :type params: dict or list of key/value tuples\\n\\n        :param first_request_time: The time of the first request (None if no\\n            retries have occurred).\\n        :type first_request_time: datetime.datetime\\n\\n        :param retry_counter: The number of this retry, or zero for first attempt.\\n        :type retry_counter: int\\n\\n        :param base_url: The base URL for the request. Defaults to the Maps API\\n            server. Should not have a trailing slash.\\n        :type base_url: string\\n\\n        :param accepts_clientid: Whether this call supports the client/signature\\n            params. Some APIs require API keys (e.g. Roads).\\n        :type accepts_clientid: bool\\n\\n        :param extract_body: A function that extracts the body from the request.\\n            If the request was not successful, the function should raise a\\n            googlemaps.HTTPError or googlemaps.ApiError as appropriate.\\n        :type extract_body: function\\n\\n        :param requests_kwargs: Same extra keywords arg for requests as per\\n            __init__, but provided here to allow overriding internally on a\\n            per-request basis.\\n        :type requests_kwargs: dict\\n\\n        :raises ApiError: when the API returns an error.\\n        :raises Timeout: if the request timed out.\\n        :raises TransportError: when something went wrong while trying to\\n            exceute a request.\\n        \"\"\"\\n\\n        if not first_request_time:\\n            first_request_time = datetime.now()\\n\\n        elapsed = datetime.now() - first_request_time\\n        if elapsed > self.retry_timeout:\\n            raise googlemaps.exceptions.Timeout()\\n\\n        if retry_counter > 0:\\n            # 0.5 * (1.5 ^ i) is an increased sleep time of 1.5x per iteration,\\n            # starting at 0.5s when retry_counter=0. The first retry will occur\\n            # at 1, so subtract that first.\\n            delay_seconds = 0.5 * 1.5 ** (retry_counter - 1)\\n\\n            # Jitter this value by 50% and pause.\\n            time.sleep(delay_seconds * (random.random() + 0.5))\\n\\n        authed_url = self._generate_auth_url(url, params, accepts_clientid)\\n\\n        # Default to the client-level self.requests_kwargs, with method-level\\n        # requests_kwargs arg overriding.\\n        requests_kwargs = requests_kwargs or {}\\n        final_requests_kwargs = dict(self.requests_kwargs, **requests_kwargs)\\n\\n        # Determine GET/POST.\\n        requests_method = self.session.get\\n        if post_json is not None:\\n            requests_method = self.session.post\\n            final_requests_kwargs[\"json\"] = post_json\\n\\n        try:\\n            response = requests_method(base_url + authed_url,\\n                                       **final_requests_kwargs)\\n        except requests.exceptions.Timeout:\\n            raise googlemaps.exceptions.Timeout()\\n        except Exception as e:\\n            raise googlemaps.exceptions.TransportError(e)\\n\\n        if response.status_code in _RETRIABLE_STATUSES:\\n            # Retry request.\\n            return self._request(url, params, first_request_time,\\n                                 retry_counter + 1, base_url, accepts_clientid,\\n                                 extract_body, requests_kwargs, post_json)\\n\\n        # Check if the time of the nth previous query (where n is\\n        # queries_per_second) is under a second ago - if so, sleep for\\n        # the difference.\\n        if self.sent_times and len(self.sent_times) == self.queries_per_second:\\n            elapsed_since_earliest = time.time() - self.sent_times[0]\\n            if elapsed_since_earliest < 1:\\n                time.sleep(1 - elapsed_since_earliest)\\n\\n        try:\\n            if extract_body:\\n                result = extract_body(response)\\n            else:\\n                result = self._get_body(response)\\n            self.sent_times.append(time.time())\\n            return result\\n        except googlemaps.exceptions._RetriableRequest as e:\\n            if isinstance(e, googlemaps.exceptions._OverQueryLimit) and not self.retry_over_query_limit:\\n                raise\\n\\n            # Retry request.\\n            return self._request(url, params, first_request_time,\\n                                 retry_counter + 1, base_url, accepts_clientid,\\n                                 extract_body, requests_kwargs, post_json)', 'def _generate_auth_url(self, path, params, accepts_clientid):\\n        \"\"\"Returns the path and query string portion of the request URL, first\\n        adding any necessary parameters.\\n\\n        :param path: The path portion of the URL.\\n        :type path: string\\n\\n        :param params: URL parameters.\\n        :type params: dict or list of key/value tuples\\n\\n        :rtype: string\\n\\n        \"\"\"\\n        # Deterministic ordering through sorting by key.\\n        # Useful for tests, and in the future, any caching.\\n        extra_params = getattr(self, \"_extra_params\", None) or {}\\n        if type(params) is dict:\\n            params = sorted(dict(extra_params, **params).items())\\n        else:\\n            params = sorted(extra_params.items()) + params[:] # Take a copy.\\n\\n        if accepts_clientid and self.client_id and self.client_secret:\\n            if self.channel:\\n                params.append((\"channel\", self.channel))\\n            params.append((\"client\", self.client_id))\\n\\n            path = \"?\".join([path, urlencode_params(params)])\\n            sig = sign_hmac(self.client_secret, path)\\n            return path + \"&signature=\" + sig\\n\\n        if self.key:\\n            params.append((\"key\", self.key))\\n            return path + \"?\" + urlencode_params(params)\\n\\n        raise ValueError(\"Must provide API key for this API. It does not accept \"\\n                         \"enterprise credentials.\")', 'def run_pending(self):\\n        \"\"\"\\n        Run all jobs that are scheduled to run.\\n\\n        Please note that it is *intended behavior that run_pending()\\n        does not run missed jobs*. For example, if you\\'ve registered a job\\n        that should run every minute and you only call run_pending()\\n        in one hour increments then your job won\\'t be run 60 times in\\n        between but only once.\\n        \"\"\"\\n        runnable_jobs = (job for job in self.jobs if job.should_run)\\n        for job in sorted(runnable_jobs):\\n            self._run_job(job)', 'def run_all(self, delay_seconds=0):\\n        \"\"\"\\n        Run all jobs regardless if they are scheduled to run or not.\\n\\n        A delay of `delay` seconds is added between each job. This helps\\n        distribute system load generated by the jobs more evenly\\n        over time.\\n\\n        :param delay_seconds: A delay added between every executed job\\n        \"\"\"\\n        logger.info(\\'Running *all* %i jobs with %is delay inbetween\\',\\n                    len(self.jobs), delay_seconds)\\n        for job in self.jobs[:]:\\n            self._run_job(job)\\n            time.sleep(delay_seconds)', 'def clear(self, tag=None):\\n        \"\"\"\\n        Deletes scheduled jobs marked with the given tag, or all jobs\\n        if tag is omitted.\\n\\n        :param tag: An identifier used to identify a subset of\\n                    jobs to delete\\n        \"\"\"\\n        if tag is None:\\n            del self.jobs[:]\\n        else:\\n            self.jobs[:] = (job for job in self.jobs if tag not in job.tags)', 'def tag(self, *tags):\\n        \"\"\"\\n        Tags the job with one or more unique indentifiers.\\n\\n        Tags must be hashable. Duplicate tags are discarded.\\n\\n        :param tags: A unique list of ``Hashable`` tags.\\n        :return: The invoked job instance\\n        \"\"\"\\n        if not all(isinstance(tag, collections.Hashable) for tag in tags):\\n            raise TypeError(\\'Tags must be hashable\\')\\n        self.tags.update(tags)\\n        return self', 'def at(self, time_str):\\n        \"\"\"\\n        Specify a particular time that the job should be run at.\\n\\n        :param time_str: A string in one of the following formats: `HH:MM:SS`,\\n            `HH:MM`,`:MM`, `:SS`. The format must make sense given how often\\n            the job is repeating; for example, a job that repeats every minute\\n            should not be given a string in the form `HH:MM:SS`. The difference\\n            between `:MM` and `:SS` is inferred from the selected time-unit\\n            (e.g. `every().hour.at(\\':30\\')` vs. `every().minute.at(\\':30\\')`).\\n        :return: The invoked job instance\\n        \"\"\"\\n        if (self.unit not in (\\'days\\', \\'hours\\', \\'minutes\\')\\n                and not self.start_day):\\n            raise ScheduleValueError(\\'Invalid unit\\')\\n        if not isinstance(time_str, str):\\n            raise TypeError(\\'at() should be passed a string\\')\\n        if self.unit == \\'days\\' or self.start_day:\\n            if not re.match(r\\'^([0-2]\\\\d:)?[0-5]\\\\d:[0-5]\\\\d$\\', time_str):\\n                raise ScheduleValueError(\\'Invalid time format\\')\\n        if self.unit == \\'hours\\':\\n            if not re.match(r\\'^([0-5]\\\\d)?:[0-5]\\\\d$\\', time_str):\\n                raise ScheduleValueError((\\'Invalid time format for\\'\\n                                          \\' an hourly job\\'))\\n        if self.unit == \\'minutes\\':\\n            if not re.match(r\\'^:[0-5]\\\\d$\\', time_str):\\n                raise ScheduleValueError((\\'Invalid time format for\\'\\n                                          \\' a minutely job\\'))\\n        time_values = time_str.split(\\':\\')\\n        if len(time_values) == 3:\\n            hour, minute, second = time_values\\n        elif len(time_values) == 2 and self.unit == \\'minutes\\':\\n            hour = 0\\n            minute = 0\\n            _, second = time_values\\n        else:\\n            hour, minute = time_values\\n            second = 0\\n        if self.unit == \\'days\\' or self.start_day:\\n            hour = int(hour)\\n            if not (0 <= hour <= 23):\\n                raise ScheduleValueError(\\'Invalid number of hours\\')\\n        elif self.unit == \\'hours\\':\\n            hour = 0\\n        elif self.unit == \\'minutes\\':\\n            hour = 0\\n            minute = 0\\n        minute = int(minute)\\n        second = int(second)\\n        self.at_time = datetime.time(hour, minute, second)\\n        return self', 'def do(self, job_func, *args, **kwargs):\\n        \"\"\"\\n        Specifies the job_func that should be called every time the\\n        job runs.\\n\\n        Any additional arguments are passed on to job_func when\\n        the job runs.\\n\\n        :param job_func: The function to be scheduled\\n        :return: The invoked job instance\\n        \"\"\"\\n        self.job_func = functools.partial(job_func, *args, **kwargs)\\n        try:\\n            functools.update_wrapper(self.job_func, job_func)\\n        except AttributeError:\\n            # job_funcs already wrapped by functools.partial won\\'t have\\n            # __name__, __module__ or __doc__ and the update_wrapper()\\n            # call will fail.\\n            pass\\n        self._schedule_next_run()\\n        self.scheduler.jobs.append(self)\\n        return self', 'def run(self):\\n        \"\"\"\\n        Run the job and immediately reschedule it.\\n\\n        :return: The return value returned by the `job_func`\\n        \"\"\"\\n        logger.info(\\'Running job %s\\', self)\\n        ret = self.job_func()\\n        self.last_run = datetime.datetime.now()\\n        self._schedule_next_run()\\n        return ret', 'def _schedule_next_run(self):\\n        \"\"\"\\n        Compute the instant when this job should run next.\\n        \"\"\"\\n        if self.unit not in (\\'seconds\\', \\'minutes\\', \\'hours\\', \\'days\\', \\'weeks\\'):\\n            raise ScheduleValueError(\\'Invalid unit\\')\\n\\n        if self.latest is not None:\\n            if not (self.latest >= self.interval):\\n                raise ScheduleError(\\'`latest` is greater than `interval`\\')\\n            interval = random.randint(self.interval, self.latest)\\n        else:\\n            interval = self.interval\\n\\n        self.period = datetime.timedelta(**{self.unit: interval})\\n        self.next_run = datetime.datetime.now() + self.period\\n        if self.start_day is not None:\\n            if self.unit != \\'weeks\\':\\n                raise ScheduleValueError(\\'`unit` should be \\\\\\'weeks\\\\\\'\\')\\n            weekdays = (\\n                \\'monday\\',\\n                \\'tuesday\\',\\n                \\'wednesday\\',\\n                \\'thursday\\',\\n                \\'friday\\',\\n                \\'saturday\\',\\n                \\'sunday\\'\\n            )\\n            if self.start_day not in weekdays:\\n                raise ScheduleValueError(\\'Invalid start day\\')\\n            weekday = weekdays.index(self.start_day)\\n            days_ahead = weekday - self.next_run.weekday()\\n            if days_ahead <= 0:  # Target day already happened this week\\n                days_ahead += 7\\n            self.next_run += datetime.timedelta(days_ahead) - self.period\\n        if self.at_time is not None:\\n            if (self.unit not in (\\'days\\', \\'hours\\', \\'minutes\\')\\n                    and self.start_day is None):\\n                raise ScheduleValueError((\\'Invalid unit without\\'\\n                                          \\' specifying start day\\'))\\n            kwargs = {\\n                \\'second\\': self.at_time.second,\\n                \\'microsecond\\': 0\\n            }\\n            if self.unit == \\'days\\' or self.start_day is not None:\\n                kwargs[\\'hour\\'] = self.at_time.hour\\n            if self.unit in [\\'days\\', \\'hours\\'] or self.start_day is not None:\\n                kwargs[\\'minute\\'] = self.at_time.minute\\n            self.next_run = self.next_run.replace(**kwargs)\\n            # If we are running for the first time, make sure we run\\n            # at the specified time *today* (or *this hour*) as well\\n            if not self.last_run:\\n                now = datetime.datetime.now()\\n                if (self.unit == \\'days\\' and self.at_time > now.time() and\\n                        self.interval == 1):\\n                    self.next_run = self.next_run - datetime.timedelta(days=1)\\n                elif self.unit == \\'hours\\' \\\\\\n                        and self.at_time.minute > now.minute \\\\\\n                        or (self.at_time.minute == now.minute\\n                            and self.at_time.second > now.second):\\n                    self.next_run = self.next_run - datetime.timedelta(hours=1)\\n                elif self.unit == \\'minutes\\' \\\\\\n                        and self.at_time.second > now.second:\\n                    self.next_run = self.next_run - \\\\\\n                                    datetime.timedelta(minutes=1)\\n        if self.start_day is not None and self.at_time is not None:\\n            # Let\\'s see if we will still make that time we specified today\\n            if (self.next_run - datetime.datetime.now()).days >= 7:\\n                self.next_run -= self.period', 'async def create(source_id: str, attrs: dict, cred_def_handle: int, name: str, price: str):\\n        \"\"\"\\n            Creates a Class representing an Issuer Credential\\n            :param source_id: Tag associated by user of sdk\\n            :param attrs: attributes that will form the credential\\n            :param cred_def_handle: Handle from previously created credential def object\\n            :param name: Name given to the Credential\\n            :param price: Price, in tokens, required as payment for the issuance of the credential.\\n\\n            Example:\\n            source_id = \\'1\\'\\n            cred_def_handle = 1\\n            attrs = {\\'key\\': \\'value\\', \\'key2\\': \\'value2\\', \\'key3\\': \\'value3\\'}\\n            name = \\'Credential Name\\'\\n            issuer_did = \\'8XFh8yBzrpJQmNyZzgoTqB\\'\\n            phone_number = \\'8019119191\\'\\n            price = 1\\n            issuer_credential = await IssuerCredential.create(source_id, attrs, cred_def_handle, name, price)\\n        \"\"\"\\n        constructor_params = (source_id, attrs, cred_def_handle, name, price)\\n\\n        c_source_id = c_char_p(source_id.encode(\\'utf-8\\'))\\n        c_cred_def_handle = c_uint32(cred_def_handle)\\n        c_price = c_char_p(price.encode(\\'utf-8\\'))\\n        # default institution_did in config is used as issuer_did\\n        c_issuer_did = None\\n        c_data = c_char_p(json.dumps(attrs).encode(\\'utf-8\\'))\\n        c_name = c_char_p(name.encode(\\'utf-8\\'))\\n        c_params = (c_source_id, c_cred_def_handle, c_issuer_did, c_data, c_name, c_price)\\n\\n        return await IssuerCredential._create(\"vcx_issuer_create_credential\",\\n                                              constructor_params,\\n                                              c_params)', 'async def deserialize(data: dict):\\n        \"\"\"\\n            Creates IssuerCredential object from a dict.\\n            :param data: dict representing a serialized IssuerCredential Object\\n            :return: IssuerCredential object\\n\\n            Example:\\n            source_id = \\'1\\'\\n            cred_def_id = \\'cred_def_id1\\'\\n            attrs = {\\'key\\': \\'value\\', \\'key2\\': \\'value2\\', \\'key3\\': \\'value3\\'}\\n            name = \\'Credential Name\\'\\n            issuer_did = \\'8XFh8yBzrpJQmNyZzgoTqB\\'\\n            phone_number = \\'8019119191\\'\\n            price = 1\\n            issuer_credential = await IssuerCredential.create(source_id, attrs, cred_def_id, name, price)\\n            data = await issuer_credential.serialize()\\n            issuer_credential2 = await IssuerCredential.deserialize(data)\\n        \"\"\"\\n        issuer_credential = await IssuerCredential._deserialize(\"vcx_issuer_credential_deserialize\",\\n                                                      json.dumps(data),\\n                                                      data.get(\\'data\\').get(\\'source_id\\'),\\n                                                      data.get(\\'data\\').get(\\'price\\'),\\n                                                      data.get(\\'data\\').get(\\'credential_attributes\\'),\\n                                                      data.get(\\'data\\').get(\\'schema_seq_no\\'),\\n                                                      data.get(\\'data\\').get(\\'credential_request\\'))\\n        return issuer_credential', 'async def send_offer(self, connection: Connection):\\n        \"\"\"\\n        Sends an offer to a prover.  Once accepted, a request will be recieved.\\n        :param connection: vcx.api.connection.Connection\\n        :return: None\\n\\n        Example:\\n        source_id = \\'1\\'\\n        cred_def_id = \\'cred_def_id1\\'\\n        attrs = {\\'key\\': \\'value\\', \\'key2\\': \\'value2\\', \\'key3\\': \\'value3\\'}\\n        name = \\'Credential Name\\'\\n        issuer_did = \\'8XFh8yBzrpJQmNyZzgoTqB\\'\\n        phone_number = \\'8019119191\\'\\n        price = 1\\n        issuer_credential = await IssuerCredential.create(source_id, attrs, cred_def_id, name, price)\\n        connection = await Connection.create(source_id)\\n        issuer_credential.send_offer(connection)\\n        \"\"\"\\n        if not hasattr(IssuerCredential.send_offer, \"cb\"):\\n            self.logger.debug(\"vcx_issuer_send_credential_offer: Creating callback\")\\n            IssuerCredential.send_offer.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n        c_credential_handle = c_uint32(self.handle)\\n        c_connection_handle = c_uint32(connection.handle)\\n\\n        await do_call(\\'vcx_issuer_send_credential_offer\\',\\n                      c_credential_handle,\\n                      c_connection_handle,\\n                      IssuerCredential.send_offer.cb)', 'async def revoke_credential(self):\\n        \"\"\"\\n        Revokes a credential.\\n        :return: None\\n            Example:\\n            credential.revoke_credential()\\n        \"\"\"\\n        if not hasattr(IssuerCredential.revoke_credential, \"cb\"):\\n            self.logger.debug(\"vcx_issuer_revoke_credential: Creating callback\")\\n            IssuerCredential.revoke_credential.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n        c_credential_handle = c_uint32(self.handle)\\n\\n        await do_call(\\'vcx_issuer_revoke_credential\\',\\n                      c_credential_handle,\\n                      IssuerCredential.revoke_credential.cb)', 'async def create_wallet(config: str,\\n                        credentials: str) -> None:\\n    \"\"\"\\n    Creates a new secure wallet with the given unique name.\\n\\n    :param config: Wallet configuration json.\\n     {\\n       \"id\": string, Identifier of the wallet.\\n             Configured storage uses this identifier to lookup exact wallet data placement.\\n       \"storage_type\": optional<string>, Type of the wallet storage. Defaults to \\'default\\'.\\n                      \\'Default\\' storage type allows to store wallet data in the local file.\\n                      Custom storage types can be registered with indy_register_wallet_storage call.\\n       \"storage_config\": optional<object>, Storage configuration json. Storage type defines set of supported keys.\\n                         Can be optional if storage supports default configuration.\\n                          For \\'default\\' storage type configuration is:\\n       {\\n         \"path\": optional<string>, Path to the directory with wallet files.\\n                 Defaults to $HOME/.indy_client/wallet.\\n                 Wallet will be stored in the file {path}/{id}/sqlite.db\\n       }\\n     }\\n    :param credentials: Wallet credentials json\\n     {\\n       \"key\": string, Key or passphrase used for wallet key derivation.\\n                      Look to key_derivation_method param for information about supported key derivation methods.\\n       \"storage_credentials\": optional<object> Credentials for wallet storage. Storage type defines set of supported keys.\\n                              Can be optional if storage supports default configuration.\\n                               For \\'default\\' storage type should be empty.\\n       \"key_derivation_method\": optional<string> Algorithm to use for wallet key derivation:\\n                                ARGON2I_MOD - derive secured wallet master key (used by default)\\n                                ARGON2I_INT - derive secured wallet master key (less secured but faster)\\n                                RAW - raw wallet key master provided (skip derivation).\\n                                      RAW keys can be generated with generate_wallet_key call\\n     }\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"create_wallet: >>> config: %r, credentials: %r\",\\n                 config,\\n                 credentials)\\n\\n    if not hasattr(create_wallet, \"cb\"):\\n        logger.debug(\"create_wallet: Creating callback\")\\n        create_wallet.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_config = c_char_p(config.encode(\\'utf-8\\'))\\n    c_credentials = c_char_p(credentials.encode(\\'utf-8\\'))\\n\\n    await do_call(\\'indy_create_wallet\\',\\n                  c_config,\\n                  c_credentials,\\n                  create_wallet.cb)\\n\\n    logger.debug(\"create_wallet: <<<\")', 'async def close_wallet(handle: int) -> None:\\n    \"\"\"\\n    Closes opened wallet and frees allocated resources.\\n\\n    :param handle: wallet handle returned by indy_open_wallet.\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"close_wallet: >>> handle: %i\", handle)\\n\\n    if not hasattr(close_wallet, \"cb\"):\\n        logger.debug(\"close_wallet: Creating callback\")\\n        close_wallet.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_handle = c_int32(handle)\\n\\n    await do_call(\\'indy_close_wallet\\',\\n                  c_handle,\\n                  close_wallet.cb)\\n\\n    logger.debug(\"close_wallet: <<<\")', 'async def export_wallet(handle: int,\\n                        export_config_json: str) -> None:\\n    \"\"\"\\n    Exports opened wallet to the file.\\n\\n    :param handle: wallet handle returned by indy_open_wallet.\\n    :param export_config_json: JSON containing settings for input operation.\\n       {\\n          \"path\": path of the file that contains exported wallet content\\n          \"key\": string, Key or passphrase used for wallet export key derivation.\\n                         Look to key_derivation_method param for information about supported key derivation methods.\\n          \"key_derivation_method\": optional<string> algorithm to use for export key derivation:\\n                                ARGON2I_MOD - derive secured wallet export key (used by default)\\n                                ARGON2I_INT - derive secured wallet export key (less secured but faster)\\n                                RAW - raw wallet export key provided (skip derivation).\\n                                      RAW keys can be generated with generate_wallet_key call\\n       }\\n    :return:\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"export_wallet: >>> handle: %r, export_config_json: %r\",\\n                 handle,\\n                 export_config_json)\\n\\n    if not hasattr(export_wallet, \"cb\"):\\n        logger.debug(\"export_wallet: Creating callback\")\\n        export_wallet.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_export_config_json = c_char_p(export_config_json.encode(\\'utf-8\\'))\\n\\n    await do_call(\\'indy_export_wallet\\',\\n                  handle,\\n                  c_export_config_json,\\n                  export_wallet.cb)\\n\\n    logger.debug(\"export_wallet: <<<\")', 'async def import_wallet(config: str,\\n                        credentials: str,\\n                        import_config_json: str) -> None:\\n    \"\"\"\\n    Creates a new secure wallet with the given unique name and then imports its content\\n    according to fields provided in import_config\\n    This can be seen as an indy_create_wallet call with additional content import\\n\\n    :param config: Wallet configuration json.\\n     {\\n       \"id\": string, Identifier of the wallet.\\n             Configured storage uses this identifier to lookup exact wallet data placement.\\n       \"storage_type\": optional<string>, Type of the wallet storage. Defaults to \\'default\\'.\\n                      \\'Default\\' storage type allows to store wallet data in the local file.\\n                      Custom storage types can be registered with indy_register_wallet_storage call.\\n       \"storage_config\": optional<object>, Storage configuration json. Storage type defines set of supported keys.\\n                         Can be optional if storage supports default configuration.\\n                         For \\'default\\' storage type configuration is:\\n       {\\n         \"path\": optional<string>, Path to the directory with wallet files.\\n                 Defaults to $HOME/.indy_client/wallet.\\n                 Wallet will be stored in the file {path}/{id}/sqlite.db\\n       }\\n     }\\n    :param credentials: Wallet credentials json\\n     {\\n       \"key\": string, Key or passphrase used for wallet key derivation.\\n                      Look to key_derivation_method param for information about supported key derivation methods.\\n       \"storage_credentials\": optional<object> Credentials for wallet storage. Storage type defines set of supported keys.\\n                              Can be optional if storage supports default configuration.\\n                              For \\'default\\' storage type should be empty.\\n       \"key_derivation_method\": optional<string> Algorithm to use for wallet key derivation:\\n                                 ARGON2I_MOD - derive secured wallet master key (used by default)\\n                                 ARGON2I_INT - derive secured wallet master key (less secured but faster)\\n                                 RAW - raw wallet key master provided (skip derivation).\\n                                       RAW keys can be generated with generate_wallet_key call\\n     }\\n    :param import_config_json: JSON containing settings for input operationЖ {\\n     \"path\": path of the file that contains exported wallet content\\n     \"key\": key used for export of the wallet\\n   }\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"import_wallet: >>> config: %r, credentials: %r, import_config_json: %r\",\\n                 config,\\n                 credentials,\\n                 import_config_json)\\n\\n    if not hasattr(import_wallet, \"cb\"):\\n        logger.debug(\"import_wallet: Creating callback\")\\n        import_wallet.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_config = c_char_p(config.encode(\\'utf-8\\'))\\n    c_credentials = c_char_p(credentials.encode(\\'utf-8\\'))\\n    c_import_config_json = c_char_p(import_config_json.encode(\\'utf-8\\'))\\n\\n    await do_call(\\'indy_import_wallet\\',\\n                  c_config,\\n                  c_credentials,\\n                  c_import_config_json,\\n                  import_wallet.cb)\\n\\n    logger.debug(\"import_wallet: <<<\")', 'async def generate_wallet_key(config: Optional[str]) -> str:\\n    \"\"\"\\n    Generate wallet master key.\\n    Returned key is compatible with \"RAW\" key derivation method.\\n    It allows to avoid expensive key derivation for use cases when wallet keys can be stored in a secure enclave.\\n\\n    :param config: (optional) key configuration json.\\n     {\\n        \"seed\": string, (optional) Seed that allows deterministic key creation (if not set random one will be created).\\n                                   Can be UTF-8, base64 or hex string.\\n     }\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"generate_wallet_key: >>> config: %r\",\\n                 config)\\n\\n    if not hasattr(generate_wallet_key, \"cb\"):\\n        logger.debug(\"generate_wallet_key: Creating callback\")\\n        generate_wallet_key.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_config = c_char_p(config.encode(\\'utf-8\\')) if config is not None else None\\n\\n    key = await do_call(\\'indy_generate_wallet_key\\',\\n                        c_config,\\n                        generate_wallet_key.cb)\\n\\n    res = key.decode()\\n\\n    logger.debug(\"generate_wallet_key: <<< res: %r\", res)\\n    return res', 'async def issuer_create_schema(issuer_did: str,\\n                               name: str,\\n                               version: str,\\n                               attrs: str) -> (str, str):\\n    \"\"\"\\n    Create credential schema entity that describes credential attributes list and allows credentials\\n    interoperability.\\n\\n    Schema is public and intended to be shared with all anoncreds workflow actors usually by publishing SCHEMA transaction\\n    to Indy distributed ledger.\\n\\n    It is IMPORTANT for current version POST Schema in Ledger and after that GET it from Ledger\\n    with correct seq_no to save compatibility with Ledger.\\n    After that can call indy_issuer_create_and_store_credential_def to build corresponding Credential Definition.\\n\\n    :param issuer_did: DID of schema issuer\\n    :param name: a name the schema\\n    :param version: a version of the schema\\n    :param attrs: a list of schema attributes descriptions (the number of attributes should be less or equal than 125)\\n    :return:\\n        schema_id: identifier of created schema\\n        schema_json: schema as json\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"issuer_create_schema: >>> issuer_did: %r, name: %r, version: %r, attrs: %r\",\\n                 issuer_did,\\n                 name,\\n                 version,\\n                 attrs)\\n\\n    if not hasattr(issuer_create_schema, \"cb\"):\\n        logger.debug(\"issuer_create_schema: Creating callback\")\\n        issuer_create_schema.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_issuer_did = c_char_p(issuer_did.encode(\\'utf-8\\'))\\n    c_name = c_char_p(name.encode(\\'utf-8\\'))\\n    c_version = c_char_p(version.encode(\\'utf-8\\'))\\n    c_attrs = c_char_p(attrs.encode(\\'utf-8\\'))\\n\\n    (schema_id, schema_json) = await do_call(\\'indy_issuer_create_schema\\',\\n                                             c_issuer_did,\\n                                             c_name,\\n                                             c_version,\\n                                             c_attrs,\\n                                             issuer_create_schema.cb)\\n\\n    res = (schema_id.decode(), schema_json.decode())\\n    logger.debug(\"issuer_create_schema: <<< res: %r\", res)\\n    return res', 'async def issuer_create_and_store_credential_def(wallet_handle: int,\\n                                                 issuer_did: str,\\n                                                 schema_json: str,\\n                                                 tag: str,\\n                                                 signature_type: Optional[str],\\n                                                 config_json: Optional[str]) -> (str, str):\\n    \"\"\"\\n    Create credential definition entity that encapsulates credentials issuer DID, credential schema, secrets used for\\n    signing credentials and secrets used for credentials revocation.\\n\\n    Credential definition entity contains private and public parts. Private part will be stored in the wallet.\\n    Public part will be returned as json intended to be shared with all anoncreds workflow actors usually by\\n    publishing CRED_DEF transaction to Indy distributed ledger.\\n\\n    It is IMPORTANT for current version GET Schema from Ledger with correct seq_no to save compatibility with Ledger.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param issuer_did: a DID of the issuer signing cred_def transaction to the Ledger\\n    :param schema_json: credential schema as a json\\n    :param tag: allows to distinct between credential definitions for the same issuer and schema\\n    :param signature_type: credential definition type (optional, \\'CL\\' by default) that defines credentials signature and revocation math.\\n    Supported types are:\\n        - \\'CL\\': Camenisch-Lysyanskaya credential signature type\\n    :param  config_json: (optional) type-specific configuration of credential definition as json:\\n        - \\'CL\\':\\n          - support_revocation: whether to request non-revocation credential (optional, default false)\\n    :return: \\n        cred_def_id: identifier of created credential definition\\n        cred_def_json: public part of created credential definition\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"issuer_create_and_store_credential_def: >>> wallet_handle: %r, issuer_did: %r, schema_json: %r,\"\\n                 \" tag: %r, signature_type: %r, config_json: %r\",\\n                 wallet_handle,\\n                 issuer_did,\\n                 schema_json,\\n                 tag,\\n                 signature_type,\\n                 config_json)\\n\\n    if not hasattr(issuer_create_and_store_credential_def, \"cb\"):\\n        logger.debug(\"issuer_create_and_store_credential_def: Creating callback\")\\n        issuer_create_and_store_credential_def.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_issuer_did = c_char_p(issuer_did.encode(\\'utf-8\\'))\\n    c_schema_json = c_char_p(schema_json.encode(\\'utf-8\\'))\\n    c_tag = c_char_p(tag.encode(\\'utf-8\\'))\\n    c_signature_type = c_char_p(signature_type.encode(\\'utf-8\\')) if signature_type is not None else None\\n    c_config_json = c_char_p(config_json.encode(\\'utf-8\\')) if config_json is not None else None\\n\\n    (credential_def_id, credential_def_json) = await do_call(\\'indy_issuer_create_and_store_credential_def\\',\\n                                                             c_wallet_handle,\\n                                                             c_issuer_did,\\n                                                             c_schema_json,\\n                                                             c_tag,\\n                                                             c_signature_type,\\n                                                             c_config_json,\\n                                                             issuer_create_and_store_credential_def.cb)\\n\\n    res = (credential_def_id.decode(), credential_def_json.decode())\\n    logger.debug(\"issuer_create_and_store_credential_def: <<< res: %r\", res)\\n    return res', 'async def issuer_create_and_store_revoc_reg(wallet_handle: int,\\n                                            issuer_did: str,\\n                                            revoc_def_type: Optional[str],\\n                                            tag: str,\\n                                            cred_def_id: str,\\n                                            config_json: str,\\n                                            tails_writer_handle: int) -> (str, str, str):\\n    \"\"\"\\n    Create a new revocation registry for the given credential definition as tuple of entities:\\n    - Revocation registry definition that encapsulates credentials definition reference, revocation type specific configuration and\\n      secrets used for credentials revocation\\n    - Revocation registry state that stores the information about revoked entities in a non-disclosing way. The state can be\\n      represented as ordered list of revocation registry entries were each entry represents the list of revocation or issuance operations.\\n\\n    Revocation registry definition entity contains private and public parts. Private part will be stored in the wallet. Public part\\n    will be returned as json intended to be shared with all anoncreds workflow actors usually by publishing REVOC_REG_DEF transaction\\n    to Indy distributed ledger.\\n\\n    Revocation registry state is stored on the wallet and also intended to be shared as the ordered list of REVOC_REG_ENTRY transactions.\\n    This call initializes the state in the wallet and returns the initial entry.\\n\\n    Some revocation registry types (for example, \\'CL_ACCUM\\') can require generation of binary blob called tails used to hide information about revoked credentials in public\\n    revocation registry and intended to be distributed out of leger (REVOC_REG_DEF transaction will still contain uri and hash of tails).\\n    This call requires access to pre-configured blob storage writer instance handle that will allow to write generated tails.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param issuer_did: a DID of the issuer signing transaction to the Ledger\\n    :param revoc_def_type: revocation registry type (optional, default value depends on credential definition type). Supported types are:\\n        - \\'CL_ACCUM\\': Type-3 pairing based accumulator. Default for \\'CL\\' credential definition type\\n    :param tag: allows to distinct between revocation registries for the same issuer and credential definition\\n    :param cred_def_id: id of stored in ledger credential definition\\n    :param config_json: type-specific configuration of revocation registry as json:\\n        - \\'CL_ACCUM\\':\\n            \"issuance_type\": (optional) type of issuance. Currently supported:\\n                1) ISSUANCE_BY_DEFAULT: all indices are assumed to be issued and initial accumulator is calculated over all indices;\\n                   Revocation Registry is updated only during revocation.\\n                2) ISSUANCE_ON_DEMAND: nothing is issued initially accumulator is 1 (used by default);\\n            \"max_cred_num\": maximum number of credentials the new registry can process (optional, default 100000)\\n        }\\n    :param tails_writer_handle:\\n    :return: \\n        revoc_reg_id: identifier of created revocation registry definition\\n        revoc_reg_def_json: public part of revocation registry definition\\n        revoc_reg_entry_json: revocation registry entry that defines initial state of revocation registry\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"issuer_create_and_store_revoc_reg: >>> wallet_handle: %r, issuer_did: %r, revoc_def_type: %r,\"\\n                 \" tag: %r, cred_def_id: %r, config_json: %r, tails_writer_handle: %r\",\\n                 wallet_handle,\\n                 issuer_did,\\n                 revoc_def_type,\\n                 tag,\\n                 cred_def_id,\\n                 config_json,\\n                 tails_writer_handle)\\n\\n    if not hasattr(issuer_create_and_store_revoc_reg, \"cb\"):\\n        logger.debug(\"issuer_create_and_store_revoc_reg: Creating callback\")\\n        issuer_create_and_store_revoc_reg.cb = create_cb(\\n            CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_issuer_did = c_char_p(issuer_did.encode(\\'utf-8\\'))\\n    c_revoc_def_type = c_char_p(revoc_def_type.encode(\\'utf-8\\')) if revoc_def_type is not None else None\\n    c_tag = c_char_p(tag.encode(\\'utf-8\\'))\\n    c_cred_def_id = c_char_p(cred_def_id.encode(\\'utf-8\\'))\\n    c_config_json = c_char_p(config_json.encode(\\'utf-8\\'))\\n    c_tails_writer_handle = c_int32(tails_writer_handle)\\n\\n    (rev_reg_id, rev_reg_def_json, rev_reg_entry_json) = await do_call(\\'indy_issuer_create_and_store_revoc_reg\\',\\n                                                                       c_wallet_handle,\\n                                                                       c_issuer_did,\\n                                                                       c_revoc_def_type,\\n                                                                       c_tag,\\n                                                                       c_cred_def_id,\\n                                                                       c_config_json,\\n                                                                       c_tails_writer_handle,\\n                                                                       issuer_create_and_store_revoc_reg.cb)\\n    res = (rev_reg_id.decode(), rev_reg_def_json.decode(), rev_reg_entry_json.decode())\\n    logger.debug(\"issuer_create_and_store_revoc_reg: <<< res: %r\", res)\\n    return res', 'async def issuer_create_credential_offer(wallet_handle: int,\\n                                         cred_def_id: str) -> str:\\n    \"\"\"\\n    Create credential offer that will be used by Prover for\\n    credential request creation. Offer includes nonce and key correctness proof\\n    for authentication between protocol steps and integrity checking.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param cred_def_id: id of credential definition stored in the wallet\\n    :return:credential offer json:\\n     {\\n         \"schema_id\": string,\\n         \"cred_def_id\": string,\\n         // Fields below can depend on Cred Def type\\n         \"nonce\": string,\\n         \"key_correctness_proof\" : <key_correctness_proof>\\n     }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"issuer_create_credential_offer: >>> wallet_handle: %r, cred_def_id: %r\",\\n                 wallet_handle,\\n                 cred_def_id)\\n\\n    if not hasattr(issuer_create_credential_offer, \"cb\"):\\n        logger.debug(\"issuer_create_credential_offer: Creating callback\")\\n        issuer_create_credential_offer.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_cred_def_id = c_char_p(cred_def_id.encode(\\'utf-8\\'))\\n\\n    credential_offer_json = await do_call(\\'indy_issuer_create_credential_offer\\',\\n                                          c_wallet_handle,\\n                                          c_cred_def_id,\\n                                          issuer_create_credential_offer.cb)\\n\\n    res = credential_offer_json.decode()\\n    logger.debug(\"issuer_create_credential_offer: <<< res: %r\", res)\\n    return res', 'async def issuer_create_credential(wallet_handle: int,\\n                                   cred_offer_json: str,\\n                                   cred_req_json: str,\\n                                   cred_values_json: str,\\n                                   rev_reg_id: Optional[str],\\n                                   blob_storage_reader_handle: Optional[int]) -> (str, Optional[str], Optional[str]):\\n    \"\"\"\\n    Check Cred Request for the given Cred Offer and issue Credential for the given Cred Request.\\n\\n    Cred Request must match Cred Offer. The credential definition and revocation registry definition\\n    referenced in Cred Offer and Cred Request must be already created and stored into the wallet.\\n\\n    Information for this credential revocation will be store in the wallet as part of revocation registry under\\n    generated cred_revoc_id local for this wallet.\\n\\n    This call returns revoc registry delta as json file intended to be shared as REVOC_REG_ENTRY transaction.\\n    Note that it is possible to accumulate deltas to reduce ledger load.\\n\\n    :param wallet_handle: wallet handle (created by open_wallet).\\n    :param cred_offer_json: a cred offer created by issuer_create_credential_offer\\n    :param cred_req_json: a credential request created by prover_create_credential_req\\n    :param cred_values_json: a credential containing attribute values for each of requested attribute names.\\n     Example:\\n     {\\n      \"attr1\" : {\"raw\": \"value1\", \"encoded\": \"value1_as_int\" },\\n      \"attr2\" : {\"raw\": \"value1\", \"encoded\": \"value1_as_int\" }\\n     }\\n    :param rev_reg_id: (Optional) id of revocation registry definition stored in the wallet\\n    :param blob_storage_reader_handle: pre-configured blob storage reader instance handle that\\n    will allow to read revocation tails\\n    :return: \\n     cred_json: Credential json containing signed credential values\\n     {\\n         \"schema_id\": string,\\n         \"cred_def_id\": string,\\n         \"rev_reg_def_id\", Optional<string>,\\n         \"values\": <see cred_values_json above>,\\n         // Fields below can depend on Cred Def type\\n         \"signature\": <signature>,\\n         \"signature_correctness_proof\": <signature_correctness_proof>\\n     }\\n     cred_revoc_id: local id for revocation info (Can be used for revocation of this cred)\\n     revoc_reg_delta_json: Revocation registry delta json with a newly issued credential\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"issuer_create_credential: >>> wallet_handle: %r, cred_offer_json: %r, cred_req_json: %r,\"\\n                 \" cred_values_json: %r, rev_reg_id: %r, blob_storage_reader_handle: %r\",\\n                 wallet_handle,\\n                 cred_offer_json,\\n                 cred_req_json,\\n                 cred_values_json,\\n                 rev_reg_id,\\n                 blob_storage_reader_handle)\\n\\n    if not hasattr(issuer_create_credential, \"cb\"):\\n        logger.debug(\"issuer_create_credential: Creating callback\")\\n        issuer_create_credential.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_cred_offer_json = c_char_p(cred_offer_json.encode(\\'utf-8\\'))\\n    c_cred_req_json = c_char_p(cred_req_json.encode(\\'utf-8\\'))\\n    c_cred_values_json = c_char_p(cred_values_json.encode(\\'utf-8\\'))\\n    c_rev_reg_id = c_char_p(rev_reg_id.encode(\\'utf-8\\')) if rev_reg_id is not None else None\\n    c_blob_storage_reader_handle = c_int32(blob_storage_reader_handle) if blob_storage_reader_handle else -1\\n\\n    (cred_json, cred_revoc_id, revoc_reg_delta_json) = await do_call(\\'indy_issuer_create_credential\\',\\n                                                                     c_wallet_handle,\\n                                                                     c_cred_offer_json,\\n                                                                     c_cred_req_json,\\n                                                                     c_cred_values_json,\\n                                                                     c_rev_reg_id,\\n                                                                     c_blob_storage_reader_handle,\\n                                                                     issuer_create_credential.cb)\\n    cred_json = cred_json.decode()\\n    cred_revoc_id = cred_revoc_id.decode() if cred_revoc_id else None\\n    revoc_reg_delta_json = revoc_reg_delta_json.decode() if revoc_reg_delta_json else None\\n    res = (cred_json, cred_revoc_id, revoc_reg_delta_json)\\n\\n    logger.debug(\"issuer_create_credential: <<< res: %r\", res)\\n    return res', 'async def issuer_revoke_credential(wallet_handle: int,\\n                                   blob_storage_reader_handle: int,\\n                                   rev_reg_id: str,\\n                                   cred_revoc_id: str) -> str:\\n    \"\"\"\\n    Revoke a credential identified by a cred_revoc_id (returned by issuer_create_credential).\\n\\n    The corresponding credential definition and revocation registry must be already\\n    created an stored into the wallet.\\n\\n    This call returns revoc registry delta as json file intended to be shared as REVOC_REG_ENTRY transaction.\\n    Note that it is possible to accumulate deltas to reduce ledger load.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param blob_storage_reader_handle: pre-configured blob storage reader instance handle that will allow\\n    to read revocation tails\\n    :param rev_reg_id: id of revocation registry stored in wallet\\n    :param cred_revoc_id: local id for revocation info\\n    :return: Revocation registry delta json with a revoked credential.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\\n        \"issuer_revoke_credential: >>> wallet_handle: %r, blob_storage_reader_handle: %r, rev_reg_id: %r, \"\\n        \"cred_revoc_id: %r\",\\n        wallet_handle,\\n        blob_storage_reader_handle,\\n        rev_reg_id,\\n        cred_revoc_id)\\n\\n    if not hasattr(issuer_revoke_credential, \"cb\"):\\n        logger.debug(\"issuer_revoke_credential: Creating callback\")\\n        issuer_revoke_credential.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_blob_storage_reader_handle = c_int32(blob_storage_reader_handle)\\n    c_rev_reg_id = c_char_p(rev_reg_id.encode(\\'utf-8\\'))\\n    c_cred_revoc_id = c_char_p(cred_revoc_id.encode(\\'utf-8\\'))\\n\\n    revoc_reg_delta_json = await do_call(\\'indy_issuer_revoke_credential\\',\\n                                         c_wallet_handle,\\n                                         c_blob_storage_reader_handle,\\n                                         c_rev_reg_id,\\n                                         c_cred_revoc_id,\\n                                         issuer_revoke_credential.cb)\\n    res = revoc_reg_delta_json.decode()\\n    logger.debug(\"issuer_revoke_credential: <<< res: %r\", res)\\n    return res', 'async def issuer_merge_revocation_registry_deltas(rev_reg_delta_json: str,\\n                                                  other_rev_reg_delta_json: str) -> str:\\n    \"\"\"\\n    Merge two revocation registry deltas (returned by issuer_create_credential or issuer_revoke_credential) to accumulate common delta.\\n    Send common delta to ledger to reduce the load.\\n\\n    :param rev_reg_delta_json: revocation registry delta json\\n    :param other_rev_reg_delta_json: revocation registry delta for which PrevAccum value  is equal to current accum value of rev_reg_delta_json.\\n    :return: Merged revocation registry delta\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\\n        \"issuer_merge_revocation_registry_deltas: >>> rev_reg_delta_json: %r, other_rev_reg_delta_json: %r\",\\n        rev_reg_delta_json,\\n        other_rev_reg_delta_json)\\n\\n    if not hasattr(issuer_merge_revocation_registry_deltas, \"cb\"):\\n        logger.debug(\"issuer_merge_revocation_registry_deltas: Creating callback\")\\n        issuer_merge_revocation_registry_deltas.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_rev_reg_delta_json = c_char_p(rev_reg_delta_json.encode(\\'utf-8\\'))\\n    c_other_rev_reg_delta_json = c_char_p(other_rev_reg_delta_json.encode(\\'utf-8\\'))\\n\\n    merged_revoc_reg_delta_json = await do_call(\\'indy_issuer_merge_revocation_registry_deltas\\',\\n                                                c_rev_reg_delta_json,\\n                                                c_other_rev_reg_delta_json,\\n                                                issuer_merge_revocation_registry_deltas.cb)\\n    res = merged_revoc_reg_delta_json.decode()\\n    logger.debug(\"issuer_merge_revocation_registry_deltas: <<< res: %r\", res)\\n    return res', 'async def prover_create_master_secret(wallet_handle: int,\\n                                      master_secret_name: Optional[str]) -> str:\\n    \"\"\"\\n    Creates a master secret with a given name and stores it in the wallet.\\n    The name must be unique.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param master_secret_name: (optional, if not present random one will be generated) new master id\\n    :return: id of generated master secret.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_create_master_secret: >>> wallet_handle: %r, master_secret_name: %r\",\\n                 wallet_handle,\\n                 master_secret_name)\\n\\n    if not hasattr(prover_create_master_secret, \"cb\"):\\n        logger.debug(\"prover_create_master_secret: Creating callback\")\\n        prover_create_master_secret.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_master_secret_name = c_char_p(master_secret_name.encode(\\'utf-8\\')) if master_secret_name else None\\n\\n    out_master_secret_id = await do_call(\\'indy_prover_create_master_secret\\',\\n                                         c_wallet_handle,\\n                                         c_master_secret_name,\\n                                         prover_create_master_secret.cb)\\n\\n    res = out_master_secret_id.decode()\\n    logger.debug(\"prover_create_master_secret: <<< res: %r\", res)\\n    return res', 'async def prover_create_credential_req(wallet_handle: int,\\n                                       prover_did: str,\\n                                       cred_offer_json: str,\\n                                       cred_def_json: str,\\n                                       master_secret_id: str) -> (str, str):\\n    \"\"\"\\n    Creates a clam request for the given credential offer.\\n\\n    The method creates a blinded master secret for a master secret identified by a provided name.\\n    The master secret identified by the name must be already stored in the secure wallet (see prover_create_master_secret)\\n    The blinded master secret is a part of the credential request.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param prover_did: a DID of the prover\\n    :param cred_offer_json: credential offer as a json containing information about the issuer and a credential\\n    :param cred_def_json: credential definition json related to <cred_def_id> in <cred_offer_json>\\n    :param master_secret_id: the id of the master secret stored in the wallet\\n    :return: \\n     cred_req_json: Credential request json for creation of credential by Issuer\\n     {\\n      \"prover_did\" : string,\\n      \"cred_def_id\" : string,\\n         // Fields below can depend on Cred Def type\\n      \"blinded_ms\" : <blinded_master_secret>,\\n      \"blinded_ms_correctness_proof\" : <blinded_ms_correctness_proof>,\\n      \"nonce\": string\\n    }\\n     cred_req_metadata_json: Credential request metadata json for processing of received form Issuer credential.\\n        Note: cred_req_metadata_json mustn\\'t be shared with Issuer.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_create_credential_req: >>> wallet_handle: %r, prover_did: %r, cred_offer_json: %r,\"\\n                 \" cred_def_json: %r, master_secret_id: %r\",\\n                 wallet_handle,\\n                 prover_did,\\n                 cred_offer_json,\\n                 cred_def_json,\\n                 master_secret_id)\\n\\n    if not hasattr(prover_create_credential_req, \"cb\"):\\n        logger.debug(\"prover_create_credential_req: Creating callback\")\\n        prover_create_credential_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_prover_did = c_char_p(prover_did.encode(\\'utf-8\\'))\\n    c_cred_offer_json = c_char_p(cred_offer_json.encode(\\'utf-8\\'))\\n    c_cred_def_json = c_char_p(cred_def_json.encode(\\'utf-8\\'))\\n    c_master_secret_id = c_char_p(master_secret_id.encode(\\'utf-8\\'))\\n\\n    (credential_req_json, credential_req_metadata_json) = await do_call(\\'indy_prover_create_credential_req\\',\\n                                                                        c_wallet_handle,\\n                                                                        c_prover_did,\\n                                                                        c_cred_offer_json,\\n                                                                        c_cred_def_json,\\n                                                                        c_master_secret_id,\\n                                                                        prover_create_credential_req.cb)\\n\\n    credential_req_json = credential_req_json.decode()\\n    credential_req_metadata_json = credential_req_metadata_json.decode()\\n    res = (credential_req_json, credential_req_metadata_json)\\n\\n    logger.debug(\"prover_create_credential_req: <<< res: %r\", res)\\n    return res', 'async def prover_store_credential(wallet_handle: int,\\n                                  cred_id: Optional[str],\\n                                  cred_req_metadata_json: str,\\n                                  cred_json: str,\\n                                  cred_def_json: str,\\n                                  rev_reg_def_json: Optional[str]) -> str:\\n    \"\"\"\\n    Check credential provided by Issuer for the given credential request,\\n    updates the credential by a master secret and stores in a secure wallet.\\n    \\n    To support efficient search the following tags will be created for stored credential:\\n        {\\n            \"schema_id\": <credential schema id>,\\n            \"schema_issuer_did\": <credential schema issuer did>,\\n            \"schema_name\": <credential schema name>,\\n            \"schema_version\": <credential schema version>,\\n            \"issuer_did\": <credential issuer did>,\\n            \"cred_def_id\": <credential definition id>,\\n            \"rev_reg_id\": <credential revocation registry id>, # \"None\" as string if not present\\n            // for every attribute in <credential values>\\n            \"attr::<attribute name>::marker\": \"1\",\\n            \"attr::<attribute name>::value\": <attribute raw value>,\\n        }\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param cred_id: (optional, default is a random one) identifier by which credential will be stored in the wallet\\n    :param cred_req_metadata_json: a credential request metadata created by prover_create_credential_req\\n    :param cred_json: credential json received from issuer\\n    :param cred_def_json: credential definition json related to <cred_def_id> in <cred_json>\\n    :param rev_reg_def_json: revocation registry definition json related to <rev_reg_def_id> in <cred_json>\\n    :return: cred_id: identifier by which credential is stored in the wallet\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_store_credential: >>> wallet_handle: %r, cred_id: %r, \"\\n                 \"cred_req_metadata_json: %r, cred_json: %r, cred_def_json: %r, rev_reg_def_json: %r\",\\n                 wallet_handle,\\n                 cred_id,\\n                 cred_req_metadata_json,\\n                 cred_json,\\n                 cred_def_json,\\n                 rev_reg_def_json)\\n\\n    if not hasattr(prover_store_credential, \"cb\"):\\n        logger.debug(\"prover_store_credential: Creating callback\")\\n        prover_store_credential.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_cred_id = c_char_p(cred_id.encode(\\'utf-8\\')) if cred_id else None\\n    c_cred_req_metadata_json = c_char_p(cred_req_metadata_json.encode(\\'utf-8\\'))\\n    c_cred_json = c_char_p(cred_json.encode(\\'utf-8\\'))\\n    c_cred_def_json = c_char_p(cred_def_json.encode(\\'utf-8\\'))\\n    c_rev_reg_def_json = c_char_p(rev_reg_def_json.encode(\\'utf-8\\')) if rev_reg_def_json is not None else None\\n\\n    cred_id = await do_call(\\'indy_prover_store_credential\\',\\n                            c_wallet_handle,\\n                            c_cred_id,\\n                            c_cred_req_metadata_json,\\n                            c_cred_json,\\n                            c_cred_def_json,\\n                            c_rev_reg_def_json,\\n                            prover_store_credential.cb)\\n\\n    res = cred_id.decode()\\n    logger.debug(\"prover_store_credential: <<< res: %r\", res)\\n    return res', 'async def prover_get_credential(wallet_handle: int,\\n                                cred_id: str) -> str:\\n    \"\"\"\\n    Gets human readable credential by the given id.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param cred_id: Identifier by which requested credential is stored in the wallet\\n    :return:  credential json\\n     {\\n         \"referent\": string, // cred_id in the wallet\\n         \"attrs\": {\"key1\":\"raw_value1\", \"key2\":\"raw_value2\"},\\n         \"schema_id\": string,\\n         \"cred_def_id\": string,\\n         \"rev_reg_id\": Optional<string>,\\n         \"cred_rev_id\": Optional<string>\\n     }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_get_credential: >>> wallet_handle: %r, cred_id: %r\",\\n                 wallet_handle,\\n                 cred_id)\\n\\n    if not hasattr(prover_get_credential, \"cb\"):\\n        logger.debug(\"prover_get_credential: Creating callback\")\\n        prover_get_credential.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_cred_id = c_char_p(cred_id.encode(\\'utf-8\\'))\\n\\n    credentials_json = await do_call(\\'indy_prover_get_credential\\',\\n                                     c_wallet_handle,\\n                                     c_cred_id,\\n                                     prover_get_credential.cb)\\n\\n    res = credentials_json.decode()\\n    logger.debug(\"prover_get_credential: <<< res: %r\", res)\\n    return res', 'async def prover_get_credentials(wallet_handle: int,\\n                                 filter_json: str) -> str:\\n    \"\"\"\\n    Gets human readable credentials according to the filter.\\n    If filter is NULL, then all credentials are returned.\\n    Credentials can be filtered by tags created during saving of credential.\\n\\n    NOTE: This method is deprecated because immediately returns all fetched credentials.\\n    Use <prover_search_credentials> to fetch records by small batches.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param filter_json: filter for credentials\\n        {\\n            \"schema_id\": string, (Optional)\\n            \"schema_issuer_did\": string, (Optional)\\n            \"schema_name\": string, (Optional)\\n            \"schema_version\": string, (Optional)\\n            \"issuer_did\": string, (Optional)\\n            \"cred_def_id\": string, (Optional)\\n        }\\n    :return:  credentials json\\n     [{\\n         \"referent\": string, // cred_id in the wallet\\n         \"attrs\": {\"key1\":\"raw_value1\", \"key2\":\"raw_value2\"},\\n         \"schema_id\": string,\\n         \"cred_def_id\": string,\\n         \"rev_reg_id\": Optional<string>,\\n         \"cred_rev_id\": Optional<string>\\n     }]\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_get_credentials: >>> wallet_handle: %r, filter_json: %r\",\\n                 wallet_handle,\\n                 filter_json)\\n\\n    if not hasattr(prover_get_credentials, \"cb\"):\\n        logger.debug(\"prover_get_credentials: Creating callback\")\\n        prover_get_credentials.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_filter_json = c_char_p(filter_json.encode(\\'utf-8\\'))\\n\\n    credentials_json = await do_call(\\'indy_prover_get_credentials\\',\\n                                     c_wallet_handle,\\n                                     c_filter_json,\\n                                     prover_get_credentials.cb)\\n\\n    res = credentials_json.decode()\\n    logger.debug(\"prover_get_credentials: <<< res: %r\", res)\\n    return res', 'async def prover_search_credentials(wallet_handle: int,\\n                                    query_json: str) -> (int, int):\\n    \"\"\"\\n    Search for credentials stored in wallet.\\n    Credentials can be filtered by tags created during saving of credential.\\n\\n    Instead of immediately returning of fetched credentials this call returns search_handle that can be used later\\n    to fetch records by small batches (with prover_credentials_search_fetch_records).\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param query_json: wql style filter for credentials searching based on tags.\\n        where wql query: indy-sdk/docs/design/011-wallet-query-language/README.md\\n    :return:\\n        search_handle: Search handle that can be used later to fetch records by small batches\\n            (with prover_credentials_search_fetch_records)\\n        total_count: Total count of records\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_search_credentials: >>> wallet_handle: %r, query_json: %r\",\\n                 wallet_handle,\\n                 query_json)\\n\\n    if not hasattr(prover_search_credentials, \"cb\"):\\n        logger.debug(\"prover_search_credentials: Creating callback\")\\n        prover_search_credentials.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_int32, c_uint))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_query_json = c_char_p(query_json.encode(\\'utf-8\\'))\\n\\n    res = await do_call(\\'indy_prover_search_credentials\\',\\n                        c_wallet_handle,\\n                        c_query_json,\\n                        prover_search_credentials.cb)\\n\\n    logger.debug(\"prover_search_credentials: <<< res: %r\", res)\\n    return res', 'async def prover_fetch_credentials(search_handle: int,\\n                                   count: int) -> str:\\n    \"\"\"\\n    Fetch next credentials for search.\\n\\n    :param search_handle: Search handle (created by prover_open_credentials_search)\\n    :param count: Count of records to fetch\\n    :return: credentials_json: List of credentials:\\n    [{\\n        \"referent\": string, // cred_id in the wallet\\n        \"attrs\": {\"key1\":\"raw_value1\", \"key2\":\"raw_value2\"},\\n        \"schema_id\": string,\\n        \"cred_def_id\": string,\\n        \"rev_reg_id\": Optional<string>,\\n        \"cred_rev_id\": Optional<string>\\n    }]\\n    NOTE: The list of length less than the requested count means credentials search iterator is completed.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_fetch_credentials: >>> search_handle: %r, count: %r\",\\n                 search_handle,\\n                 count)\\n\\n    if not hasattr(prover_fetch_credentials, \"cb\"):\\n        logger.debug(\"prover_fetch_credentials: Creating callback\")\\n        prover_fetch_credentials.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_search_handle = c_int32(search_handle)\\n    c_count = c_uint(count)\\n\\n    credentials_json = await do_call(\\'indy_prover_fetch_credentials\\',\\n                                     c_search_handle,\\n                                     c_count,\\n                                     prover_fetch_credentials.cb)\\n\\n    res = credentials_json.decode()\\n    logger.debug(\"prover_fetch_credentials: <<< res: %r\", res)\\n    return res', 'async def prover_close_credentials_search(search_handle: int) -> None:\\n    \"\"\"\\n    Close credentials search (make search handle invalid)\\n\\n    :param search_handle: Search handle (created by prover_open_credentials_search)\\n    :return: None\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_close_credentials_search: >>> search_handle: %r\",\\n                 search_handle)\\n\\n    if not hasattr(prover_close_credentials_search, \"cb\"):\\n        logger.debug(\"prover_close_credentials_search: Creating callback\")\\n        prover_close_credentials_search.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_search_handle = c_int32(search_handle)\\n\\n    res = await do_call(\\'indy_prover_close_credentials_search\\',\\n                        c_search_handle,\\n                        prover_close_credentials_search.cb)\\n\\n    logger.debug(\"prover_close_credentials_search: <<< res: %r\", res)\\n    return res', 'async def prover_get_credentials_for_proof_req(wallet_handle: int,\\n                                               proof_request_json: str) -> str:\\n    \"\"\"\\n    Gets human readable credentials matching the given proof request.\\n\\n    NOTE: This method is deprecated because immediately returns all fetched credentials.\\n    Use <prover_search_credentials_for_proof_req> to fetch records by small batches.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param proof_request_json: proof request json\\n        {\\n            \"name\": string,\\n            \"version\": string,\\n            \"nonce\": string,\\n            \"requested_attributes\": { // set of requested attributes\\n                 \"<attr_referent>\": <attr_info>, // see below\\n                 ...,\\n            },\\n            \"requested_predicates\": { // set of requested predicates\\n                 \"<predicate_referent>\": <predicate_info>, // see below\\n                 ...,\\n             },\\n            \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\\n                           // If specified prover must proof non-revocation\\n                           // for date in this interval for each attribute\\n                           // (can be overridden on attribute level)\\n        }\\n    where:\\n         attr_referent: Proof-request local identifier of requested attribute\\n         attr_info: Describes requested attribute\\n             {\\n                 \"name\": string, // attribute name, (case insensitive and ignore spaces)\\n                 \"restrictions\": Optional<[<filter_json>]>, // see above\\n                                  // if specified, credential must satisfy to one of the given restriction.\\n                 \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\\n                                // If specified prover must proof non-revocation\\n                                // for date in this interval this attribute\\n                                // (overrides proof level interval)\\n             }\\n         predicate_referent: Proof-request local identifier of requested attribute predicate\\n         predicate_info: Describes requested attribute predicate\\n             {\\n                 \"name\": attribute name, (case insensitive and ignore spaces)\\n                 \"p_type\": predicate type (Currently >= only)\\n                 \"p_value\": predicate value\\n                 \"restrictions\": Optional<[<filter_json>]>, // see above\\n                                 // if specified, credential must satisfy to one of the given restriction.\\n                 \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\\n                                // If specified prover must proof non-revocation\\n                                // for date in this interval this attribute\\n                                // (overrides proof level interval)\\n             }\\n         non_revoc_interval: Defines non-revocation interval\\n             {\\n                 \"from\": Optional<int>, // timestamp of interval beginning\\n                 \"to\": Optional<int>, // timestamp of interval ending\\n             }\\n    :return: json with credentials for the given proof request.\\n             {\\n                 \"requested_attrs\": {\\n                     \"<attr_referent>\": [{ cred_info: <credential_info>, interval: Optional<non_revoc_interval> }],\\n                     ...,\\n                 },\\n                 \"requested_predicates\": {\\n                     \"requested_predicates\": [{ cred_info: <credential_info>, timestamp: Optional<integer> }, { cred_info: <credential_2_info>, timestamp: Optional<integer> }],\\n                     \"requested_predicate_2_referent\": [{ cred_info: <credential_2_info>, timestamp: Optional<integer> }]\\n                 }\\n             }, where credential is\\n             {\\n                 \"referent\": <string>,\\n                 \"attrs\": [{\"attr_name\" : \"attr_raw_value\"}],\\n                 \"schema_id\": string,\\n                 \"cred_def_id\": string,\\n                 \"rev_reg_id\": Optional<int>,\\n                 \"cred_rev_id\": Optional<int>,\\n             }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_get_credentials_for_proof_req: >>> wallet_handle: %r, proof_request_json: %r\",\\n                 wallet_handle,\\n                 proof_request_json)\\n\\n    if not hasattr(prover_get_credentials_for_proof_req, \"cb\"):\\n        logger.debug(\"prover_get_credentials_for_proof_req: Creating callback\")\\n        prover_get_credentials_for_proof_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_proof_request_json = c_char_p(proof_request_json.encode(\\'utf-8\\'))\\n\\n    credentials_json = await do_call(\\'indy_prover_get_credentials_for_proof_req\\',\\n                                     c_wallet_handle,\\n                                     c_proof_request_json,\\n                                     prover_get_credentials_for_proof_req.cb)\\n\\n    res = credentials_json.decode()\\n    logger.debug(\"prover_get_credentials_for_proof_req: <<< res: %r\", res)\\n    return res', 'async def prover_search_credentials_for_proof_req(wallet_handle: int,\\n                                                  proof_request_json: str,\\n                                                  extra_query_json: Optional[str]) -> int:\\n    \"\"\"\\n    Search for credentials matching the given proof request.\\n\\n    Instead of immediately returning of fetched credentials this call returns search_handle that can be used later\\n    to fetch records by small batches (with prover_fetch_credentials_for_proof_req).\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param proof_request_json: proof request json\\n        {\\n            \"name\": string,\\n            \"version\": string,\\n            \"nonce\": string,\\n            \"requested_attributes\": { // set of requested attributes\\n                 \"<attr_referent>\": <attr_info>, // see below\\n                 ...,\\n            },\\n            \"requested_predicates\": { // set of requested predicates\\n                 \"<predicate_referent>\": <predicate_info>, // see below\\n                 ...,\\n             },\\n            \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\\n                           // If specified prover must proof non-revocation\\n                           // for date in this interval for each attribute\\n                           // (can be overridden on attribute level)\\n        }\\n    :param extra_query_json:(Optional) List of extra queries that will be applied to correspondent attribute/predicate:\\n        {\\n            \"<attr_referent>\": <wql query>,\\n            \"<predicate_referent>\": <wql query>,\\n        }\\n        where wql query: indy-sdk/docs/design/011-wallet-query-language/README.md\\n    :return: search_handle: Search handle that can be used later to fetch records by small batches (with prover_fetch_credentials_for_proof_req)\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_search_credentials_for_proof_req: >>> wallet_handle: %r, proof_request_json: %r, \"\\n                 \"extra_query_json: %r\",\\n                 wallet_handle,\\n                 proof_request_json,\\n                 extra_query_json)\\n\\n    if not hasattr(prover_search_credentials_for_proof_req, \"cb\"):\\n        logger.debug(\"prover_search_credentials_for_proof_req: Creating callback\")\\n        prover_search_credentials_for_proof_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_int32))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_proof_request_json = c_char_p(proof_request_json.encode(\\'utf-8\\'))\\n    c_extra_query_json = c_char_p(extra_query_json.encode(\\'utf-8\\')) if extra_query_json is not None else None\\n\\n    res = await do_call(\\'indy_prover_search_credentials_for_proof_req\\',\\n                        c_wallet_handle,\\n                        c_proof_request_json,\\n                        c_extra_query_json,\\n                        prover_search_credentials_for_proof_req.cb)\\n\\n    logger.debug(\"prover_search_credentials_for_proof_req: <<< res: %r\", res)\\n    return res', 'async def prover_create_proof(wallet_handle: int,\\n                              proof_req_json: str,\\n                              requested_credentials_json: str,\\n                              master_secret_name: str,\\n                              schemas_json: str,\\n                              credential_defs_json: str,\\n                              rev_states_json: str) -> str:\\n    \"\"\"\\n    Creates a proof according to the given proof request\\n    Either a corresponding credential with optionally revealed attributes or self-attested attribute must be provided\\n    for each requested attribute (see indy_prover_get_credentials_for_pool_req).\\n    A proof request may request multiple credentials from different schemas and different issuers.\\n    All required schemas, public keys and revocation registries must be provided.\\n    The proof request also contains nonce.\\n    The proof contains either proof or self-attested attribute value for each requested attribute.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param proof_req_json: proof request json\\n        {\\n            \"name\": string,\\n            \"version\": string,\\n            \"nonce\": string,\\n            \"requested_attributes\": { // set of requested attributes\\n                 \"<attr_referent>\": <attr_info>, // see below\\n                 ...,\\n            },\\n            \"requested_predicates\": { // set of requested predicates\\n                 \"<predicate_referent>\": <predicate_info>, // see below\\n                 ...,\\n             },\\n            \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\\n                           // If specified prover must proof non-revocation\\n                           // for date in this interval for each attribute\\n                           // (can be overridden on attribute level)\\n        }\\n    :param requested_credentials_json: either a credential or self-attested attribute for each requested attribute\\n        {\\n            \"self_attested_attributes\": {\\n                \"self_attested_attribute_referent\": string\\n            },\\n            \"requested_attributes\": {\\n                \"requested_attribute_referent_1\": {\"cred_id\": string, \"timestamp\": Optional<number>, revealed: <bool> }},\\n                \"requested_attribute_referent_2\": {\"cred_id\": string, \"timestamp\": Optional<number>, revealed: <bool> }}\\n            },\\n            \"requested_predicates\": {\\n                \"requested_predicates_referent_1\": {\"cred_id\": string, \"timestamp\": Optional<number> }},\\n            }\\n        }\\n    :param master_secret_name: the id of the master secret stored in the wallet\\n    :param schemas_json: all schemas json participating in the proof request\\n          {\\n              <schema1_id>: <schema1_json>,\\n              <schema2_id>: <schema2_json>,\\n              <schema3_id>: <schema3_json>,\\n          }\\n    :param credential_defs_json: all credential definitions json participating in the proof request\\n          {\\n              \"cred_def1_id\": <credential_def1_json>,\\n              \"cred_def2_id\": <credential_def2_json>,\\n              \"cred_def3_id\": <credential_def3_json>,\\n          }\\n    :param rev_states_json: all revocation states json participating in the proof request\\n          {\\n              \"rev_reg_def1_id\": {\\n                  \"timestamp1\": <rev_state1>,\\n                  \"timestamp2\": <rev_state2>,\\n              },\\n              \"rev_reg_def2_id\": {\\n                  \"timestamp3\": <rev_state3>\\n              },\\n              \"rev_reg_def3_id\": {\\n                  \"timestamp4\": <rev_state4>\\n              },\\n          }\\n    where\\n     wql query: indy-sdk/docs/design/011-wallet-query-language/README.md\\n     attr_referent: Proof-request local identifier of requested attribute\\n     attr_info: Describes requested attribute\\n         {\\n             \"name\": string, // attribute name, (case insensitive and ignore spaces)\\n             \"restrictions\": Optional<[<wql query>]>,\\n                              // if specified, credential must satisfy to one of the given restriction.\\n             \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\\n                            // If specified prover must proof non-revocation\\n                            // for date in this interval this attribute\\n                            // (overrides proof level interval)\\n         }\\n     predicate_referent: Proof-request local identifier of requested attribute predicate\\n     predicate_info: Describes requested attribute predicate\\n         {\\n             \"name\": attribute name, (case insensitive and ignore spaces)\\n             \"p_type\": predicate type (Currently >= only)\\n             \"p_value\": predicate value\\n             \"restrictions\": Optional<[<wql query>]>,\\n                             // if specified, credential must satisfy to one of the given restriction.\\n             \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\\n                            // If specified prover must proof non-revocation\\n                            // for date in this interval this attribute\\n                            // (overrides proof level interval)\\n         }\\n     non_revoc_interval: Defines non-revocation interval\\n         {\\n             \"from\": Optional<int>, // timestamp of interval beginning\\n             \"to\": Optional<int>, // timestamp of interval ending\\n         }\\n\\n    :return: Proof json\\n      For each requested attribute either a proof (with optionally revealed attribute value) or\\n      self-attested attribute value is provided.\\n      Each proof is associated with a credential and corresponding schema_id, cred_def_id, rev_reg_id and timestamp.\\n      There is also aggregated proof part common for all credential proofs.\\n          {\\n              \"requested_proof\": {\\n                  \"revealed_attrs\": {\\n                      \"requested_attr1_id\": {sub_proof_index: number, raw: string, encoded: string},\\n                      \"requested_attr4_id\": {sub_proof_index: number: string, encoded: string},\\n                  },\\n                  \"unrevealed_attrs\": {\\n                      \"requested_attr3_id\": {sub_proof_index: number}\\n                  },\\n                  \"self_attested_attrs\": {\\n                      \"requested_attr2_id\": self_attested_value,\\n                  },\\n                  \"requested_predicates\": {\\n                      \"requested_predicate_1_referent\": {sub_proof_index: int},\\n                      \"requested_predicate_2_referent\": {sub_proof_index: int},\\n                  }\\n              }\\n              \"proof\": {\\n                  \"proofs\": [ <credential_proof>, <credential_proof>, <credential_proof> ],\\n                  \"aggregated_proof\": <aggregated_proof>\\n              }\\n              \"identifiers\": [{schema_id, cred_def_id, Optional<rev_reg_id>, Optional<timestamp>}]\\n          }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"prover_create_proof: >>> wallet_handle: %r, proof_req_json: %r, requested_credentials_json: %r, \"\\n                 \"schemas_json: %r, master_secret_name: %r, credential_defs_json: %r, rev_infos_json: %r\",\\n                 wallet_handle,\\n                 proof_req_json,\\n                 requested_credentials_json,\\n                 schemas_json,\\n                 master_secret_name,\\n                 credential_defs_json,\\n                 rev_states_json)\\n\\n    if not hasattr(prover_create_proof, \"cb\"):\\n        logger.debug(\"prover_create_proof: Creating callback\")\\n        prover_create_proof.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_proof_req_json = c_char_p(proof_req_json.encode(\\'utf-8\\'))\\n    c_requested_credentials_json = c_char_p(requested_credentials_json.encode(\\'utf-8\\'))\\n    c_schemas_json = c_char_p(schemas_json.encode(\\'utf-8\\'))\\n    c_master_secret_name = c_char_p(master_secret_name.encode(\\'utf-8\\'))\\n    c_credential_defs_json = c_char_p(credential_defs_json.encode(\\'utf-8\\'))\\n    c_rev_infos_json = c_char_p(rev_states_json.encode(\\'utf-8\\'))\\n\\n    proof_json = await do_call(\\'indy_prover_create_proof\\',\\n                               c_wallet_handle,\\n                               c_proof_req_json,\\n                               c_requested_credentials_json,\\n                               c_master_secret_name,\\n                               c_schemas_json,\\n                               c_credential_defs_json,\\n                               c_rev_infos_json,\\n                               prover_create_proof.cb)\\n\\n    res = proof_json.decode()\\n    logger.debug(\"prover_create_proof: <<< res: %r\", res)\\n    return res', 'async def verifier_verify_proof(proof_request_json: str,\\n                                proof_json: str,\\n                                schemas_json: str,\\n                                credential_defs_json: str,\\n                                rev_reg_defs_json: str,\\n                                rev_regs_json: str) -> bool:\\n    \"\"\"\\n    Verifies a proof (of multiple credential).\\n    All required schemas, public keys and revocation registries must be provided.\\n\\n    :param proof_request_json: \\n         {\\n             \"name\": string,\\n             \"version\": string,\\n             \"nonce\": string,\\n             \"requested_attributes\": { // set of requested attributes\\n                  \"<attr_referent>\": <attr_info>, // see below\\n                  ...,\\n             },\\n             \"requested_predicates\": { // set of requested predicates\\n                  \"<predicate_referent>\": <predicate_info>, // see below\\n                  ...,\\n              },\\n             \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\\n                            // If specified prover must proof non-revocation\\n                            // for date in this interval for each attribute\\n                            // (can be overridden on attribute level)\\n         }\\n    :param proof_json: created for request proof json\\n         {\\n             \"requested_proof\": {\\n                 \"revealed_attrs\": {\\n                     \"requested_attr1_id\": {sub_proof_index: number, raw: string, encoded: string},\\n                     \"requested_attr4_id\": {sub_proof_index: number: string, encoded: string},\\n                 },\\n                 \"unrevealed_attrs\": {\\n                     \"requested_attr3_id\": {sub_proof_index: number}\\n                 },\\n                 \"self_attested_attrs\": {\\n                     \"requested_attr2_id\": self_attested_value,\\n                 },\\n                 \"requested_predicates\": {\\n                     \"requested_predicate_1_referent\": {sub_proof_index: int},\\n                     \"requested_predicate_2_referent\": {sub_proof_index: int},\\n                 }\\n             }\\n             \"proof\": {\\n                 \"proofs\": [ <credential_proof>, <credential_proof>, <credential_proof> ],\\n                 \"aggregated_proof\": <aggregated_proof>\\n             }\\n             \"identifiers\": [{schema_id, cred_def_id, Optional<rev_reg_id>, Optional<timestamp>}]\\n         }\\n    :param schemas_json: all schema jsons participating in the proof\\n         {\\n             <schema1_id>: <schema1_json>,\\n             <schema2_id>: <schema2_json>,\\n             <schema3_id>: <schema3_json>,\\n         }\\n    :param credential_defs_json: all credential definitions json participating in the proof\\n         {\\n             \"cred_def1_id\": <credential_def1_json>,\\n             \"cred_def2_id\": <credential_def2_json>,\\n             \"cred_def3_id\": <credential_def3_json>,\\n         }\\n    :param rev_reg_defs_json: all revocation registry definitions json participating in the proof\\n         {\\n             \"rev_reg_def1_id\": <rev_reg_def1_json>,\\n             \"rev_reg_def2_id\": <rev_reg_def2_json>,\\n             \"rev_reg_def3_id\": <rev_reg_def3_json>,\\n         }\\n    :param rev_regs_json: all revocation registries json participating in the proof\\n         {\\n             \"rev_reg_def1_id\": {\\n                 \"timestamp1\": <rev_reg1>,\\n                 \"timestamp2\": <rev_reg2>,\\n             },\\n             \"rev_reg_def2_id\": {\\n                 \"timestamp3\": <rev_reg3>\\n             },\\n             \"rev_reg_def3_id\": {\\n                 \"timestamp4\": <rev_reg4>\\n             },\\n         }\\n    :return: valid: true - if signature is valid, false - otherwise\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"verifier_verify_proof: >>> proof_request_json: %r, proof_json: %r, schemas_json: %r, \"\\n                 \"credential_defs_jsons: %r, rev_reg_defs_json: %r, rev_regs_json: %r\",\\n                 proof_request_json,\\n                 proof_json,\\n                 schemas_json,\\n                 credential_defs_json,\\n                 rev_reg_defs_json,\\n                 rev_regs_json)\\n\\n    if not hasattr(verifier_verify_proof, \"cb\"):\\n        logger.debug(\"verifier_verify_proof: Creating callback\")\\n        verifier_verify_proof.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_bool))\\n\\n    c_proof_request_json = c_char_p(proof_request_json.encode(\\'utf-8\\'))\\n    c_proof_json = c_char_p(proof_json.encode(\\'utf-8\\'))\\n    c_schemas_json = c_char_p(schemas_json.encode(\\'utf-8\\'))\\n    c_credential_defs_json = c_char_p(credential_defs_json.encode(\\'utf-8\\'))\\n    c_rev_reg_defs_json = c_char_p(rev_reg_defs_json.encode(\\'utf-8\\'))\\n    c_rev_regs_json = c_char_p(rev_regs_json.encode(\\'utf-8\\'))\\n\\n    res = await do_call(\\'indy_verifier_verify_proof\\',\\n                        c_proof_request_json,\\n                        c_proof_json,\\n                        c_schemas_json,\\n                        c_credential_defs_json,\\n                        c_rev_reg_defs_json,\\n                        c_rev_regs_json,\\n                        verifier_verify_proof.cb)\\n\\n    logger.debug(\"verifier_verify_proof: <<< res: %r\", res)\\n    return res', 'async def create_revocation_state(blob_storage_reader_handle: int,\\n                                  rev_reg_def_json: str,\\n                                  rev_reg_delta_json: str,\\n                                  timestamp: int,\\n                                  cred_rev_id: str) -> str:\\n    \"\"\"\\n    Create revocation state for a credential in the particular time moment.\\n\\n    :param blob_storage_reader_handle: configuration of blob storage reader handle that will allow to read revocation tails\\n    :param rev_reg_def_json: revocation registry definition json\\n    :param rev_reg_delta_json: revocation registry definition delta json\\n    :param timestamp: time represented as a total number of seconds from Unix Epoch\\n    :param cred_rev_id: user credential revocation id in revocation registry\\n    :return: revocation state json {\\n         \"rev_reg\": <revocation registry>,\\n         \"witness\": <witness>,\\n         \"timestamp\" : integer\\n    }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"create_revocation_info: >>> blob_storage_reader_handle: %r, rev_reg_def_json: %r,\"\\n                 \" rev_reg_delta_json: %r, timestamp: %r, cred_rev_id: %r\",\\n                 blob_storage_reader_handle,\\n                 rev_reg_def_json,\\n                 rev_reg_delta_json,\\n                 timestamp,\\n                 cred_rev_id)\\n\\n    if not hasattr(create_revocation_state, \"cb\"):\\n        logger.debug(\"create_revocation_state: Creating callback\")\\n        create_revocation_state.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_blob_storage_reader_handle = c_int32(blob_storage_reader_handle)\\n    c_rev_reg_def_json = c_char_p(rev_reg_def_json.encode(\\'utf-8\\'))\\n    c_rev_reg_delta_json = c_char_p(rev_reg_delta_json.encode(\\'utf-8\\'))\\n    c_timestamp = c_uint64(timestamp)\\n    c_cred_rev_id = c_char_p(cred_rev_id.encode(\\'utf-8\\'))\\n\\n    rev_state_json = await do_call(\\'indy_create_revocation_state\\',\\n                                   c_blob_storage_reader_handle,\\n                                   c_rev_reg_def_json,\\n                                   c_rev_reg_delta_json,\\n                                   c_timestamp,\\n                                   c_cred_rev_id,\\n                                   create_revocation_state.cb)\\n\\n    res = rev_state_json.decode()\\n    logger.debug(\"create_revocation_state: <<< res: %r\", res)\\n    return res', 'async def sign_and_submit_request(pool_handle: int,\\n                                  wallet_handle: int,\\n                                  submitter_did: str,\\n                                  request_json: str) -> str:\\n    \"\"\"\\n    Signs and submits request message to validator pool.\\n\\n    Adds submitter information to passed request json, signs it with submitter\\n    sign key (see wallet_sign), and sends signed request message\\n    to validator pool (see write_request).\\n\\n    :param pool_handle: pool handle (created by open_pool_ledger).\\n    :param wallet_handle: wallet handle (created by open_wallet).\\n    :param submitter_did: Id of Identity stored in secured Wallet.\\n    :param request_json: Request data json.\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"sign_and_submit_request: >>> pool_handle: %r, wallet_handle: %r, submitter_did: %r, request_json: %r\",\\n                 pool_handle,\\n                 wallet_handle,\\n                 submitter_did,\\n                 request_json)\\n\\n    if not hasattr(sign_and_submit_request, \"cb\"):\\n        logger.debug(\"sign_and_submit_request: Creating callback\")\\n        sign_and_submit_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_pool_handle = c_int32(pool_handle)\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\'))\\n    c_request_json = c_char_p(request_json.encode(\\'utf-8\\'))\\n\\n    request_result = await do_call(\\'indy_sign_and_submit_request\\',\\n                                   c_pool_handle,\\n                                   c_wallet_handle,\\n                                   c_submitter_did,\\n                                   c_request_json,\\n                                   sign_and_submit_request.cb)\\n\\n    res = request_result.decode()\\n    logger.debug(\"sign_and_submit_request: <<< res: %r\", res)\\n    return res', 'async def submit_request(pool_handle: int,\\n                         request_json: str) -> str:\\n    \"\"\"\\n    Publishes request message to validator pool (no signing, unlike sign_and_submit_request).\\n    The request is sent to the validator pool as is. It\\'s assumed that it\\'s already prepared.\\n\\n    :param pool_handle: pool handle (created by open_pool_ledger).\\n    :param request_json: Request data json.\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"submit_request: >>> pool_handle: %r, request_json: %r\",\\n                 pool_handle,\\n                 request_json)\\n\\n    if not hasattr(submit_request, \"cb\"):\\n        logger.debug(\"submit_request: Creating callback\")\\n        submit_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_pool_handle = c_int32(pool_handle)\\n    c_request_json = c_char_p(request_json.encode(\\'utf-8\\'))\\n\\n    request_result = await do_call(\\'indy_submit_request\\',\\n                                   c_pool_handle,\\n                                   c_request_json,\\n                                   submit_request.cb)\\n\\n    res = request_result.decode()\\n    logger.debug(\"submit_request: <<< res: %r\", res)\\n    return res', 'async def submit_action(pool_handle: int,\\n                        request_json: str,\\n                        nodes: Optional[str],\\n                        timeout: Optional[int]) -> str:\\n    \"\"\"\\n    Send action to particular nodes of validator pool.\\n    \\n    The list of requests can be send:\\n        POOL_RESTART\\n        GET_VALIDATOR_INFO\\n   \\n    The request is sent to the nodes as is. It\\'s assumed that it\\'s already prepared.\\n\\n    :param pool_handle: pool handle (created by open_pool_ledger).\\n    :param request_json: Request data json.\\n    :param nodes: (Optional) List of node names to send the request.\\n           [\"Node1\", \"Node2\",....\"NodeN\"]\\n    :param timeout: (Optional) Time to wait respond from nodes (override the default timeout) (in sec).\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"submit_action: >>> pool_handle: %r, request_json: %r, nodes: %r, timeout: %r\",\\n                 pool_handle,\\n                 request_json,\\n                 nodes,\\n                 timeout)\\n\\n    if not hasattr(submit_action, \"cb\"):\\n        logger.debug(\"submit_action: Creating callback\")\\n        submit_action.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_pool_handle = c_int32(pool_handle)\\n    c_request_json = c_char_p(request_json.encode(\\'utf-8\\'))\\n    c_nodes = c_char_p(nodes.encode(\\'utf-8\\')) if nodes is not None else None\\n    c_timeout = c_int32(timeout) if timeout is not None else None\\n\\n    request_result = await do_call(\\'indy_submit_action\\',\\n                                   c_pool_handle,\\n                                   c_request_json,\\n                                   c_nodes,\\n                                   c_timeout,\\n                                   submit_action.cb)\\n\\n    res = request_result.decode()\\n    logger.debug(\"submit_action: <<< res: %r\", res)\\n    return res', 'async def build_get_ddo_request(submitter_did: Optional[str],\\n                                target_did: str) -> str:\\n    \"\"\"\\n    Builds a request to get a DDO.\\n\\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\\n    :param target_did: Id of Identity stored in secured Wallet.\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_get_ddo_request: >>> submitter_did: %r, target_did: %r\",\\n                 submitter_did,\\n                 target_did)\\n\\n    if not hasattr(build_get_ddo_request, \"cb\"):\\n        logger.debug(\"build_get_ddo_request: Creating callback\")\\n        build_get_ddo_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_target_did = c_char_p(target_did.encode(\\'utf-8\\'))\\n\\n    request_json = await do_call(\\'indy_build_get_ddo_request\\',\\n                                 c_submitter_did,\\n                                 c_target_did,\\n                                 build_get_ddo_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_get_ddo_request: <<< res: %r\", res)\\n    return res', 'async def build_nym_request(submitter_did: str,\\n                            target_did: str,\\n                            ver_key: Optional[str],\\n                            alias: Optional[str],\\n                            role: Optional[str]) -> str:\\n    \"\"\"\\n    Builds a NYM request.\\n\\n    :param submitter_did: DID of the submitter stored in secured Wallet.\\n    :param target_did: Target DID as base58-encoded string for 16 or 32 bit DID value.\\n    :param ver_key: Target identity verification key as base58-encoded string.\\n    :param alias: NYM\\'s alias.\\n    :param role: Role of a user NYM record:\\n                             null (common USER)\\n                             TRUSTEE\\n                             STEWARD\\n                             TRUST_ANCHOR\\n                             NETWORK_MONITOR\\n                             empty string to reset role\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_nym_request: >>> submitter_did: %r, target_did: %r, ver_key: %r, alias: %r, role: %r\",\\n                 submitter_did,\\n                 target_did,\\n                 ver_key,\\n                 alias,\\n                 role)\\n\\n    if not hasattr(build_nym_request, \"cb\"):\\n        logger.debug(\"build_nym_request: Creating callback\")\\n        build_nym_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\'))\\n    c_target_did = c_char_p(target_did.encode(\\'utf-8\\'))\\n    c_ver_key = c_char_p(ver_key.encode(\\'utf-8\\')) if ver_key is not None else None\\n    c_alias = c_char_p(alias.encode(\\'utf-8\\')) if alias is not None else None\\n    c_role = c_char_p(role.encode(\\'utf-8\\')) if role is not None else None\\n\\n    request_json = await do_call(\\'indy_build_nym_request\\',\\n                                 c_submitter_did,\\n                                 c_target_did,\\n                                 c_ver_key,\\n                                 c_alias,\\n                                 c_role,\\n                                 build_nym_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_nym_request: <<< res: %r\", res)\\n    return res', 'async def build_attrib_request(submitter_did: str,\\n                               target_did: str,\\n                               xhash: Optional[str],\\n                               raw: Optional[str],\\n                               enc: Optional[str]) -> str:\\n    \"\"\"\\n    Builds an ATTRIB request. Request to add attribute to a NYM record.\\n\\n    :param submitter_did: DID of the submitter stored in secured Wallet.\\n    :param target_did: Target DID as base58-encoded string for 16 or 32 bit DID value.\\n    :param xhash: (Optional) Hash of attribute data.\\n    :param raw: (Optional) Json, where key is attribute name and value is attribute value.\\n    :param enc: (Optional) Encrypted value attribute data.\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_attrib_request: >>> submitter_did: %r, target_did: %r, hash: %r, raw: %r, enc: %r\",\\n                 submitter_did,\\n                 target_did,\\n                 xhash,\\n                 raw,\\n                 enc)\\n\\n    if not hasattr(build_attrib_request, \"cb\"):\\n        logger.debug(\"build_attrib_request: Creating callback\")\\n        build_attrib_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\'))\\n    c_target_did = c_char_p(target_did.encode(\\'utf-8\\'))\\n    c_hash = c_char_p(xhash.encode(\\'utf-8\\')) if xhash is not None else None\\n    c_raw = c_char_p(raw.encode(\\'utf-8\\')) if raw is not None else None\\n    c_enc = c_char_p(enc.encode(\\'utf-8\\')) if enc is not None else None\\n\\n    request_json = await do_call(\\'indy_build_attrib_request\\',\\n                                 c_submitter_did,\\n                                 c_target_did,\\n                                 c_hash,\\n                                 c_raw,\\n                                 c_enc,\\n                                 build_attrib_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_attrib_request: <<< res: %r\", res)\\n    return res', 'async def parse_get_schema_response(get_schema_response: str) -> (str, str):\\n    \"\"\"\\n    Parse a GET_SCHEMA response to get Schema in the format compatible with Anoncreds API\\n\\n    :param get_schema_response: response of GET_SCHEMA request.\\n    :return: Schema Id and Schema json.\\n     {\\n         id: identifier of schema\\n         attrNames: array of attribute name strings\\n         name: Schema\\'s name string\\n         version: Schema\\'s version string\\n         ver: Version of the Schema json\\n     }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"parse_get_schema_response: >>> get_schema_response: %r\", get_schema_response)\\n\\n    if not hasattr(parse_get_schema_response, \"cb\"):\\n        logger.debug(\"parse_get_schema_response: Creating callback\")\\n        parse_get_schema_response.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_get_schema_response = c_char_p(get_schema_response.encode(\\'utf-8\\'))\\n\\n    (schema_id, schema_json) = await do_call(\\'indy_parse_get_schema_response\\',\\n                                             c_get_schema_response,\\n                                             parse_get_schema_response.cb)\\n\\n    res = (schema_id.decode(), schema_json.decode())\\n    logger.debug(\"parse_get_schema_response: <<< res: %r\", res)\\n    return res', 'async def build_get_cred_def_request(submitter_did: Optional[str],\\n                                     id_: str) -> str:\\n    \"\"\"\\n   Builds a GET_CRED_DEF request. Request to get a credential definition (in particular, public key),\\n   that Issuer creates for a particular Credential Schema.\\n\\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\\n    :param id_: Credential Definition Id in ledger.\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_get_cred_def_request: >>> submitter_did: %r, id: %r\",\\n                 submitter_did,\\n                 id_)\\n\\n    if not hasattr(build_get_cred_def_request, \"cb\"):\\n        logger.debug(\"build_get_cred_def_request: Creating callback\")\\n        build_get_cred_def_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_id = c_char_p(id_.encode(\\'utf-8\\'))\\n\\n    request_json = await do_call(\\'indy_build_get_cred_def_request\\',\\n                                 c_submitter_did,\\n                                 c_id,\\n                                 build_get_cred_def_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_get_cred_def_request: <<< res: %r\", res)\\n    return res', 'async def parse_get_cred_def_response(get_cred_def_response: str) -> (str, str):\\n    \"\"\"\\n    Parse a GET_CRED_DEF response to get Credential Definition in the format compatible with Anoncreds API.\\n\\n    :param get_cred_def_response: response of GET_CRED_DEF request.\\n    :return: Credential Definition Id and Credential Definition json.\\n      {\\n          id: string - identifier of credential definition\\n          schemaId: string - identifier of stored in ledger schema\\n          type: string - type of the credential definition. CL is the only supported type now.\\n          tag: string - allows to distinct between credential definitions for the same issuer and schema\\n          value: Dictionary with Credential Definition\\'s data: {\\n              primary: primary credential public key,\\n              Optional<revocation>: revocation credential public key\\n          },\\n          ver: Version of the Credential Definition json\\n      }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"parse_get_cred_def_response: >>> get_cred_def_response: %r\", get_cred_def_response)\\n\\n    if not hasattr(parse_get_cred_def_response, \"cb\"):\\n        logger.debug(\"parse_get_cred_def_response: Creating callback\")\\n        parse_get_cred_def_response.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_get_cred_def_response = c_char_p(get_cred_def_response.encode(\\'utf-8\\'))\\n\\n    (cred_def_id, cred_def_json) = await do_call(\\'indy_parse_get_cred_def_response\\',\\n                                                 c_get_cred_def_response,\\n                                                 parse_get_cred_def_response.cb)\\n\\n    res = (cred_def_id.decode(), cred_def_json.decode())\\n    logger.debug(\"parse_get_cred_def_response: <<< res: %r\", res)\\n    return res', 'async def build_get_validator_info_request(submitter_did: str) -> str:\\n    \"\"\"\\n    Builds a GET_VALIDATOR_INFO request.\\n    :param submitter_did: Id of Identity stored in secured Wallet.\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_get_validator_info_request: >>> submitter_did: %r\", submitter_did)\\n\\n    if not hasattr(build_get_validator_info_request, \"cb\"):\\n        logger.debug(\"build_get_validator_info_request: Creating callback\")\\n        build_get_validator_info_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\'))\\n\\n    request_json = await do_call(\\'indy_build_get_validator_info_request\\',\\n                                 c_submitter_did,\\n                                 build_get_validator_info_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_get_validator_info_request: <<< res: %r\", res)\\n    return res', 'async def build_get_txn_request(submitter_did: Optional[str],\\n                                ledger_type: Optional[str],\\n                                seq_no: int) -> str:\\n    \"\"\"\\n    Builds a GET_TXN request. Request to get any transaction by its seq_no.\\n\\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\\n    :param ledger_type: (Optional) type of the ledger the requested transaction belongs to:\\n        DOMAIN - used default,\\n        POOL,\\n        CONFIG\\n        any number\\n    :param seq_no: requested transaction sequence number as it\\'s stored on Ledger.\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_get_txn_request: >>> submitter_did: %r, ledger_type: %r, seq_no: %r\",\\n                 submitter_did,\\n                 ledger_type,\\n                 seq_no)\\n\\n    if not hasattr(build_get_txn_request, \"cb\"):\\n        logger.debug(\"build_get_txn_request: Creating callback\")\\n        build_get_txn_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_ledger_type = c_char_p(ledger_type.encode(\\'utf-8\\')) if ledger_type is not None else None\\n    c_seq_no = c_int32(seq_no)\\n\\n    request_json = await do_call(\\'indy_build_get_txn_request\\',\\n                                 c_submitter_did,\\n                                 c_ledger_type,\\n                                 c_seq_no,\\n                                 build_get_txn_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_get_txn_request: <<< res: %r\", res)\\n    return res', 'async def build_pool_config_request(submitter_did: str,\\n                                    writes: bool,\\n                                    force: bool) -> str:\\n    \"\"\"\\n    Builds a POOL_CONFIG request. Request to change Pool\\'s configuration.\\n\\n    :param submitter_did: DID of the submitter stored in secured Wallet.\\n    :param writes: Whether any write requests can be processed by the pool\\n                   (if false, then pool goes to read-only state). True by default.\\n    :param force: Whether we should apply transaction (for example, move pool to read-only state)\\n                  without waiting for consensus of this transaction\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_pool_config_request: >>> submitter_did: %r, writes: %r, force: %r\",\\n                 submitter_did,\\n                 writes,\\n                 force)\\n\\n    if not hasattr(build_pool_config_request, \"cb\"):\\n        logger.debug(\"build_pool_config_request: Creating callback\")\\n        build_pool_config_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\'))\\n    c_writes = c_bool(writes)\\n    c_force = c_bool(force)\\n\\n    request_json = await do_call(\\'indy_build_pool_config_request\\',\\n                                 c_submitter_did,\\n                                 c_writes,\\n                                 c_force,\\n                                 build_pool_config_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_pool_config_request: <<< res: %r\", res)\\n    return res', 'async def build_pool_restart_request(submitter_did: str, action: str, datetime: str) -> str:\\n    \"\"\"\\n    Builds a POOL_RESTART request\\n\\n    :param submitter_did: Id of Identity that sender transaction\\n    :param action       : Action that pool has to do after received transaction.\\n                          Can be \"start\" or \"cancel\"\\n    :param datetime           : Time when pool must be restarted.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_pool_restart_request: >>> submitter_did: %r, action: %r, datetime: %r\")\\n\\n    if not hasattr(build_pool_restart_request, \"cb\"):\\n        logger.debug(\"build_pool_restart_request: Creating callback\")\\n        build_pool_restart_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\'))\\n    c_action = c_char_p(action.encode(\\'utf-8\\'))\\n    c_datetime = c_char_p(datetime.encode(\\'utf-8\\')) if datetime else None\\n\\n    request_json = await do_call(\\'indy_build_pool_restart_request\\',\\n                                 c_submitter_did,\\n                                 c_action,\\n                                 c_datetime,\\n                                 build_pool_restart_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_pool_upgrade_request: <<< res: %r\", res)\\n    return res', 'async def build_pool_upgrade_request(submitter_did: str,\\n                                     name: str,\\n                                     version: str,\\n                                     action: str,\\n                                     _sha256: str,\\n                                     _timeout: Optional[int],\\n                                     schedule: Optional[str],\\n                                     justification: Optional[str],\\n                                     reinstall: bool,\\n                                     force: bool,\\n                                     package: Optional[str]) -> str:\\n    \"\"\"\\n    Builds a POOL_UPGRADE request. Request to upgrade the Pool (sent by Trustee).\\n    It upgrades the specified Nodes (either all nodes in the Pool, or some specific ones).\\n\\n    :param submitter_did: DID of the submitter stored in secured Wallet.\\n    :param name: Human-readable name for the upgrade.\\n    :param version: The version of indy-node package we perform upgrade to.\\n                    Must be greater than existing one (or equal if reinstall flag is True).\\n    :param action: Either start or cancel.\\n    :param _sha256: sha256 hash of the package.\\n    :param _timeout: (Optional) Limits upgrade time on each Node.\\n    :param schedule: (Optional) Schedule of when to perform upgrade on each node. Map Node DIDs to upgrade time.\\n    :param justification: (Optional) justification string for this particular Upgrade.\\n    :param reinstall: Whether it\\'s allowed to re-install the same version. False by default.\\n    :param force: Whether we should apply transaction (schedule Upgrade) without waiting\\n                  for consensus of this transaction.\\n    :param package: (Optional) Package to be upgraded.\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_pool_upgrade_request: >>> submitter_did: %r, name: %r, version: %r, action: %r, _sha256: %r, \"\\n                 \"timeout: %r, schedule: %r, justification: %r, reinstall: %r, force: %r, package: %r\",\\n                 submitter_did, name, version, action, _sha256, _timeout, schedule, justification, reinstall, force,\\n                 package)\\n\\n    if not hasattr(build_pool_upgrade_request, \"cb\"):\\n        logger.debug(\"build_pool_upgrade_request: Creating callback\")\\n        build_pool_upgrade_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\'))\\n    c_name = c_char_p(name.encode(\\'utf-8\\'))\\n    c_version = c_char_p(version.encode(\\'utf-8\\'))\\n    c_action = c_char_p(action.encode(\\'utf-8\\'))\\n    c_sha256 = c_char_p(_sha256.encode(\\'utf-8\\'))\\n    c_timeout = c_int32(_timeout) if _timeout else c_int32(-1)\\n    c_schedule = c_char_p(schedule.encode(\\'utf-8\\')) if schedule is not None else None\\n    c_justification = c_char_p(justification.encode(\\'utf-8\\')) if justification is not None else None\\n    c_reinstall = c_bool(reinstall)\\n    c_force = c_bool(force)\\n    c_package = c_char_p(package.encode(\\'utf-8\\')) if package is not None else None\\n\\n    request_json = await do_call(\\'indy_build_pool_upgrade_request\\',\\n                                 c_submitter_did,\\n                                 c_name,\\n                                 c_version,\\n                                 c_action,\\n                                 c_sha256,\\n                                 c_timeout,\\n                                 c_schedule,\\n                                 c_justification,\\n                                 c_reinstall,\\n                                 c_force,\\n                                 c_package,\\n                                 build_pool_upgrade_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_pool_upgrade_request: <<< res: %r\", res)\\n    return res', 'async def build_get_revoc_reg_def_request(submitter_did: Optional[str],\\n                                          rev_reg_def_id: str) -> str:\\n    \"\"\"\\n    Builds a GET_REVOC_REG_DEF request. Request to get a revocation registry definition,\\n    that Issuer creates for a particular Credential Definition.\\n\\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\\n    :param rev_reg_def_id: ID of Revocation Registry Definition in ledger.\\n\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_get_revoc_reg_def_request: >>> submitter_did: %r, rev_reg_def_id: %r\", submitter_did,\\n                 rev_reg_def_id)\\n\\n    if not hasattr(build_get_revoc_reg_def_request, \"cb\"):\\n        logger.debug(\"build_get_revoc_reg_def_request: Creating callback\")\\n        build_get_revoc_reg_def_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_rev_reg_def_id = c_char_p(rev_reg_def_id.encode(\\'utf-8\\'))\\n\\n    request_json = await do_call(\\'indy_build_get_revoc_reg_def_request\\',\\n                                 c_submitter_did,\\n                                 c_rev_reg_def_id,\\n                                 build_get_revoc_reg_def_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_get_revoc_reg_def_request: <<< res: %r\", res)\\n    return res', 'async def parse_get_revoc_reg_def_response(get_revoc_ref_def_response: str) -> (str, str):\\n    \"\"\"\\n    Parse a GET_REVOC_REG_DEF response to get Revocation Registry Definition in the format compatible with Anoncreds API.\\n\\n    :param get_revoc_ref_def_response: response of GET_REVOC_REG_DEF request.\\n    :return: Revocation Registry Definition Id and Revocation Registry Definition json.\\n      {\\n          \"id\": string - ID of the Revocation Registry,\\n          \"revocDefType\": string - Revocation Registry type (only CL_ACCUM is supported for now),\\n          \"tag\": string - Unique descriptive ID of the Registry,\\n          \"credDefId\": string - ID of the corresponding CredentialDefinition,\\n          \"value\": Registry-specific data {\\n              \"issuanceType\": string - Type of Issuance(ISSUANCE_BY_DEFAULT or ISSUANCE_ON_DEMAND),\\n              \"maxCredNum\": number - Maximum number of credentials the Registry can serve.\\n              \"tailsHash\": string - Hash of tails.\\n              \"tailsLocation\": string - Location of tails file.\\n              \"publicKeys\": <public_keys> - Registry\\'s public key.\\n          },\\n          \"ver\": string - version of revocation registry definition json.\\n      }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"parse_get_revoc_reg_def_response: >>> get_revoc_ref_def_response: %r\", get_revoc_ref_def_response)\\n\\n    if not hasattr(parse_get_revoc_reg_def_response, \"cb\"):\\n        logger.debug(\"parse_get_revoc_reg_def_response: Creating callback\")\\n        parse_get_revoc_reg_def_response.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_get_revoc_ref_def_response = c_char_p(get_revoc_ref_def_response.encode(\\'utf-8\\'))\\n\\n    (revoc_reg_def_id, revoc_reg_def_json) = await do_call(\\'indy_parse_get_revoc_reg_def_response\\',\\n                                                           c_get_revoc_ref_def_response,\\n                                                           parse_get_revoc_reg_def_response.cb)\\n\\n    res = (revoc_reg_def_id.decode(), revoc_reg_def_json.decode())\\n    logger.debug(\"parse_get_revoc_reg_def_response: <<< res: %r\", res)\\n    return res', 'async def build_revoc_reg_entry_request(submitter_did: str,\\n                                        revoc_reg_def_id: str,\\n                                        rev_def_type: str,\\n                                        value: str) -> str:\\n    \"\"\"\\n    Builds a REVOC_REG_ENTRY request.  Request to add the RevocReg entry containing\\n    the new accumulator value and issued/revoked indices.\\n    This is just a delta of indices, not the whole list. So, it can be sent each time a new credential is issued/revoked.\\n\\n    :param submitter_did: DID of the submitter stored in secured Wallet.\\n    :param revoc_reg_def_id:  ID of the corresponding RevocRegDef.\\n    :param rev_def_type:  Revocation Registry type (only CL_ACCUM is supported for now).\\n    :param value: Registry-specific data: \\n       {\\n           value: {\\n               prevAccum: string - previous accumulator value.\\n               accum: string - current accumulator value.\\n               issued: array<number> - an array of issued indices.\\n               revoked: array<number> an array of revoked indices.\\n           },\\n           ver: string - version revocation registry entry json\\n      \\n       }\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_revoc_reg_entry_request: >>> submitter_did: %r, rev_def_type: %r, revoc_reg_def_id: %r, \"\\n                 \"value: %r\", submitter_did, rev_def_type, revoc_reg_def_id, value)\\n\\n    if not hasattr(build_revoc_reg_entry_request, \"cb\"):\\n        logger.debug(\"build_revoc_reg_entry_request: Creating callback\")\\n        build_revoc_reg_entry_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\'))\\n    c_rev_def_type = c_char_p(rev_def_type.encode(\\'utf-8\\'))\\n    c_revoc_reg_def_id = c_char_p(revoc_reg_def_id.encode(\\'utf-8\\'))\\n    c_value = c_char_p(value.encode(\\'utf-8\\'))\\n\\n    request_json = await do_call(\\'indy_build_revoc_reg_entry_request\\',\\n                                 c_submitter_did,\\n                                 c_revoc_reg_def_id,\\n                                 c_rev_def_type,\\n                                 c_value,\\n                                 build_revoc_reg_entry_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_revoc_reg_entry_request: <<< res: %r\", res)\\n    return res', 'async def build_get_revoc_reg_request(submitter_did: Optional[str],\\n                                      revoc_reg_def_id: str,\\n                                      timestamp: int) -> str:\\n    \"\"\"\\n    Builds a GET_REVOC_REG request. Request to get the accumulated state of the Revocation Registry\\n    by ID. The state is defined by the given timestamp.\\n\\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\\n    :param revoc_reg_def_id:  ID of the corresponding Revocation Registry Definition in ledger.\\n    :param timestamp: Requested time represented as a total number of seconds from Unix Epoch\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_get_revoc_reg_request: >>> submitter_did: %r, revoc_reg_def_id: %r, timestamp: %r\",\\n                 submitter_did, revoc_reg_def_id, timestamp)\\n\\n    if not hasattr(build_get_revoc_reg_request, \"cb\"):\\n        logger.debug(\"build_get_revoc_reg_request: Creating callback\")\\n        build_get_revoc_reg_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_revoc_reg_def_id = c_char_p(revoc_reg_def_id.encode(\\'utf-8\\'))\\n    c_timestamp = c_int64(timestamp)\\n\\n    request_json = await do_call(\\'indy_build_get_revoc_reg_request\\',\\n                                 c_submitter_did,\\n                                 c_revoc_reg_def_id,\\n                                 c_timestamp,\\n                                 build_get_revoc_reg_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_get_revoc_reg_request: <<< res: %r\", res)\\n    return res', 'async def parse_get_revoc_reg_response(get_revoc_reg_response: str) -> (str, str, int):\\n    \"\"\"\\n    Parse a GET_REVOC_REG response to get Revocation Registry in the format compatible with Anoncreds API.\\n\\n    :param get_revoc_reg_response: response of GET_REVOC_REG request.\\n    :return: Revocation Registry Definition Id, Revocation Registry json and Timestamp.\\n      {\\n          \"value\": Registry-specific data {\\n              \"accum\": string - current accumulator value.\\n          },\\n          \"ver\": string - version revocation registry json\\n      }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"parse_get_revoc_reg_response: >>> get_revoc_reg_response: %r\", get_revoc_reg_response)\\n\\n    if not hasattr(parse_get_revoc_reg_response, \"cb\"):\\n        logger.debug(\"parse_get_revoc_reg_response: Creating callback\")\\n        parse_get_revoc_reg_response.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p, c_uint64))\\n\\n    c_get_revoc_reg_response = c_char_p(get_revoc_reg_response.encode(\\'utf-8\\'))\\n\\n    (revoc_reg_def_id, revoc_reg_json, timestamp) = await do_call(\\'indy_parse_get_revoc_reg_response\\',\\n                                                                  c_get_revoc_reg_response,\\n                                                                  parse_get_revoc_reg_response.cb)\\n\\n    res = (revoc_reg_def_id.decode(), revoc_reg_json.decode(), timestamp)\\n    logger.debug(\"parse_get_revoc_reg_response: <<< res: %r\", res)\\n    return res', 'async def build_get_revoc_reg_delta_request(submitter_did: Optional[str],\\n                                            revoc_reg_def_id: str,\\n                                            from_: Optional[int],\\n                                            to: int) -> str:\\n    \"\"\"\\n    Builds a GET_REVOC_REG_DELTA request. Request to get the delta of the accumulated state of the Revocation Registry.\\n    The Delta is defined by from and to timestamp fields.\\n    If from is not specified, then the whole state till to will be returned.\\n\\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\\n    :param revoc_reg_def_id:  ID of the corresponding Revocation Registry Definition in ledger.\\n    :param from_: Requested time represented as a total number of seconds from Unix Epoch\\n    :param to: Requested time represented as a total number of seconds from Unix Epoch\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_get_revoc_reg_delta_request: >>> submitter_did: %r, revoc_reg_def_id: %r, from: %r, to: %r\",\\n                 submitter_did, revoc_reg_def_id, from_, to)\\n\\n    if not hasattr(build_get_revoc_reg_delta_request, \"cb\"):\\n        logger.debug(\"build_get_revoc_reg_delta_request: Creating callback\")\\n        build_get_revoc_reg_delta_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_revoc_reg_def_id = c_char_p(revoc_reg_def_id.encode(\\'utf-8\\'))\\n    c_from = c_int64(from_) if from_  else -1\\n    c_to = c_int64(to)\\n\\n    request_json = await do_call(\\'indy_build_get_revoc_reg_delta_request\\',\\n                                 c_submitter_did,\\n                                 c_revoc_reg_def_id,\\n                                 c_from,\\n                                 c_to,\\n                                 build_get_revoc_reg_delta_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_get_revoc_reg_delta_request: <<< res: %r\", res)\\n    return res', 'async def parse_get_revoc_reg_delta_response(get_revoc_reg_delta_response: str) -> (str, str, int):\\n    \"\"\"\\n    Parse a GET_REVOC_REG_DELTA response to get Revocation Registry Delta in the format compatible with Anoncreds API.\\n\\n    :param get_revoc_reg_delta_response: response of GET_REVOC_REG_DELTA request.\\n    :return: Revocation Registry Definition Id, Revocation Registry Delta json and Timestamp.\\n      {\\n          \"value\": Registry-specific data {\\n              prevAccum: string - previous accumulator value.\\n              accum: string - current accumulator value.\\n              issued: array<number> - an array of issued indices.\\n              revoked: array<number> an array of revoked indices.\\n          },\\n          \"ver\": string\\n      }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"parse_get_revoc_reg_delta_response: >>> get_revoc_reg_delta_response: %r\",\\n                 get_revoc_reg_delta_response)\\n\\n    if not hasattr(parse_get_revoc_reg_delta_response, \"cb\"):\\n        logger.debug(\"parse_get_revoc_reg_delta_response: Creating callback\")\\n        parse_get_revoc_reg_delta_response.cb = create_cb(\\n            CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p, c_uint64))\\n\\n    c_get_revoc_reg_delta_response = c_char_p(get_revoc_reg_delta_response.encode(\\'utf-8\\'))\\n\\n    (revoc_reg_def_id, revoc_reg_delta_json, timestamp) = await do_call(\\'indy_parse_get_revoc_reg_delta_response\\',\\n                                                                        c_get_revoc_reg_delta_response,\\n                                                                        parse_get_revoc_reg_delta_response.cb)\\n\\n    res = (revoc_reg_def_id.decode(), revoc_reg_delta_json.decode(), timestamp)\\n    logger.debug(\"parse_get_revoc_reg_delta_response: <<< res: %r\", res)\\n    return res', 'async def get_response_metadata(response: str) -> str:\\n    \"\"\"\\n     Parse transaction response to fetch metadata.\\n     The important use case for this method is validation of Node\\'s response freshens.\\n\\n     Distributed Ledgers can reply with outdated information for consequence read request after write.\\n     To reduce pool load libindy sends read requests to one random node in the pool.\\n     Consensus validation is performed based on validation of nodes multi signature for current ledger Merkle Trie root.\\n     This multi signature contains information about the latest ldeger\\'s transaction ordering time and sequence number that this method returns.\\n\\n     If node that returned response for some reason is out of consensus and has outdated ledger\\n     it can be caught by analysis of the returned latest ledger\\'s transaction ordering time and sequence number.\\n\\n     There are two ways to filter outdated responses:\\n         1) based on \"seqNo\" - sender knows the sequence number of transaction that he consider as a fresh enough.\\n         2) based on \"txnTime\" - sender knows the timestamp that he consider as a fresh enough.\\n\\n     Note: response of GET_VALIDATOR_INFO request isn\\'t supported\\n\\n    :param response: response of write or get request.\\n    :return: Response Metadata.\\n    {\\n        \"seqNo\": Option<u64> - transaction sequence number,\\n        \"txnTime\": Option<u64> - transaction ordering time,\\n        \"lastSeqNo\": Option<u64> - the latest transaction seqNo for particular Node,\\n        \"lastTxnTime\": Option<u64> - the latest transaction ordering time for particular Node\\n    }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"get_response_metadata: >>> response: %r\",\\n                 response)\\n\\n    if not hasattr(get_response_metadata, \"cb\"):\\n        logger.debug(\"get_response_metadata: Creating callback\")\\n        get_response_metadata.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_response = c_char_p(response.encode(\\'utf-8\\'))\\n\\n    response_metadata = await do_call(\\'indy_get_response_metadata\\',\\n                                      c_response,\\n                                      get_response_metadata.cb)\\n\\n    res = response_metadata.decode()\\n    logger.debug(\"get_response_metadata: <<< res: %r\", res)\\n    return res', 'async def build_auth_rule_request(submitter_did: str,\\n                                  txn_type: str,\\n                                  action: str,\\n                                  field: str,\\n                                  old_value: Optional[str],\\n                                  new_value: Optional[str],\\n                                  constraint: str) -> str:\\n    \"\"\"\\n    Builds a AUTH_RULE request. Request to change authentication rules for a ledger transaction.\\n\\n    :param submitter_did: DID of the submitter stored in secured Wallet.\\n    :param txn_type: ledger transaction alias or associated value.\\n    :param action: type of an action.\\n       Can be either \"ADD\" (to add a new rule) or \"EDIT\" (to edit an existing one).\\n    :param field: transaction field.\\n    :param old_value: (Optional) old value of a field, which can be changed to a new_value (mandatory for EDIT action).\\n    :param new_value: (Optional) new value that can be used to fill the field.\\n    :param constraint: set of constraints required for execution of an action in the following format:\\n        {\\n            constraint_id - <string> type of a constraint.\\n                Can be either \"ROLE\" to specify final constraint or  \"AND\"/\"OR\" to combine constraints.\\n            role - <string> role of a user which satisfy to constrain.\\n            sig_count - <u32> the number of signatures required to execution action.\\n            need_to_be_owner - <bool> if user must be an owner of transaction.\\n            metadata - <object> additional parameters of the constraint.\\n        }\\n      can be combined by\\n        {\\n            \\'constraint_id\\': <\"AND\" or \"OR\">\\n            \\'auth_constraints\\': [<constraint_1>, <constraint_2>]\\n        }\\n\\n    Default ledger auth rules: https://github.com/hyperledger/indy-node/blob/master/docs/source/auth_rules.md\\n\\n    More about AUTH_RULE request: https://github.com/hyperledger/indy-node/blob/master/docs/source/requests.md#auth_rule\\n\\n    :return: Request result as json.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_auth_rule_request: >>> submitter_did: %r, txn_type: %r, action: %r, field: %r, \"\\n                 \"old_value: %r, new_value: %r, constraint: %r\",\\n                 submitter_did,\\n                 txn_type,\\n                 action,\\n                 field,\\n                 old_value,\\n                 new_value,\\n                 constraint)\\n\\n    if not hasattr(build_auth_rule_request, \"cb\"):\\n        logger.debug(\"build_auth_rule_request: Creating callback\")\\n        build_auth_rule_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\'))\\n    c_txn_type = c_char_p(txn_type.encode(\\'utf-8\\'))\\n    c_action = c_char_p(action.encode(\\'utf-8\\'))\\n    c_field = c_char_p(field.encode(\\'utf-8\\'))\\n    c_old_value = c_char_p(old_value.encode(\\'utf-8\\')) if old_value is not None else None\\n    c_new_value = c_char_p(new_value.encode(\\'utf-8\\')) if new_value is not None else None\\n    c_constraint = c_char_p(constraint.encode(\\'utf-8\\'))\\n\\n    request_json = await do_call(\\'indy_build_auth_rule_request\\',\\n                                 c_submitter_did,\\n                                 c_txn_type,\\n                                 c_action,\\n                                 c_field,\\n                                 c_old_value,\\n                                 c_new_value,\\n                                 c_constraint,\\n                                 build_auth_rule_request.cb)\\n\\n    res = request_json.decode()\\n    logger.debug(\"build_auth_rule_request: <<< res: %r\", res)\\n    return res', 'async def create_and_store_my_did(wallet_handle: int,\\n                                  did_json: str) -> (str, str):\\n    \"\"\"\\n    Creates keys (signing and encryption keys) for a new\\n    DID (owned by the caller of the library).\\n    Identity\\'s DID must be either explicitly provided, or taken as the first 16 bit of verkey.\\n    Saves the Identity DID with keys in a secured Wallet, so that it can be used to sign\\n    and encrypt transactions.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param did_json: Identity information as json. Example:\\n        {\\n            \"did\": string, (optional;\\n                    if not provided and cid param is false then the first 16 bit of the verkey will be\\n                    used as a new DID;\\n                    if not provided and cid is true then the full verkey will be used as a new DID;\\n                    if provided, then keys will be replaced - key rotation use case)\\n\\t        \"seed\": string, (optional) Seed that allows deterministic key creation (if not set random one will be created).\\n\\t                                    Can be UTF-8, base64 or hex string.\\n            \"crypto_type\": string, (optional; if not set then ed25519 curve is used;\\n                      currently only \\'ed25519\\' value is supported for this field)\\n            \"cid\": bool, (optional; if not set then false is used;)\\n        }\\n    :return: DID and verkey (for verification of signature)\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"create_and_store_my_did: >>> wallet_handle: %r, did_json: %r\",\\n                 wallet_handle,\\n                 did_json)\\n\\n    if not hasattr(create_and_store_my_did, \"cb\"):\\n        logger.debug(\"create_wallet: Creating callback\")\\n        create_and_store_my_did.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_did_json = c_char_p(did_json.encode(\\'utf-8\\'))\\n\\n    did, verkey = await do_call(\\'indy_create_and_store_my_did\\',\\n                                c_wallet_handle,\\n                                c_did_json,\\n                                create_and_store_my_did.cb)\\n\\n    res = (did.decode(), verkey.decode())\\n\\n    logger.debug(\"create_and_store_my_did: <<< res: %r\", res)\\n    return res', 'async def replace_keys_start(wallet_handle: int,\\n                             did: str,\\n                             identity_json: str) -> str:\\n    \"\"\"\\n    Generated new keys (signing and encryption keys) for an existing\\n    DID (owned by the caller of the library).\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param did: signing DID\\n    :param identity_json: Identity information as json. Example:\\n        {\\n\\t        \"seed\": string, (optional) Seed that allows deterministic key creation (if not set random one will be created).\\n\\t                                   Can be UTF-8, base64 or hex string.\\n            \"crypto_type\": string, (optional; if not set then ed25519 curve is used;\\n                      currently only \\'ed25519\\' value is supported for this field)\\n        }\\n    :return: verkey\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"replace_keys_start: >>> wallet_handle: %r, did: %r, identity_json: %r\",\\n                 wallet_handle,\\n                 did,\\n                 identity_json)\\n\\n    if not hasattr(replace_keys_start, \"cb\"):\\n        logger.debug(\"replace_keys_start: Creating callback\")\\n        replace_keys_start.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_did = c_char_p(did.encode(\\'utf-8\\'))\\n    c_identity_json = c_char_p(identity_json.encode(\\'utf-8\\'))\\n\\n    verkey = await do_call(\\'indy_replace_keys_start\\',\\n                           c_wallet_handle,\\n                           c_did,\\n                           c_identity_json,\\n                           replace_keys_start.cb)\\n\\n    res = verkey.decode()\\n\\n    logger.debug(\"replace_keys_start: <<< res: %r\", res)\\n    return res', 'async def replace_keys_apply(wallet_handle: int,\\n                             did: str) -> None:\\n    \"\"\"\\n    Apply temporary keys as main for an existing DID (owned by the caller of the library).\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param did: The DID to resolve key.\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"replace_keys_apply: >>> wallet_handle: %r, did: %r\",\\n                 wallet_handle,\\n                 did)\\n\\n    if not hasattr(replace_keys_apply, \"cb\"):\\n        logger.debug(\"replace_keys_apply: Creating callback\")\\n        replace_keys_apply.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_did = c_char_p(did.encode(\\'utf-8\\'))\\n\\n    await do_call(\\'indy_replace_keys_apply\\',\\n                  c_wallet_handle,\\n                  c_did,\\n                  replace_keys_apply.cb)\\n\\n    logger.debug(\"replace_keys_apply: <<<\")', 'async def store_their_did(wallet_handle: int,\\n                          identity_json: str) -> None:\\n    \"\"\"\\n    Saves their DID for a pairwise connection in a secured Wallet,\\n    so that it can be used to verify transaction.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param identity_json: Identity information as json. Example:\\n        {\\n           \"did\": string, (required)\\n           \"verkey\": string (optional, if only pk is provided),\\n           \"crypto_type\": string, (optional; if not set then ed25519 curve is used;\\n                  currently only \\'ed25519\\' value is supported for this field)\\n        }\\n    :return: None\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"store_their_did: >>> wallet_handle: %r, identity_json: %r\",\\n                 wallet_handle,\\n                 identity_json)\\n\\n    if not hasattr(store_their_did, \"cb\"):\\n        logger.debug(\"store_their_did: Creating callback\")\\n        store_their_did.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_identity_json = c_char_p(identity_json.encode(\\'utf-8\\'))\\n\\n    res = await do_call(\\'indy_store_their_did\\',\\n                        c_wallet_handle,\\n                        c_identity_json,\\n                        store_their_did.cb)\\n\\n    logger.debug(\"store_their_did: <<< res: %r\", res)\\n    return res', 'async def create_key(wallet_handle: int,\\n                     key_json: str) -> str:\\n    \"\"\"\\n    Creates keys pair and stores in the wallet.\\n\\n    :param wallet_handle: Wallet handle (created by open_wallet).\\n    :param key_json: Key information as json. Example:\\n        {\\n\\t        \"seed\": string, (optional) Seed that allows deterministic key creation (if not set random one will be created).\\n\\t                                   Can be UTF-8, base64 or hex string.\\n            \"crypto_type\": string, // Optional (if not set then ed25519 curve is used);\\n                    Currently only \\'ed25519\\' value is supported for this field.\\n        }\\n    :return: verkey: Ver key of generated key pair, also used as key identifier\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"create_key: >>> wallet_handle: %r, key_json: %r\",\\n                 wallet_handle,\\n                 key_json)\\n\\n    if not hasattr(create_key, \"cb\"):\\n        logger.debug(\"create_key: Creating callback\")\\n        create_key.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_key_json = c_char_p(key_json.encode(\\'utf-8\\'))\\n\\n    verkey = await do_call(\\'indy_create_key\\',\\n                           c_wallet_handle,\\n                           c_key_json,\\n                           create_key.cb)\\n\\n    res = verkey.decode()\\n\\n    logger.debug(\"create_key: <<< res: %r\", res)\\n    return res', 'async def set_key_metadata(wallet_handle: int,\\n                           verkey: str,\\n                           metadata: str) -> None:\\n    \"\"\"\\n    Creates keys pair and stores in the wallet.\\n\\n    :param wallet_handle: Wallet handle (created by open_wallet).\\n    :param verkey: the key (verkey, key id) to store metadata.\\n    :param metadata: the meta information that will be store with the key.\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"set_key_metadata: >>> wallet_handle: %r, verkey: %r, metadata: %r\",\\n                 wallet_handle,\\n                 verkey,\\n                 metadata)\\n\\n    if not hasattr(set_key_metadata, \"cb\"):\\n        logger.debug(\"set_key_metadata: Creating callback\")\\n        set_key_metadata.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_verkey = c_char_p(verkey.encode(\\'utf-8\\'))\\n    c_metadata = c_char_p(metadata.encode(\\'utf-8\\'))\\n\\n    await do_call(\\'indy_set_key_metadata\\',\\n                  c_wallet_handle,\\n                  c_verkey,\\n                  c_metadata,\\n                  set_key_metadata.cb)\\n\\n    logger.debug(\"create_key: <<<\")', 'async def get_key_metadata(wallet_handle: int,\\n                           verkey: str) -> str:\\n    \"\"\"\\n    Retrieves the meta information for the giving key in the wallet.\\n\\n    :param wallet_handle: Wallet handle (created by open_wallet).\\n    :param verkey: The key (verkey, key id) to retrieve metadata.\\n    :return: metadata: The meta information stored with the key; Can be null if no metadata was saved for this key.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"get_key_metadata: >>> wallet_handle: %r, verkey: %r\",\\n                 wallet_handle,\\n                 verkey)\\n\\n    if not hasattr(get_key_metadata, \"cb\"):\\n        logger.debug(\"get_key_metadata: Creating callback\")\\n        get_key_metadata.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_verkey = c_char_p(verkey.encode(\\'utf-8\\'))\\n\\n    metadata = await do_call(\\'indy_get_key_metadata\\',\\n                             c_wallet_handle,\\n                             c_verkey,\\n                             get_key_metadata.cb)\\n\\n    res = metadata.decode()\\n\\n    logger.debug(\"get_key_metadata: <<< res: %r\", res)\\n    return res', 'async def set_endpoint_for_did(wallet_handle: int,\\n                               did: str,\\n                               address: str,\\n                               transport_key: str) -> None:\\n    \"\"\"\\n    Set/replaces endpoint information for the given DID.\\n\\n    :param wallet_handle: Wallet handle (created by open_wallet).\\n    :param did: The DID to resolve endpoint.\\n    :param address: The DIDs endpoint address.\\n    :param transport_key: The DIDs transport key (ver key, key id).\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"set_endpoint_for_did: >>> wallet_handle: %r, did: %r, address: %r, transport_key: %r\",\\n                 wallet_handle,\\n                 did,\\n                 address,\\n                 transport_key)\\n\\n    if not hasattr(set_endpoint_for_did, \"cb\"):\\n        logger.debug(\"set_endpoint_for_did: Creating callback\")\\n        set_endpoint_for_did.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_did = c_char_p(did.encode(\\'utf-8\\'))\\n    c_address = c_char_p(address.encode(\\'utf-8\\'))\\n    c_transport_key = c_char_p(transport_key.encode(\\'utf-8\\'))\\n\\n    await do_call(\\'indy_set_endpoint_for_did\\',\\n                  c_wallet_handle,\\n                  c_did,\\n                  c_address,\\n                  c_transport_key,\\n                  set_endpoint_for_did.cb)\\n\\n    logger.debug(\"set_endpoint_for_did: <<<\")', 'async def get_endpoint_for_did(wallet_handle: int,\\n                               pool_handle: int,\\n                               did: str) -> (str, Optional[str]):\\n    \"\"\"\\n    Returns endpoint information for the given DID.\\n\\n    :param wallet_handle: Wallet handle (created by open_wallet).\\n    :param pool_handle: Pool handle (created by open_pool).\\n    :param did: The DID to resolve endpoint.\\n    :return: (endpoint, transport_vk)\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"get_endpoint_for_did: >>> wallet_handle: %r, pool_handle: %r, did: %r\",\\n                 wallet_handle,\\n                 pool_handle,\\n                 did)\\n\\n    if not hasattr(get_endpoint_for_did, \"cb\"):\\n        logger.debug(\"get_endpoint_for_did: Creating callback\")\\n        get_endpoint_for_did.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_pool_handle = c_int32(pool_handle)\\n    c_did = c_char_p(did.encode(\\'utf-8\\'))\\n\\n    endpoint, transport_vk = await do_call(\\'indy_get_endpoint_for_did\\',\\n                                           c_wallet_handle,\\n                                           c_pool_handle,\\n                                           c_did,\\n                                           get_endpoint_for_did.cb)\\n\\n    endpoint = endpoint.decode()\\n    transport_vk = transport_vk.decode() if transport_vk is not None else None\\n    res = (endpoint, transport_vk)\\n\\n    logger.debug(\"get_endpoint_for_did: <<< res: %r\", res)\\n    return res', 'async def set_did_metadata(wallet_handle: int,\\n                           did: str,\\n                           metadata: str) -> None:\\n    \"\"\"\\n    Saves/replaces the meta information for the giving DID in the wallet.\\n\\n    :param wallet_handle: Wallet handle (created by open_wallet).\\n    :param did: the DID to store metadata.\\n    :param metadata: the meta information that will be store with the DID.\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"set_did_metadata: >>> wallet_handle: %r, did: %r, metadata: %r\",\\n                 wallet_handle,\\n                 did,\\n                 metadata)\\n\\n    if not hasattr(set_did_metadata, \"cb\"):\\n        logger.debug(\"set_did_metadata: Creating callback\")\\n        set_did_metadata.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_did = c_char_p(did.encode(\\'utf-8\\'))\\n    c_metadata = c_char_p(metadata.encode(\\'utf-8\\'))\\n\\n    await do_call(\\'indy_set_did_metadata\\',\\n                  c_wallet_handle,\\n                  c_did,\\n                  c_metadata,\\n                  set_did_metadata.cb)\\n\\n    logger.debug(\"set_did_metadata: <<<\")', 'async def get_my_did_with_meta(wallet_handle: int, did: str) -> str:\\n    \"\"\"\\n    Get DID metadata and verkey stored in the wallet.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param did: The DID to retrieve metadata.\\n    :return: DID with verkey and metadata.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"get_my_did_with_meta: >>> wallet_handle: %r, did: %r\",\\n                 wallet_handle,\\n                 did)\\n\\n    if not hasattr(get_my_did_with_meta, \"cb\"):\\n        logger.debug(\"get_my_did_with_meta: Creating callback\")\\n        get_my_did_with_meta.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_did = c_char_p(did.encode(\\'utf-8\\'))\\n\\n    did_with_meta = await do_call(\\'indy_get_my_did_with_meta\\',\\n                                  c_wallet_handle,\\n                                  c_did,\\n                                  get_my_did_with_meta.cb)\\n\\n    res = did_with_meta.decode()\\n\\n    logger.debug(\"get_my_did_with_meta: <<< res: %r\", res)\\n    return res', 'async def abbreviate_verkey(did: str,\\n                          full_verkey: str) -> str:\\n    \"\"\"\\n    Retrieves abbreviated verkey if it is possible otherwise return full verkey.\\n\\n    :param did: The DID.\\n    :param full_verkey: The DIDs verification key,\\n    :return: metadata: Either abbreviated or full verkey.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"abbreviate_verkey: >>> did: %r, full_verkey: %r\",\\n                 did, full_verkey)\\n\\n    if not hasattr(abbreviate_verkey, \"cb\"):\\n        logger.debug(\"abbreviate_verkey: Creating callback\")\\n        abbreviate_verkey.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_did = c_char_p(did.encode(\\'utf-8\\'))\\n    c_full_verkey = c_char_p(full_verkey.encode(\\'utf-8\\'))\\n\\n    metadata = await do_call(\\'indy_abbreviate_verkey\\',\\n                             c_did,\\n                             c_full_verkey,\\n                             abbreviate_verkey.cb)\\n\\n    res = metadata.decode()\\n\\n    logger.debug(\"abbreviate_verkey: <<< res: %r\", res)\\n    return res', 'async def create(source_id: str, credential_offer: str):\\n        \"\"\"\\n        Creates a credential with an offer.\\n        :param source_id: user defined id of object.\\n        :param credential_offer: JSON string representing the offer used as the basis of creation.\\n        :return: A created credential\\n        Example:\\n        offer = [{\\n           \"msg_type\": \"CLAIM_OFFER\",\\n           \"version\": \"0.1\",\\n           \"to_did\": \"8XFh8yBzrpJQmNyZzgoTqB\",\\n           \"from_did\": \"8XFh8yBzrpJQmNyZzgoTqB\",\\n           \"libindy_offer\": \\'{}\\',\\n           \"credential_attrs\": {\\n              \"address1\": [\\n                 \"101 Tela Lane\"\\n              ],\\n              \"address2\": [\\n                 \"101 Wilson Lane\"\\n              ],\\n              \"city\": [\\n                 \"SLC\"\\n              ],\\n              \"state\": [\\n                 \"UT\"\\n              ],\\n              \"zip\": [\\n                 \"87121\"\\n              ]\\n           },\\n           \"schema_seq_no\": 1487,\\n           \"cred_def_id\": \"id1\",\\n           \"claim_name\": \"Credential\",\\n           \"claim_id\": \"defaultCredentialId\",\\n           \"msg_ref_id\": None,\\n        }]\\n        credential = await Credential.create(source_id, offer)\\n        \"\"\"\\n        constructor_params = (source_id,)\\n\\n        c_source_id = c_char_p(source_id.encode(\\'utf-8\\'))\\n        c_offer = c_char_p(json.dumps(credential_offer).encode(\\'utf-8\\'))\\n        c_params = (c_source_id, c_offer, )\\n\\n        return await Credential._create(\"vcx_credential_create_with_offer\",\\n                                        constructor_params,\\n                                        c_params)', 'async def create_with_msgid(source_id: str, connection: Connection, msg_id: str):\\n        \"\"\"\\n        Create a credential based off of a known message id for a given connection.\\n        :param source_id: user defined id of object.\\n        :param connection: connection handle of connection to receive offer from\\n        :param msg_id: message id\\n        :return: A created credential\\n        Example:\\n        credential = await Credential.create_with_msgid(source_id, connection, msg_id)\\n        assert await credential.get_state() == State.RequestReceived\\n        \"\"\"\\n        credential = Credential(source_id,)\\n\\n        c_source_id = c_char_p(source_id.encode(\\'utf-8\\'))\\n        c_msg_id = c_char_p(json.dumps(msg_id).encode(\\'utf-8\\'))\\n        c_connection_handle = c_uint32(connection.handle)\\n\\n        if not hasattr(Credential.create_with_msgid, \"cb\"):\\n            Credential.create_with_msgid.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_uint32, c_char_p))\\n\\n        credential.handle, cred_offer = await do_call(\\'vcx_credential_create_with_msgid\\',\\n                                                      c_source_id,\\n                                                      c_connection_handle,\\n                                                      c_msg_id,\\n                                                      Credential.create_with_msgid.cb)\\n\\n        credential.cred_offer = json.loads(cred_offer.decode())\\n\\n        return credential', 'async def deserialize(data: dict):\\n        \"\"\"\\n        Create a credential object from a previously serialized credential object\\n        :param data: JSON data from a serialized object.\\n        :return: A created credential\\n        Example:\\n        credential = await Credential.create(source_id, offer)\\n        data = await credential.serialize()\\n        credential2 = await Credential.deserialize(data)\\n        \"\"\"\\n\\n        credential = await Credential._deserialize(\"vcx_credential_deserialize\",\\n                                                   json.dumps(data),\\n                                                   data.get(\\'data\\').get(\\'source_id\\'))\\n        return credential', 'async def get_offers(connection: Connection) -> dict:\\n        \"\"\"\\n        Retrieves all pending credential offers for a given connection.\\n        :param connection: A connection handle\\n        :return: A list of dictionary objects representing offers from a given connection.\\n        Example:\\n        credential = await Credential.create_with_msgid(source_id, connection, msg_id)\\n        offers = await credential.get_offers(connection)\\n        \"\"\"\\n        if not hasattr(Credential.get_offers, \"cb\"):\\n            Credential.get_offers.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_connection_handle = c_uint32(connection.handle)\\n\\n        data = await do_call(\\'vcx_credential_get_offers\\',\\n                             c_connection_handle,\\n                             Credential.get_offers.cb)\\n\\n        return json.loads(data.decode())', 'async def send_request(self, connection: Connection, payment_handle: int):\\n        \"\"\"\\n        Approves the credential offer and submits a credential request. The result will be a credential stored in the prover\\'s wallet.\\n        :param connection: connection to submit request from\\n        :param payment_handle: currently unused\\n        :return:\\n        Example:\\n        connection = await Connection.create(source_id)\\n        await connection.connect(phone_number)\\n        credential = await Credential.create(source_id, offer)\\n        await credential.send_request(connection, 0)\\n        \"\"\"\\n        if not hasattr(Credential.send_request, \"cb\"):\\n            self.logger.debug(\"vcx_credential_send_request: Creating callback\")\\n            Credential.send_request.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n        c_credential_handle = c_uint32(self.handle)\\n        c_connection_handle = c_uint32(connection.handle)\\n        c_payment = c_uint32(payment_handle)\\n\\n        await do_call(\\'vcx_credential_send_request\\',\\n                      c_credential_handle,\\n                      c_connection_handle,\\n                      c_payment,\\n                      Credential.send_request.cb)', 'async def get_payment_info(self):\\n        \"\"\"\\n        Retrieve Payment Transaction Information for this Credential. Typically this will include\\n        how much payment is requried by the issuer, which needs to be provided by the prover, before the issuer will\\n        issue the credential to the prover. Ideally a prover would want to know how much payment is being asked before\\n        submitting the credential request (which triggers the payment to be made).\\n        Example:\\n        info = credential.get_payment_info()\\n        :return:\\n        \"\"\"\\n        if not hasattr(Credential.get_payment_info, \"cb\"):\\n            self.logger.debug(\"vcx_credential_get_payment_info: Creating callback\")\\n            Credential.get_payment_info.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_credential_handle = c_uint32(self.handle)\\n        data = await do_call(\\'vcx_credential_get_payment_info\\',\\n                      c_credential_handle,\\n                      Credential.get_payment_info.cb)\\n        return json.loads(data.decode())', 'async def get_payment_txn(self):\\n        \"\"\"\\n        Retirieve the payment transaction associated with this credential. This can be used to get the txn that\\n        was used to pay the issuer from the prover.  This could be considered a receipt of payment from the payer to\\n        the issuer.\\n        :return:\\n        Example:\\n        txn = credential.get_payment_txn()\\n        \"\"\"\\n        if not hasattr(Credential.get_payment_txn, \"cb\"):\\n            self.logger.debug(\"vcx_credential_get_payment_txn: Creating callback\")\\n            Credential.get_payment_txn.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_credential_handle = c_uint32(self.handle)\\n\\n        payment_txn = await do_call(\\'vcx_credential_get_payment_txn\\',\\n                      c_credential_handle,\\n                      Credential.get_payment_txn.cb)\\n\\n        return json.loads(payment_txn.decode())', 'async def create(source_id: str):\\n        \"\"\"\\n        Create a connection object, represents a single endpoint and can be used for sending and receiving\\n        credentials and proofs\\n\\n        :param source_id: Institution\\'s unique ID for the connection\\n        :return: connection object\\n        Example:\\n        connection = await Connection.create(source_id)\\n        \"\"\"\\n        constructor_params = (source_id,)\\n\\n        c_source_id = c_char_p(source_id.encode(\\'utf-8\\'))\\n        c_params = (c_source_id,)\\n\\n        return await Connection._create( \"vcx_connection_create\",\\n                                        constructor_params,\\n                                        c_params)', 'async def create_with_details(source_id: str, invite_details: str):\\n        \"\"\"\\n        Create a connection object with a provided invite, represents a single endpoint and can be used for sending and receiving\\n        credentials and proofs\\n\\n        Invite details are provided by the entity offering a connection and generally pulled from a provided QRCode.\\n        :param source_id: Institution\\'s unique ID for the connection\\n        :param invite_details: A string representing a json object which is provided by an entity that wishes to make a connection.\\n        Example:\\n        connection2 = await Connection.create_with_details(\\'MyBank\\', invite_details)\\n        :return: connection object\\n        \"\"\"\\n        constructor_params = (source_id,)\\n\\n        c_source_id = c_char_p(source_id.encode(\\'utf-8\\'))\\n        c_invite_details = c_char_p(invite_details.encode(\\'utf-8\\'))\\n\\n        c_params = (c_source_id, c_invite_details, )\\n\\n        return await Connection._create( \"vcx_connection_create_with_invite\",\\n                                        constructor_params,\\n                                        c_params)', 'async def deserialize(data: dict):\\n        \"\"\"\\n        Create the object from a previously serialized object.\\n\\n        :param data: The output of the \"serialize\" call\\n        Example:\\n        data = await connection1.serialize()\\n        connection2 = await Connection.deserialize(data)\\n        :return: A re-instantiated object\\n        \"\"\"\\n        return await Connection._deserialize(\"vcx_connection_deserialize\",\\n                                             json.dumps(data),\\n                                             data.get(\\'source_id\\'))', 'async def connect(self, options: str) -> str:\\n        \"\"\"\\n        Connect securely and privately to the endpoint represented by the object.\\n\\n        :param options: detailed connection options\\n        Example options:\\n        {\"connection_type\":\"SMS\",\"phone\":\"5555555555\",\"use_public_did\":true}\\n        or:\\n        {\"connection_type\":\"QR\"}\\n        Example code:\\n        connection = await Connection.create(\\'Sally\\')\\n        invite_details = await connection.connect(\\'{\"connection_type\":\"QR\"}\\')\\n        :return: the invite details sent via SMS or ready to be sent via some other mechanism (QR for example)\\n        \"\"\"\\n        if not hasattr(Connection.connect, \"cb\"):\\n            self.logger.debug(\"vcx_connection_connect: Creating callback\")\\n            Connection.connect.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_connection_handle = c_uint32(self.handle)\\n        c_connection_data = c_char_p(options.encode(\\'utf-8\\'))\\n        invite_details = await do_call(\\'vcx_connection_connect\\',\\n                                       c_connection_handle,\\n                                       c_connection_data,\\n                                       Connection.connect.cb)\\n        return invite_details', 'async def send_message(self, msg: str, msg_type: str, msg_title: str) -> str:\\n        \"\"\"\\n            Send a generic message to the connection\\n            :param msg:\\n            :param msg_type:\\n            :param msg_title:\\n            :return:\\n            \"\"\"\\n        if not hasattr(Connection.send_message, \"cb\"):\\n            self.logger.debug(\"vcx_connection_send_message: Creating callback\")\\n            Connection.send_message.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_connection_handle = c_uint32(self.handle)\\n        c_msg = c_char_p(msg.encode(\\'utf-8\\'))\\n        c_msg_type = c_char_p(msg_type.encode(\\'utf-8\\'))\\n        c_msg_title = c_char_p(msg_title.encode(\\'utf-8\\'))\\n\\n        result = await do_call(\\'vcx_connection_send_message\\',\\n                               c_connection_handle,\\n                               c_msg,\\n                               c_msg_type,\\n                               c_msg_title,\\n                               Connection.send_message.cb)\\n\\n        self.logger.debug(\"vcx_connection_send_message completed\")\\n        return result', 'async def sign_data(self, msg: bytes) -> bytes:\\n        \"\"\"\\n        Sign data using connection\\'s pairwise key\\n        :param msg:\\n        :return: signature\\n        \"\"\"\\n\\n        def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\\n            return bytes(arr_ptr[:arr_len]),\\n\\n        if not hasattr(Connection.sign_data, \"cb\"):\\n            self.logger.debug(\"vcx_connection_sign_data: Creating callback\")\\n            Connection.sign_data.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, POINTER(c_uint8), c_uint32), transform_cb)\\n\\n        c_connection_handle = c_uint32(self.handle)\\n        c_msg_len = c_uint32(len(msg))\\n\\n        result = await do_call(\\'vcx_connection_sign_data\\',\\n                               c_connection_handle,\\n                               msg,\\n                               c_msg_len,\\n                               Connection.sign_data.cb)\\n\\n        self.logger.debug(\"vcx_connection_sign_data completed\")\\n        return result', 'async def verify_signature(self, msg: bytes, signature: bytes) -> bool:\\n        \"\"\"\\n        Verification the signature of a msg\\n        :param msg:\\n        :param signature:\\n        :return: bool\\n        \"\"\"\\n        if not hasattr(Connection.verify_signature, \"cb\"):\\n            self.logger.debug(\"vcx_connection_verify_signature: Creating callback\")\\n            Connection.verify_signature.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_bool))\\n\\n        c_connection_handle = c_uint32(self.handle)\\n        c_msg_len = c_uint32(len(msg))\\n        c_signature_len = c_uint32(len(signature))\\n\\n        result = await do_call(\\'vcx_connection_verify_signature\\',\\n                               c_connection_handle,\\n                               msg,\\n                               c_msg_len,\\n                               signature,\\n                               c_signature_len,\\n                               Connection.verify_signature.cb)\\n\\n        self.logger.debug(\"vcx_connection_verify_signature completed\")\\n        return result', 'async def invite_details(self, abbreviated: bool) -> dict:\\n        \"\"\"\\n        Get the invite details that were sent or can be sent to the endpoint.\\n\\n        :param abbreviated: abbreviate invite details or not\\n        Example:\\n        phone_number = \\'8019119191\\'\\n        connection = await Connection.create(\\'foobar123\\')\\n        invite_details = await connection.connect(phone_number)\\n        inivte_details_again = await connection.invite_details()\\n        :return: JSON of invite_details sent to connection\\n        \"\"\"\\n        if not hasattr(Connection.invite_details, \"cb\"):\\n            self.logger.debug(\"vcx_connection_invite_details: Creating callback\")\\n            Connection.invite_details.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_connection_handle = c_uint32(self.handle)\\n        c_abbreviated = c_bool(abbreviated)\\n\\n        details = await do_call(\\'vcx_connection_invite_details\\',\\n                                c_connection_handle,\\n                                c_abbreviated,\\n                                Connection.invite_details.cb)\\n\\n        return json.loads(details.decode())', 'async def create(source_id: str, proof_request: str):\\n        \"\"\"\\n        Create a proof for fulfilling a corresponding proof request\\n        :param source_id: Tag associated by user of sdk\\n        :param proof_request: Proof Request data sent by requestor.\\n        Example:\\n        source_id = \\'sourceId\\'\\n        request = {\\n            \"@topic\": {\\n            \"mid\": 9,\\n            \"tid\": 1\\n            },\\n            \"@type\": {\\n                \"name\": \"PROOF_REQUEST\",\\n                \"version\":\"1.0\"\\n            },\\n            \"msg_ref_id\": \"ymy5nth\",\\n            \"proof_request_data\": {\\n                \"name\": \"Account Certificate\",\\n                \"nonce\": \"838186471541979035208225\",\\n                \"requested_attributes\": {\\n                    \"business_2\": {\\n                        \"name\": \"business\"\\n                    },\\n                    \"email_1\": {\\n                        \"name\": \"email\"\\n                    },\\n                    \"name_0\": {\\n                        \"name\": \"name\"\\n                    }\\n                },\\n                \"requested_predicates\": {},\\n                \"version\": \"0.1\"\\n            }\\n        }\\n        disclosed_proof = await DisclosedProof.create(source_id, request)\\n        :return: Disclosed Proof Object\\n        \"\"\"\\n        constructor_params = (source_id,)\\n\\n        c_source_id = c_char_p(source_id.encode(\\'utf-8\\'))\\n        c_proof_request = c_char_p(json.dumps(proof_request).encode(\\'utf-8\\'))\\n        c_params = (c_source_id, c_proof_request, )\\n\\n        return await DisclosedProof._create(\"vcx_disclosed_proof_create_with_request\",\\n                                   constructor_params,\\n                                   c_params)', 'async def deserialize(data: dict):\\n        \"\"\"\\n        :param data: Data provided by the serialize method\\n        Example:\\n        msg_id = \\'1\\'\\n        phone_number = \\'8019119191\\'\\n        connection = await Connection.create(source_id)\\n        await connection.connect(phone_number)\\n        disclosed_proof = await DisclosedProof.create_with_msgid(source_id, connection, msg_id)\\n        data = await disclosed_proof.serialize()\\n        disclosed_proof2 = await DisclosedProof.deserialize(data)\\n        :return:  DisclosedProof\\n        \"\"\"\\n        disclosed_proof = await DisclosedProof._deserialize(\"vcx_disclosed_proof_deserialize\",\\n                                                      json.dumps(data),\\n                                                      data.get(\\'data\\').get(\\'source_id\\'))\\n        return disclosed_proof', 'async def get_requests(connection: Connection) -> dict:\\n        \"\"\"\\n        Example:\\n        msg_id = \\'1\\'\\n        phone_number = \\'8019119191\\'\\n        connection = await Connection.create(source_id)\\n        await connection.connect(phone_number)\\n        disclosed_proof = await DisclosedProof.create_with_msgid(source_id, connection, msg_id)\\n        requests = await DisclosedProof.get_requests(connection)\\n        :param connection: Connection\\n        :return: requests associated with the connection\\n        \"\"\"\\n        if not hasattr(DisclosedProof.get_requests, \"cb\"):\\n            DisclosedProof.get_requests.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_connection_handle = c_uint32(connection.handle)\\n\\n        data = await do_call(\\'vcx_disclosed_proof_get_requests\\',\\n                      c_connection_handle,\\n                      DisclosedProof.get_requests.cb)\\n\\n        return json.loads(data.decode())', 'async def get_creds(self) -> dict:\\n        \"\"\"\\n        Gets the credentials from a disclosed proof\\n        Example:\\n        msg_id = \\'1\\'\\n        phone_number = \\'8019119191\\'\\n        connection = await Connection.create(source_id)\\n        await connection.connect(phone_number)\\n        disclosed_proof = await DisclosedProof.create_with_msgid(source_id, connection, msg_id)\\n        creds = await disclosed_proof.get_creds()\\n        :return: credentials\\n        \"\"\"\\n        if not hasattr(DisclosedProof.get_creds, \"cb\"):\\n            self.logger.debug(\"vcx_disclosed_proof_retrieve_credentials: Creating callback\")\\n            DisclosedProof.get_creds.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_disclosed_proof_handle = c_uint32(self.handle)\\n\\n        data = await do_call(\\'vcx_disclosed_proof_retrieve_credentials\\',\\n                             c_disclosed_proof_handle,\\n                             DisclosedProof.get_creds.cb)\\n        return json.loads(data.decode())', 'async def send_proof(self, connection: Connection):\\n        \"\"\"\\n        Sends the proof to the Connection\\n        Example:\\n        msg_id = \\'1\\'\\n        phone_number = \\'8019119191\\'\\n        connection = await Connection.create(source_id)\\n        await connection.connect(phone_number)\\n        disclosed_proof = await DisclosedProof.create_with_msgid(source_id, connection, msg_id)\\n        await disclosed_proof.send_proof(connection)\\n        :param connection: Connection\\n        :return: None\\n        \"\"\"\\n        if not hasattr(DisclosedProof.send_proof, \"cb\"):\\n            self.logger.debug(\"vcx_disclosed_proof_send_proof: Creating callback\")\\n            DisclosedProof.send_proof.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n        c_disclosed_proof_handle = c_uint32(self.handle)\\n        c_connection_handle = c_uint32(connection.handle)\\n\\n        await do_call(\\'vcx_disclosed_proof_send_proof\\',\\n                      c_disclosed_proof_handle,\\n                      c_connection_handle,\\n                      DisclosedProof.send_proof.cb)', 'async def generate_proof(self, selected_creds: dict, self_attested_attrs: dict):\\n        \"\"\"\\n        Generates the proof\\n        Example:\\n        msg_id = \\'1\\'\\n        phone_number = \\'8019119191\\'\\n        connection = await Connection.create(source_id)\\n        await connection.connect(phone_number)\\n        disclosed_proof = await DisclosedProof.create_with_msgid(source_id, connection, msg_id)\\n        await disclosed_proof.generate_proof({}, {})\\n        :param selected_creds: Credentials issued\\n        :param self_attested_attrs: Self Attested Attributes\\n        :return: None\\n        \"\"\"\\n        if not hasattr(DisclosedProof.generate_proof, \"cb\"):\\n            self.logger.debug(\"vcx_disclosed_proof_generate_proof: Creating callback\")\\n            DisclosedProof.generate_proof.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n        c_disclosed_proof_handle = c_uint32(self.handle)\\n        c_selected_creds = c_char_p(json.dumps(selected_creds).encode(\\'utf-8\\'))\\n        c_self_attested_attrs = c_char_p(json.dumps(self_attested_attrs).encode(\\'utf-8\\'))\\n\\n        await do_call(\\'vcx_disclosed_proof_generate_proof\\',\\n                      c_disclosed_proof_handle,\\n                      c_selected_creds,\\n                      c_self_attested_attrs,\\n                      DisclosedProof.generate_proof.cb)', 'async def create_payment_address(wallet_handle: int,\\n                                 payment_method: str,\\n                                 config: str) -> str:\\n    \"\"\"\\n     Create the payment address for specified payment method\\n\\n\\n     This method generates private part of payment address\\n     and stores it in a secure place. Ideally it should be\\n     secret in libindy wallet (see crypto module).\\n\\n     Note that payment method should be able to resolve this\\n     secret by fully resolvable payment address format.\\n\\n    :param wallet_handle: wallet handle (created by open_wallet).\\n    :param payment_method: Payment method to use (for example, \\'sov\\').\\n    :param config: payment address config as json:\\n       {\\n         seed: <str>, // allows deterministic creation of payment address\\n       }\\n    :return: payment_address: public identifier of payment address in fully resolvable payment address format.\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"create_payment_address: >>> wallet_handle: %r, payment_method: %r, config: %r\",\\n                 wallet_handle,\\n                 payment_method,\\n                 config)\\n\\n    if not hasattr(create_payment_address, \"cb\"):\\n        logger.debug(\"create_payment_address: Creating callback\")\\n        create_payment_address.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_payment_method = c_char_p(payment_method.encode(\\'utf-8\\'))\\n    config = c_char_p(config.encode(\\'utf-8\\'))\\n\\n    request_result = await do_call(\\'indy_create_payment_address\\',\\n                                   c_wallet_handle,\\n                                   c_payment_method,\\n                                   config,\\n                                   create_payment_address.cb)\\n\\n    res = request_result.decode()\\n    logger.debug(\"create_payment_address: <<< res: %r\", res)\\n    return res', 'async def build_get_payment_sources_request(wallet_handle: int,\\n                                            submitter_did: str,\\n                                            payment_address: str) -> (str, str):\\n    \"\"\"\\n    Builds Indy request for getting sources list for payment address\\n    according to this payment method.\\n\\n    :param wallet_handle: wallet handle (created by open_wallet).\\n    :param submitter_did : (Option) DID of request sender\\n    :param payment_address: target payment address\\n    :return: get_sources_txn_json: Indy request for getting sources list for payment address\\n             payment_method: used payment method\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_get_payment_sources_request: >>> wallet_handle: %r, submitter_did: %r, payment_address: %r\",\\n                 wallet_handle,\\n                 submitter_did,\\n                 payment_address)\\n\\n    if not hasattr(build_get_payment_sources_request, \"cb\"):\\n        logger.debug(\"build_get_payment_sources_request: Creating callback\")\\n        build_get_payment_sources_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_payment_address = c_char_p(payment_address.encode(\\'utf-8\\'))\\n\\n    (get_sources_txn_json, payment_method) = await do_call(\\'indy_build_get_payment_sources_request\\',\\n                                                           c_wallet_handle,\\n                                                           c_submitter_did,\\n                                                           c_payment_address,\\n                                                           build_get_payment_sources_request.cb)\\n    res = (get_sources_txn_json.decode(), payment_method.decode())\\n\\n    logger.debug(\"build_get_payment_sources_request: <<< res: %r\", res)\\n    return res', 'async def build_payment_req(wallet_handle: int,\\n                            submitter_did: str,\\n                            inputs_json: str,\\n                            outputs_json: str,\\n                            extra: Optional[str]) -> (str, str):\\n    \"\"\"\\n    Builds Indy request for doing payment\\n    according to this payment method.\\n   \\n    This method consumes set of inputs and outputs.\\n   \\n    Format of inputs is specific for payment method. Usually it should reference payment transaction\\n    with at least one output that corresponds to payment address that user owns.\\n\\n    :param wallet_handle: wallet handle (created by open_wallet).\\n    :param submitter_did : (Option) DID of request sender\\n    :param inputs_json: The list of payment sources as json array:\\n      [\"source1\", ...]\\n      Note that each source should reference payment address    \\n    :param outputs_json: The list of outputs as json array:\\n      [{\\n        recipient: <str>, // payment address of recipient\\n        amount: <int>, // amount\\n      }]\\n    :param extra: // optional information for payment operation\\n\\n    :return: payment_req_json: Indy request for doing payment\\n             payment_method: used payment method\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_payment_req: >>> wallet_handle: %r, submitter_did: %r, inputs_json: %r, outputs_json: %r,\"\\n                 \" extra: %r\",\\n                 wallet_handle,\\n                 submitter_did,\\n                 inputs_json,\\n                 outputs_json,\\n                 extra)\\n\\n    if not hasattr(build_payment_req, \"cb\"):\\n        logger.debug(\"build_payment_req: Creating callback\")\\n        build_payment_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_inputs_json = c_char_p(inputs_json.encode(\\'utf-8\\'))\\n    c_outputs_json = c_char_p(outputs_json.encode(\\'utf-8\\'))\\n    c_extra = c_char_p(extra.encode(\\'utf-8\\')) if extra is not None else None\\n\\n    (payment_req_json, payment_method) = await do_call(\\'indy_build_payment_req\\',\\n                                                       c_wallet_handle,\\n                                                       c_submitter_did,\\n                                                       c_inputs_json,\\n                                                       c_outputs_json,\\n                                                       c_extra,\\n                                                       build_payment_req.cb)\\n    res = (payment_req_json.decode(), payment_method.decode())\\n\\n    logger.debug(\"build_payment_req: <<< res: %r\", res)\\n    return res', 'async def build_set_txn_fees_req(wallet_handle: int,\\n                                 submitter_did: str,\\n                                 payment_method: str,\\n                                 fees_json: str) -> str:\\n    \"\"\"\\n    Builds Indy request for setting fees for transactions in the ledger\\n\\n    :param wallet_handle: wallet handle (created by open_wallet).\\n    :param submitter_did : (Option) DID of request sender\\n    :param payment_method: Payment method to use (for example, \\'sov\\').\\n    :param fees_json: {\\n       txnType1: amount1,\\n       txnType2: amount2,\\n       .................\\n       txnTypeN: amountN,\\n     }\\n    :return: set_txn_fees_json: Indy request for setting fees for transactions in the ledger\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_set_txn_fees_req: >>> wallet_handle: %r, submitter_did: %r, payment_method: %r, fees_json: %r\",\\n                 wallet_handle,\\n                 submitter_did,\\n                 payment_method,\\n                 fees_json)\\n\\n    if not hasattr(build_set_txn_fees_req, \"cb\"):\\n        logger.debug(\"build_set_txn_fees_req: Creating callback\")\\n        build_set_txn_fees_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_payment_method = c_char_p(payment_method.encode(\\'utf-8\\'))\\n    c_fees_json = c_char_p(fees_json.encode(\\'utf-8\\'))\\n\\n    set_txn_fees_json = await do_call(\\'indy_build_set_txn_fees_req\\',\\n                                      c_wallet_handle,\\n                                      c_submitter_did,\\n                                      c_payment_method,\\n                                      c_fees_json,\\n                                      build_set_txn_fees_req.cb)\\n\\n    res = set_txn_fees_json.decode()\\n    logger.debug(\"build_set_txn_fees_req: <<< res: %r\", res)\\n    return res', 'async def build_get_txn_fees_req(wallet_handle: int,\\n                                 submitter_did: str,\\n                                 payment_method: str) -> str:\\n    \"\"\"\\n    Builds Indy request for getting fees for transactions in the ledger\\n\\n    :param wallet_handle: wallet handle (created by open_wallet).\\n    :param submitter_did : (Option) DID of request sender\\n    :param payment_method: Payment method to use (for example, \\'sov\\').\\n    :return: set_txn_fees_json: Indy request for setting fees for transactions in the ledger\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_get_txn_fees_req: >>> wallet_handle: %r, submitter_did: %r, payment_method: %r\",\\n                 wallet_handle,\\n                 submitter_did,\\n                 payment_method)\\n\\n    if not hasattr(build_get_txn_fees_req, \"cb\"):\\n        logger.debug(\"build_get_txn_fees_req: Creating callback\")\\n        build_get_txn_fees_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_payment_method = c_char_p(payment_method.encode(\\'utf-8\\'))\\n\\n    get_txn_fees_json = await do_call(\\'indy_build_get_txn_fees_req\\',\\n                                      c_wallet_handle,\\n                                      c_submitter_did,\\n                                      c_payment_method,\\n                                      build_get_txn_fees_req.cb)\\n\\n    res = get_txn_fees_json.decode()\\n    logger.debug(\"build_get_txn_fees_req: <<< res: %r\", res)\\n    return res', 'async def parse_get_txn_fees_response(payment_method: str,\\n                                      resp_json: str) -> str:\\n    \"\"\"\\n    Parses response for Indy request for getting fees\\n\\n    :param payment_method: Payment method to use (for example, \\'sov\\').\\n    :param resp_json: response for Indy request for getting fees\\n    :return: fees_json: {\\n       txnType1: amount1,\\n       txnType2: amount2,\\n       .................\\n       txnTypeN: amountN,\\n     }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"parse_get_txn_fees_response: >>> payment_method: %r, resp_json: %r\",\\n                 payment_method,\\n                 resp_json)\\n\\n    if not hasattr(parse_get_txn_fees_response, \"cb\"):\\n        logger.debug(\"parse_get_txn_fees_response: Creating callback\")\\n        parse_get_txn_fees_response.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_payment_method = c_char_p(payment_method.encode(\\'utf-8\\'))\\n    c_resp_json = c_char_p(resp_json.encode(\\'utf-8\\'))\\n\\n    fees_json = await do_call(\\'indy_parse_get_txn_fees_response\\',\\n                              c_payment_method,\\n                              c_resp_json,\\n                              parse_get_txn_fees_response.cb)\\n\\n    res = fees_json.decode()\\n    logger.debug(\"parse_get_txn_fees_response: <<< res: %r\", res)\\n    return res', 'async def build_verify_payment_req(wallet_handle: int,\\n                                   submitter_did: str,\\n                                   receipt: str) -> (str, str):\\n    \"\"\"\\n    Builds Indy request for information to verify the payment receipt\\n\\n    :param wallet_handle: wallet handle (created by open_wallet).\\n    :param submitter_did : (Option) DID of request sender\\n    :param receipt: payment receipt to verify\\n\\n    :return: verify_txn_json: Indy request for verification receipt for transactions in the ledger\\n             payment_method: used payment method\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"build_verify_payment_req: >>> wallet_handle: %r, submitter_did: %r, receipt: %r\",\\n                 wallet_handle,\\n                 submitter_did,\\n                 receipt)\\n\\n    if not hasattr(build_verify_payment_req, \"cb\"):\\n        logger.debug(\"build_verify_payment_req: Creating callback\")\\n        build_verify_payment_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_submitter_did = c_char_p(submitter_did.encode(\\'utf-8\\')) if submitter_did is not None else None\\n    c_receipt = c_char_p(receipt.encode(\\'utf-8\\'))\\n\\n    (verify_txn_json, payment_method) = await do_call(\\'indy_build_verify_payment_req\\',\\n                                                      c_wallet_handle,\\n                                                      c_submitter_did,\\n                                                      c_receipt,\\n                                                      build_verify_payment_req.cb)\\n    res = (verify_txn_json.decode(), payment_method.decode())\\n\\n    logger.debug(\"build_verify_payment_req: <<< res: %r\", res)\\n    return res', 'async def create(source_id: str, name: str, version: str, attrs: list, payment_handle: int):\\n        \"\"\"\\n        Creates a new schema object that is written to the ledger\\n\\n        :param source_id: Institution\\'s unique ID for the schema\\n        :param name: Name of schema\\n        :param version: Version of the schema\\n        :param attrs: Atttributes of the schema\\n        :param payment_handle: NYI - payment of ledger fee is taken from wallet automatically\\n        Example:\\n        source_id = \\'foobar123\\'\\n        name = \\'Address Schema\\'\\n        version = \\'1.0\\'\\n        attrs = [\\'address\\', \\'city\\', \\'state\\']\\n        payment_handle = 0\\n        schema = await Schema.create(source_id, name, version, attrs, payment_handle)\\n        :return: schema object, written to ledger\\n        \"\"\"\\n        constructor_params = (source_id, name, version, attrs)\\n\\n        c_source_id = c_char_p(source_id.encode(\\'utf-8\\'))\\n        c_name = c_char_p(name.encode(\\'utf-8\\'))\\n        c_version = c_char_p(version.encode(\\'utf-8\\'))\\n        c_schema_data = c_char_p(json.dumps(attrs).encode(\\'utf-8\\'))\\n        c_payment = c_uint32(payment_handle)\\n        c_params = (c_source_id, c_name, c_version, c_schema_data, c_payment)\\n\\n        schema = await Schema._create(\"vcx_schema_create\", constructor_params, c_params)\\n        schema.schema_id = await schema.get_schema_id()\\n        return schema', 'async def deserialize(data: dict):\\n        \"\"\"\\n        Create the object from a previously serialized object.\\n\\n        :param data: The output of the \"serialize\" call\\n        Example:\\n        source_id = \\'foobar123\\'\\n        name = \\'Address Schema\\'\\n        version = \\'1.0\\'\\n        attrs = [\\'address\\', \\'city\\', \\'state\\']\\n        payment_handle = 0\\n        schema1 = await Schema.create(source_id, name, version, attrs, payment_handle)\\n        data1 = await schema1.serialize()\\n        :return: A re-instantiated object\\n        \"\"\"\\n        try:\\n            # Todo: Find better way to access attr_names. Potential for issues.\\n            schema = await Schema._deserialize(\"vcx_schema_deserialize\",\\n                                               json.dumps(data),\\n                                               data[\\'data\\'][\\'source_id\\'],\\n                                               data[\\'data\\'][\\'name\\'],\\n                                               data[\\'data\\'][\\'version\\'],\\n                                               data[\\'data\\'][\\'data\\'])\\n\\n            schema.schema_id = await schema.get_schema_id()\\n            return schema\\n        except KeyError:\\n            raise VcxError(ErrorCode.InvalidSchema)', 'async def lookup(source_id: str, schema_id: str):\\n        \"\"\"\\n        Create a new schema object from an existing ledger schema\\n\\n        :param source_id: Institution\\'s personal identification for the schema\\n        :param schema_id: Ledger schema ID for lookup\\n        Example:\\n        source_id = \\'foobar123\\'\\n        name = \\'Address Schema\\'\\n        version = \\'1.0\\'\\n        attrs = [\\'address\\', \\'city\\', \\'state\\']\\n        payment_handle = 0\\n        schema1 = await Schema.create(source_id, name, version, attrs, payment_handle)\\n        id1 = await schema.get_schema_id()\\n        data = await Schema.lookup(source_id, schema_id)\\n        assert data.attrs.sort() == [\\'sex\\', \\'age\\', \\'name\\', \\'height\\'].sort()\\n        assert data.name == \\'test-licence\\'\\n        assert data.handle > 0\\n        :return: schema object\\n        \"\"\"\\n        try:\\n            schema = Schema(source_id, \\'\\', \\'\\', [])\\n\\n            if not hasattr(Schema.lookup, \"cb\"):\\n                schema.logger.debug(\"vcx_schema_get_attributes: Creating callback\")\\n                Schema.lookup.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_uint32, c_char_p))\\n\\n            c_source_id = c_char_p(source_id.encode(\\'utf-8\\'))\\n            c_schema_id = c_char_p(schema_id.encode(\\'utf-8\\'))\\n\\n            handle, data = await do_call(\\'vcx_schema_get_attributes\\',\\n                                         c_source_id,\\n                                         c_schema_id,\\n                                         Schema.lookup.cb)\\n            schema.logger.debug(\"created schema object\")\\n\\n            schema_result = json.loads(data.decode())\\n            schema.attrs = schema_result[\\'data\\']\\n            schema.name = schema_result[\\'name\\']\\n            schema.version = schema_result[\\'version\\']\\n            schema.handle = handle\\n            return schema\\n        except KeyError:\\n            raise VcxError(ErrorCode.InvalidSchema)', 'async def get_schema_id(self):\\n        \"\"\"\\n        Get the ledger ID of the object\\n\\n        Example:\\n        source_id = \\'foobar123\\'\\n        name = \\'Address Schema\\'\\n        version = \\'1.0\\'\\n        attrs = [\\'address\\', \\'city\\', \\'state\\']\\n        payment_handle = 0\\n        schema1 = await Schema.create(source_id, name, version, attrs, payment_handle)\\n        id1 = await schema.get_schema_id()\\n        :return: ID string\\n        \"\"\"\\n        cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n        c_handle = c_uint32(self.handle)\\n        id = await do_call(\\'vcx_schema_get_schema_id\\', c_handle, cb)\\n        return id.decode()', 'async def is_pairwise_exists(wallet_handle: int,\\n                             their_did: str) -> bool:\\n    \"\"\"\\n    Check if pairwise is exists.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param their_did: encoded Did.\\n    :return: true - if pairwise is exists, false - otherwise\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"is_pairwise_exists: >>> wallet_handle: %r, their_did: %r\",\\n                 wallet_handle,\\n                 their_did)\\n\\n    if not hasattr(is_pairwise_exists, \"cb\"):\\n        logger.debug(\"is_pairwise_exists: Creating callback\")\\n        is_pairwise_exists.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_bool))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_their_did = c_char_p(their_did.encode(\\'utf-8\\'))\\n\\n    res = await do_call(\\'indy_is_pairwise_exists\\',\\n                        c_wallet_handle,\\n                        c_their_did,\\n                        is_pairwise_exists.cb)\\n\\n    logger.debug(\"is_pairwise_exists: <<< res: %r\", res)\\n    return res', 'async def list_pairwise(wallet_handle: int) -> str:\\n    \"\"\"\\n    Get list of saved pairwise.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :return: pairwise_list: list of saved pairwise\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"list_pairwise: >>> wallet_handle: %r\", wallet_handle)\\n\\n    if not hasattr(list_pairwise, \"cb\"):\\n        logger.debug(\"list_pairwise: Creating callback\")\\n        list_pairwise.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n\\n    pairwise_list = await do_call(\\'indy_list_pairwise\\',\\n                                  c_wallet_handle,\\n                                  list_pairwise.cb)\\n\\n    res = pairwise_list.decode()\\n    logger.debug(\"list_pairwise: <<< res: %r\", res)\\n    return res', 'async def get_pairwise(wallet_handle: int,\\n                       their_did: str) -> None:\\n    \"\"\"\\n    Gets pairwise information for specific their_did.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param their_did: encoded Did\\n    :return: pairwise_info_json: did info associated with their did\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"get_pairwise: >>> wallet_handle: %r, their_did: %r\",\\n                 wallet_handle,\\n                 their_did)\\n\\n    if not hasattr(get_pairwise, \"cb\"):\\n        logger.debug(\"get_pairwise: Creating callback\")\\n        get_pairwise.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_their_did = c_char_p(their_did.encode(\\'utf-8\\'))\\n\\n    pairwise_info_json = await do_call(\\'indy_get_pairwise\\',\\n                                       c_wallet_handle,\\n                                       c_their_did,\\n                                       get_pairwise.cb)\\n\\n    res = pairwise_info_json.decode()\\n    logger.debug(\"get_pairwise: <<< res: %r\", res)\\n    return res', 'async def set_pairwise_metadata(wallet_handle: int,\\n                                their_did: str,\\n                                metadata: Optional[str]) -> None:\\n    \"\"\"\\n    Save some data in the Wallet for pairwise associated with Did.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param their_did: encoded DID\\n    :param metadata: some extra information for pairwise\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"set_pairwise_metadata: >>> wallet_handle: %r, their_did: %r, metadata: %r\",\\n                 wallet_handle,\\n                 their_did,\\n                 metadata)\\n\\n    if not hasattr(set_pairwise_metadata, \"cb\"):\\n        logger.debug(\"set_pairwise_metadata: Creating callback\")\\n        set_pairwise_metadata.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_their_did = c_char_p(their_did.encode(\\'utf-8\\'))\\n    c_metadata = c_char_p(metadata.encode(\\'utf-8\\')) if metadata is not None else None\\n\\n    await do_call(\\'indy_set_pairwise_metadata\\',\\n                  c_wallet_handle,\\n                  c_their_did,\\n                  c_metadata,\\n                  set_pairwise_metadata.cb)\\n\\n    logger.debug(\"set_pairwise_metadata: <<<\")', 'def print_log(value_color=\"\", value_noncolor=\"\"):\\n    \"\"\"set the colors for text.\"\"\"\\n    HEADER = \\'\\\\033[92m\\'\\n    ENDC = \\'\\\\033[0m\\'\\n    print(HEADER + value_color + ENDC + str(value_noncolor))', 'def set_runtime_config(config: str):\\n    \"\"\"\\n     Set libindy runtime configuration. Can be optionally called to change current params.\\n\\n     :param config: {\\n      \"crypto_thread_pool_size\": Optional<int> - size of thread pool for the most expensive crypto operations. (4 by default)\\n      \"collect_backtrace\": Optional<bool> - whether errors backtrace should be collected.\\n          Capturing of backtrace can affect library performance.\\n          NOTE: must be set before invocation of any other API functions.\\n      }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"set_runtime_config: >>> config: %r\", config)\\n\\n    c_config = c_char_p(config.encode(\\'utf-8\\'))\\n\\n    do_call_sync(\\'indy_set_runtime_config\\',\\n                 c_config)\\n\\n    logger.debug(\"set_runtime_config: <<<\")', 'async def create(source_id: str, name: str, requested_attrs: list, revocation_interval: dict, requested_predicates: list = []):\\n        \"\"\"\\n         Builds a generic proof object\\n        :param source_id: Tag associated by user of sdk\\n        :param name: Name of the Proof\\n        :param requested_attrs: Attributes associated with the Proof\\n        :param revocation_interval: interval applied to all requested attributes indicating when the claim must be valid (NOT revoked)\\n        Example:\\n        name = \"proof name\"\\n        requested_attrs = [{\"name\": \"age\", \"restrictions\": [{\"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\" } ] }, { \"name\":\"name\", \"restrictions\": [ { \"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\"}]}]\\n        revocation_interval = {\"from\": 1, \"to\": 2}  // Both values are optional\\n        proof = await Proof.create(source_id, name, requested_attrs)\\n        :return: Proof Object\\n        \"\"\"\\n        constructor_params = (source_id,)\\n\\n        c_source_id = c_char_p(source_id.encode(\\'utf-8\\'))\\n        c_name = c_char_p(name.encode(\\'utf-8\\'))\\n        c_req_predicates = c_char_p(json.dumps(requested_predicates).encode(\\'utf-8\\'))\\n        c_req_attrs = c_char_p(json.dumps(requested_attrs).encode(\\'utf-8\\'))\\n        c_revocation_interval = c_char_p(json.dumps(revocation_interval).encode(\\'utf-8\\'))\\n        c_params = (c_source_id, c_req_attrs, c_req_predicates, c_revocation_interval, c_name)\\n\\n        return await Proof._create(\"vcx_proof_create\",\\n                                   constructor_params,\\n                                   c_params)', 'async def deserialize(data: dict):\\n        \"\"\"\\n        Builds a Proof object with defined attributes.\\n        Attributes are provided by a previous call to the serialize function.\\n        :param data:\\n        Example:\\n        name = \"proof name\"\\n        requested_attrs = [{\"name\": \"age\", \"restrictions\": [{\"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\" } ] }, { \"name\":\"name\", \"restrictions\": [ { \"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\"}]}]\\n        proof = await Proof.create(source_id, name, requested_attrs)\\n        data = proof.serialize()\\n        proof2 = await Proof.deserialize(data)\\n        :return: Proof Object\\n        \"\"\"\\n        return await Proof._deserialize(\"vcx_proof_deserialize\",\\n                                        json.dumps(data),\\n                                        data.get(\\'data\\').get(\\'source_id\\'))', 'async def request_proof(self, connection: Connection):\\n        \"\"\"\\n        Example:\\n        connection = await Connection.create(source_id)\\n        await connection.connect(phone_number)\\n        name = \"proof name\"\\n        requested_attrs = [{\"name\": \"age\", \"restrictions\": [{\"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\" } ] }, { \"name\":\"name\", \"restrictions\": [ { \"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\"}]}]\\n        proof = await Proof.create(source_id, name, requested_attrs)\\n        await proof.request_proof(connection)\\n        :param connection: Connection\\n        :return:\\n        \"\"\"\\n        if not hasattr(Proof.request_proof, \"cb\"):\\n            self.logger.debug(\"vcx_proof_send_request: Creating callback\")\\n            Proof.request_proof.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n        c_proof_handle = c_uint32(self.handle)\\n        c_connection_handle = c_uint32(connection.handle)\\n\\n        await do_call(\\'vcx_proof_send_request\\',\\n                      c_proof_handle,\\n                      c_connection_handle,\\n                      Proof.request_proof.cb)', 'async def get_proof(self, connection: Connection) -> list:\\n        \"\"\"\\n        Example:\\n        connection = await Connection.create(source_id)\\n        await connection.connect(phone_number)\\n        name = \"proof name\"\\n        requested_attrs = [{\"name\": \"age\", \"restrictions\": [{\"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\" } ] }, { \"name\":\"name\", \"restrictions\": [ { \"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\"}]}]\\n        proof = await Proof.create(source_id, name, requested_attrs)\\n        await proof.request_proof(connection)\\n        await proof.get_proof(connection)\\n        :param connection: Handle for the connection to receive a proof from.\\n        :return: List of proofs received from the given connection.\\n        \"\"\"\\n        if not hasattr(Proof.get_proof, \"cb\"):\\n            self.logger.debug(\"vcx_get_proof: Creating callback\")\\n            Proof.get_proof.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_uint32, c_char_p))\\n\\n        c_proof_handle = c_uint32(self.handle)\\n        c_connection_handle = c_uint32(connection.handle)\\n\\n        proof_state, proof = await do_call(\\'vcx_get_proof\\',\\n                                           c_proof_handle,\\n                                           c_connection_handle,\\n                                           Proof.get_proof.cb)\\n        self.proof_state = proof_state\\n        return json.loads(proof.decode())', 'async def vcx_agent_provision(config: str) -> None:\\n    \"\"\"\\n    Provision an agent in the agency, populate configuration and wallet for this agent.\\n    Example:\\n    import json\\n    enterprise_config = {\\n        \\'agency_url\\': \\'http://localhost:8080\\',\\n        \\'agency_did\\': \\'VsKV7grR1BUE29mG2Fm2kX\\',\\n        \\'agency_verkey\\': \"Hezce2UWMZ3wUhVkh2LfKSs8nDzWwzs2Win7EzNN3YaR\",\\n        \\'wallet_name\\': \\'LIBVCX_SDK_WALLET\\',\\n        \\'agent_seed\\': \\'00000000000000000000000001234561\\',\\n        \\'enterprise_seed\\': \\'000000000000000000000000Trustee1\\',\\n        \\'wallet_key\\': \\'1234\\'\\n    }\\n    vcx_config = await vcx_agent_provision(json.dumps(enterprise_config))\\n    :param config: JSON configuration\\n    :return: Configuration for vcx_init call.\\n    \"\"\"\\n    logger = logging.getLogger(__name__)\\n\\n    if not hasattr(vcx_agent_provision, \"cb\"):\\n        logger.debug(\"vcx_agent_provision: Creating callback\")\\n        vcx_agent_provision.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n    c_config = c_char_p(config.encode(\\'utf-8\\'))\\n\\n    result = await do_call(\\'vcx_agent_provision_async\\',\\n                           c_config,\\n                           vcx_agent_provision.cb)\\n\\n    logger.debug(\"vcx_agent_provision completed\")\\n    return result.decode()', 'async def vcx_ledger_get_fees() -> str:\\n    \"\"\"\\n    Get ledger fees from the sovrin network\\n    Example:\\n    fees = await vcx_ledger_get_fees()\\n    :return: JSON representing fees\\n    \"\"\"\\n    logger = logging.getLogger(__name__)\\n\\n    if not hasattr(vcx_ledger_get_fees, \"cb\"):\\n        logger.debug(\"vcx_ledger_get_fees: Creating callback\")\\n        vcx_ledger_get_fees.cb = create_cb(CFUNCTYPE(None, c_uint32))\\n\\n    result = await do_call(\\'vcx_ledger_get_fees\\',\\n                           vcx_ledger_get_fees.cb)\\n\\n    logger.debug(\"vcx_ledger_get_fees completed\")\\n    return result', 'async def vcx_messages_download(status: str = None, uids: str = None, pw_dids: str = None) -> str:\\n    \"\"\"\\n    Retrieve messages from the specified connection\\n    :param status:\\n    :param uids:\\n    :param pw_dids:\\n    :return:\\n    \"\"\"\\n    logger = logging.getLogger(__name__)\\n\\n    if not hasattr(vcx_messages_download, \"cb\"):\\n        logger.debug(\"vcx_messages_download: Creating callback\")\\n        vcx_messages_download.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n    if status:\\n        c_status = c_char_p(status.encode(\\'utf-8\\'))\\n    else:\\n        c_status = None\\n\\n    if uids:\\n        c_uids = c_char_p(uids.encode(\\'utf-8\\'))\\n    else:\\n        c_uids = None\\n\\n    if pw_dids:\\n        c_pw_dids = c_char_p(pw_dids.encode(\\'utf-8\\'))\\n    else:\\n        c_pw_dids = None\\n\\n    result = await do_call(\\'vcx_messages_download\\',\\n                           c_status,\\n                           c_uids,\\n                           c_pw_dids,\\n                           vcx_messages_download.cb)\\n\\n    logger.debug(\"vcx_messages_download completed\")\\n    return result', 'async def vcx_messages_update_status(msg_json: str):\\n    \"\"\"\\n    Update the status of messages from the specified connection\\n    :param msg_json:\\n    :return:\\n    \"\"\"\\n    logger = logging.getLogger(__name__)\\n\\n    if not hasattr(vcx_messages_update_status, \"cb\"):\\n        logger.debug(\"vcx_messages_update_status: Creating callback\")\\n        vcx_messages_update_status.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n    c_msg_json = c_char_p(msg_json.encode(\\'utf-8\\'))\\n    c_status = c_char_p(\"MS-106\".encode(\\'utf-8\\'))\\n\\n    result = await do_call(\\'vcx_messages_update_status\\',\\n                           c_status,\\n                           c_msg_json,\\n                           vcx_messages_update_status.cb)\\n\\n    logger.debug(\"vcx_messages_update_status completed\")\\n    return result', 'async def crypto_sign(wallet_handle: int,\\n                      signer_vk: str,\\n                      msg: bytes) -> bytes:\\n    \"\"\"\\n    Signs a message with a key.\\n\\n    Note to use DID keys with this function you can call indy_key_for_did to get key id (verkey) for specific DID.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param signer_vk:  id (verkey) of my key. The key must be created by calling create_key or create_and_store_my_did\\n    :param msg: a message to be signed\\n    :return: a signature string\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"crypto_sign: >>> wallet_handle: %r, signer_vk: %r, msg: %r\",\\n                 wallet_handle,\\n                 signer_vk,\\n                 msg)\\n\\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\\n        return bytes(arr_ptr[:arr_len]),\\n\\n    if not hasattr(crypto_sign, \"cb\"):\\n        logger.debug(\"crypto_sign: Creating callback\")\\n        crypto_sign.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_signer_vk = c_char_p(signer_vk.encode(\\'utf-8\\'))\\n    c_msg_len = c_uint32(len(msg))\\n\\n    signature = await do_call(\\'indy_crypto_sign\\',\\n                              c_wallet_handle,\\n                              c_signer_vk,\\n                              msg,\\n                              c_msg_len,\\n                              crypto_sign.cb)\\n\\n    logger.debug(\"crypto_sign: <<< res: %r\", signature)\\n    return signature', 'async def crypto_verify(signer_vk: str,\\n                        msg: bytes,\\n                        signature: bytes) -> bool:\\n    \"\"\"\\n    Verify a signature with a verkey.\\n\\n    Note to use DID keys with this function you can call indy_key_for_did to get key id (verkey) for specific DID.\\n\\n    :param signer_vk: verkey of signer of the message\\n    :param msg: message that has been signed\\n    :param signature: a signature to be verified\\n    :return: valid: true - if signature is valid, false - otherwise\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"crypto_verify: >>> my_vk: %r, signed_msg: %r, signature: %r\",\\n                 signer_vk,\\n                 msg,\\n                 signature)\\n\\n    if not hasattr(crypto_verify, \"cb\"):\\n        logger.debug(\"crypto_verify: Creating callback\")\\n        crypto_verify.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_bool))\\n\\n    c_signer_vk = c_char_p(signer_vk.encode(\\'utf-8\\'))\\n    c_msg_len = c_uint32(len(msg))\\n    c_signature_len = c_uint32(len(signature))\\n\\n    res = await do_call(\\'indy_crypto_verify\\',\\n                        c_signer_vk,\\n                        msg,\\n                        c_msg_len,\\n                        signature,\\n                        c_signature_len,\\n                        crypto_verify.cb)\\n\\n    logger.debug(\"crypto_verify: <<< res: %r\", res)\\n    return res', 'async def auth_crypt(wallet_handle: int,\\n                     sender_vk: str,\\n                     recipient_vk: str,\\n                     msg: bytes) -> bytes:\\n    \"\"\"\\n    **** THIS FUNCTION WILL BE DEPRECATED USE pack_message INSTEAD ****\\n\\n    Encrypt a message by authenticated-encryption scheme.\\n\\n    Sender can encrypt a confidential message specifically for Recipient, using Sender\\'s public key.\\n    Using Recipient\\'s public key, Sender can compute a shared secret key.\\n    Using Sender\\'s public key and his secret key, Recipient can compute the exact same shared secret key.\\n    That shared secret key can be used to verify that the encrypted message was not tampered with,\\n    before eventually decrypting it.\\n\\n    Note to use DID keys with this function you can call indy_key_for_did to get key id (verkey)\\n    for specific DID.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param sender_vk: id (verkey) of my key. The key must be created by calling indy_create_key or\\n    indy_create_and_store_my_did\\n    :param recipient_vk: id (verkey) of their key\\n    :param msg: a message to be signed\\n    :return: encrypted message as an array of bytes\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"auth_crypt: >>> wallet_handle: %r,sender_vk: %r, recipient_vk: %r, msg: %r\",\\n                 wallet_handle,\\n                 sender_vk,\\n                 recipient_vk,\\n                 msg)\\n\\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\\n        return bytes(arr_ptr[:arr_len]),\\n\\n    if not hasattr(auth_crypt, \"cb\"):\\n        logger.debug(\"auth_crypt: Creating callback\")\\n        auth_crypt.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_sender_vk = c_char_p(sender_vk.encode(\\'utf-8\\'))\\n    c_recipient_vk = c_char_p(recipient_vk.encode(\\'utf-8\\'))\\n    c_msg_len = c_uint32(len(msg))\\n\\n    res = await do_call(\\'indy_crypto_auth_crypt\\',\\n                        c_wallet_handle,\\n                        c_sender_vk,\\n                        c_recipient_vk,\\n                        msg,\\n                        c_msg_len,\\n                        auth_crypt.cb)\\n\\n    logger.debug(\"auth_crypt: <<< res: %r\", res)\\n    return res', 'async def auth_decrypt(wallet_handle: int,\\n                       recipient_vk: str,\\n                       encrypted_msg: bytes) -> (str, bytes):\\n    \"\"\"\\n    **** THIS FUNCTION WILL BE DEPRECATED USE unpack_message INSTEAD ****\\n\\n    Decrypt a message by authenticated-encryption scheme.\\n\\n    Sender can encrypt a confidential message specifically for Recipient, using Sender\\'s public key.\\n    Using Recipient\\'s public key, Sender can compute a shared secret key.\\n    Using Sender\\'s public key and his secret key, Recipient can compute the exact same shared secret key.\\n    That shared secret key can be used to verify that the encrypted message was not tampered with,\\n    before eventually decrypting it.\\n\\n    Note to use DID keys with this function you can call key_for_did to get key id (verkey)\\n    for specific DID.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param recipient_vk: id (verkey) of my key. The key must be created by calling create_key or create_and_store_my_did\\n    :param encrypted_msg: encrypted message\\n    :return: sender verkey and decrypted message\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"auth_decrypt: >>> wallet_handle: %r, recipient_vk: %r, encrypted_msg: %r\",\\n                 wallet_handle,\\n                 recipient_vk,\\n                 encrypted_msg)\\n\\n    def transform_cb(key: c_char_p, arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\\n        return key, bytes(arr_ptr[:arr_len]),\\n\\n    if not hasattr(auth_decrypt, \"cb\"):\\n        logger.debug(\"crypto_box_open: Creating callback\")\\n        auth_decrypt.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, POINTER(c_uint8), c_uint32),\\n                                    transform_cb)\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_recipient_vk = c_char_p(recipient_vk.encode(\\'utf-8\\'))\\n    c_encrypted_msg_len = c_uint32(len(encrypted_msg))\\n\\n    (sender_vk, msg) = await do_call(\\'indy_crypto_auth_decrypt\\',\\n                                     c_wallet_handle,\\n                                     c_recipient_vk,\\n                                     encrypted_msg,\\n                                     c_encrypted_msg_len,\\n                                     auth_decrypt.cb)\\n\\n    sender_vk = sender_vk.decode()\\n    res = (sender_vk, msg)\\n\\n    logger.debug(\"auth_decrypt: <<< res: %r\", res)\\n    return res', 'async def anon_crypt(recipient_vk: str,\\n                     msg: bytes) -> bytes:\\n    \"\"\"\\n    Encrypts a message by anonymous-encryption scheme.\\n\\n    Sealed boxes are designed to anonymously send messages to a Recipient given its public key.\\n    Only the Recipient can decrypt these messages, using its private key.\\n    While the Recipient can verify the integrity of the message, it cannot verify the identity of the Sender.\\n\\n    Note to use DID keys with this function you can call key_for_did to get key id (verkey)\\n    for specific DID.\\n\\n    Note: use pack_message function for A2A goals.\\n\\n    :param recipient_vk: verkey of message recipient\\n    :param msg: a message to be signed\\n    :return: an encrypted message as an array of bytes\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"anon_crypt: >>> recipient_vk: %r, msg: %r\",\\n                 recipient_vk,\\n                 msg)\\n\\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\\n        return bytes(arr_ptr[:arr_len]),\\n\\n    if not hasattr(anon_crypt, \"cb\"):\\n        logger.debug(\"anon_crypt: Creating callback\")\\n        anon_crypt.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\\n\\n    c_recipient_vk = c_char_p(recipient_vk.encode(\\'utf-8\\'))\\n    c_msg_len = c_uint32(len(msg))\\n\\n    encrypted_message = await do_call(\\'indy_crypto_anon_crypt\\',\\n                                      c_recipient_vk,\\n                                      msg,\\n                                      c_msg_len,\\n                                      anon_crypt.cb)\\n    res = encrypted_message\\n    logger.debug(\"anon_crypt: <<< res: %r\", res)\\n    return res', 'async def anon_decrypt(wallet_handle: int,\\n                       recipient_vk: str,\\n                       encrypted_msg: bytes) -> bytes:\\n    \"\"\"\\n    Decrypts a message by anonymous-encryption scheme.\\n\\n    Sealed boxes are designed to anonymously send messages to a Recipient given its public key.\\n    Only the Recipient can decrypt these messages, using its private key.\\n    While the Recipient can verify the integrity of the message, it cannot verify the identity of the Sender.\\n\\n    Note to use DID keys with this function you can call key_for_did to get key id (verkey)\\n    for specific DID.\\n\\n    Note: use unpack_message function for A2A goals.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param recipient_vk: id (verkey) of my key. The key must be created by calling indy_create_key or create_and_store_my_did\\n    :param encrypted_msg: encrypted message\\n    :return: decrypted message as an array of bytes\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"anon_decrypt: >>> wallet_handle: %r, recipient_vk: %r, encrypted_msg: %r\",\\n                 wallet_handle,\\n                 recipient_vk,\\n                 encrypted_msg)\\n\\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\\n        return bytes(arr_ptr[:arr_len]),\\n\\n    if not hasattr(anon_decrypt, \"cb\"):\\n        logger.debug(\"anon_decrypt: Creating callback\")\\n        anon_decrypt.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_recipient_vk = c_char_p(recipient_vk.encode(\\'utf-8\\'))\\n    c_encrypted_msg_len = c_uint32(len(encrypted_msg))\\n    decrypted_message = await do_call(\\'indy_crypto_anon_decrypt\\',\\n                                      c_wallet_handle,\\n                                      c_recipient_vk,\\n                                      encrypted_msg,\\n                                      c_encrypted_msg_len,\\n                                      anon_decrypt.cb)\\n\\n    logger.debug(\"crypto_box_seal_open: <<< res: %r\", decrypted_message)\\n    return decrypted_message', 'async def pack_message(wallet_handle: int,\\n                       message: str,\\n                       recipient_verkeys: list,\\n                       sender_verkey: Optional[str]) -> bytes:\\n    \"\"\"\\n    Packs a message by encrypting the message and serializes it in a JWE-like format (Experimental)\\n\\n    Note to use DID keys with this function you can call did.key_for_did to get key id (verkey)\\n    for specific DID.\\n\\n    #Params\\n    command_handle: command handle to map callback to user context.\\n    wallet_handle: wallet handler (created by open_wallet)\\n    message: the message being sent as a string. If it\\'s JSON formatted it should be converted to a string\\n    recipient_verkeys: a list of Strings which are recipient verkeys\\n    sender_verkey: the sender\\'s verkey as a string. -> When None is passed in this parameter, anoncrypt mode is used\\n\\n    returns an Agent Wire Message format as a byte array. See HIPE 0028 for detailed formats\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"pack_message: >>> wallet_handle: %r, message: %r, recipient_verkeys: %r, sender_verkey: %r\",\\n                 wallet_handle,\\n                 message,\\n                 recipient_verkeys,\\n                 sender_verkey)\\n\\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\\n        return bytes(arr_ptr[:arr_len]),\\n\\n    if not hasattr(pack_message, \"cb\"):\\n        logger.debug(\"pack_message: Creating callback\")\\n        pack_message.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    msg_bytes = message.encode(\"utf-8\")\\n    c_msg_len = c_uint32(len(msg_bytes))\\n    c_recipient_verkeys = c_char_p(json.dumps(recipient_verkeys).encode(\\'utf-8\\'))\\n    c_sender_vk = c_char_p(sender_verkey.encode(\\'utf-8\\')) if sender_verkey is not None else None\\n    res = await do_call(\\'indy_pack_message\\',\\n                        c_wallet_handle,\\n                        msg_bytes,\\n                        c_msg_len,\\n                        c_recipient_verkeys,\\n                        c_sender_vk,\\n                        pack_message.cb)\\n    logger.debug(\"pack_message: <<< res: %r\", res)\\n    return res', 'async def unpack_message(wallet_handle: int,\\n                         jwe: bytes) -> bytes:\\n    \"\"\"\\n    Unpacks a JWE-like formatted message outputted by pack_message (Experimental)\\n\\n    #Params\\n    command_handle: command handle to map callback to user context.\\n    wallet_handle: wallet handler (created by open_wallet)\\n    message: the output of a pack message\\n\\n    #Returns -> See HIPE 0028 for details\\n    (Authcrypt mode)\\n\\n    {\\n        \"message\": <decrypted message>,\\n        \"recipient_verkey\": <recipient verkey used to decrypt>,\\n        \"sender_verkey\": <sender verkey used to encrypt>\\n    }\\n\\n    (Anoncrypt mode)\\n\\n    {\\n        \"message\": <decrypted message>,\\n        \"recipient_verkey\": <recipient verkey used to decrypt>,\\n    }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"unpack_message: >>> wallet_handle: %r, jwe: %r\",\\n                 wallet_handle,\\n                 jwe)\\n\\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\\n        return bytes(arr_ptr[:arr_len]),\\n\\n    if not hasattr(unpack_message, \"cb\"):\\n        logger.debug(\"unpack_message: Creating callback\")\\n        unpack_message.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_jwe_len = c_uint32(len(jwe))\\n    res = await do_call(\\'indy_unpack_message\\',\\n                        c_wallet_handle,\\n                        jwe,\\n                        c_jwe_len,\\n                        unpack_message.cb)\\n\\n    logger.debug(\"unpack_message: <<< res: %r\", res)\\n    return res', 'async def vcx_init(config_path: str) -> None:\\n    \"\"\"\\n    Initializes VCX with config file.\\n    :param config_path: String\\n    Example:\\n    await vcx_init(\\'/home/username/vcxconfig.json\\')\\n    :return:\\n    \"\"\"\\n    logger = logging.getLogger(__name__)\\n\\n    if not hasattr(vcx_init, \"cb\"):\\n        logger.debug(\"vcx_init: Creating callback\")\\n        vcx_init.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n    c_config_path = c_char_p(config_path.encode(\\'utf-8\\'))\\n\\n    result = await do_call(\\'vcx_init\\',\\n                           c_config_path,\\n                           vcx_init.cb)\\n\\n    logger.debug(\"vcx_init completed\")\\n    return result', 'async def create(source_id: str, name: str, schema_id: str, payment_handle: int):\\n        \"\"\"\\n        Creates a new CredentialDef object that is written to the ledger\\n\\n        :param source_id: Institution\\'s unique ID for the credential definition\\n        :param name: Name of credential definition\\n        :param schema_id: The schema ID given during the creation of the schema\\n        :param payment_handle: NYI - payment of ledger fee is taken from wallet automatically\\n        Example:\\n        source_id = \\'foobar123\\'\\n        schema_name = \\'Schema Name\\'\\n        payment_handle = 0\\n        credential_def1 = await CredentialDef.create(source_id, name, schema_id, payment_handle)\\n        :return: credential_def object, written to ledger\\n        \"\"\"\\n        constructor_params = (source_id, name, schema_id)\\n\\n        c_source_id = c_char_p(source_id.encode(\\'utf-8\\'))\\n        c_schema_id = c_char_p(schema_id.encode(\\'utf-8\\'))\\n        c_name = c_char_p(name.encode(\\'utf-8\\'))\\n        # default institution_did in config is used as issuer_did\\n        c_issuer_did = None\\n        c_payment = c_uint32(payment_handle)\\n        # Todo: add params for tag and config\\n        c_tag = c_char_p(\\'tag1\\'.encode(\\'utf-8\\'))\\n        c_config = c_char_p(\\'{\"support_revocation\":false}\\'.encode(\\'utf-8\\'))\\n        c_params = (c_source_id, c_name, c_schema_id, c_issuer_did, c_tag, c_config, c_payment)\\n\\n        return await CredentialDef._create(\"vcx_credentialdef_create\",\\n                                           constructor_params,\\n                                           c_params)', 'async def deserialize(data: dict):\\n        \"\"\"\\n        Create the object from a previously serialized object.\\n\\n        :param data: The output of the \"serialize\" call\\n        Example:\\n        source_id = \\'foobar123\\'\\n        schema_name = \\'Schema Name\\'\\n        payment_handle = 0\\n        credential_def1 = await CredentialDef.create(source_id, name, schema_id, payment_handle)\\n        data1 = await credential_def1.serialize()\\n        credential_def2 = await CredentialDef.deserialize(data1)\\n        :return: A re-instantiated object\\n        \"\"\"\\n        try:\\n            credential_def = await CredentialDef._deserialize(\"vcx_credentialdef_deserialize\",\\n                                                              json.dumps(data),\\n                                                              data[\\'data\\'][\\'source_id\\'],\\n                                                              data[\\'data\\'][\\'name\\'],\\n                                                              data[\\'data\\'][\\'id\\'])\\n            return credential_def\\n        except KeyError:\\n            raise VcxError(ErrorCode.InvalidCredentialDef)', 'async def get_cred_def_id(self):\\n        \"\"\"\\n        Get the ledger ID of the object\\n\\n        Example:\\n        source_id = \\'foobar123\\'\\n        schema_name = \\'Schema Name\\'\\n        payment_handle = 0\\n        credential_def1 = await CredentialDef.create(source_id, name, schema_id, payment_handle)\\n        assert await credential_def.get_cred_def_id() == \\'2hoqvcwupRTUNkXn6ArYzs:3:CL:2471\\'\\n        :return: ID string\\n        \"\"\"\\n        cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n        c_handle = c_uint32(self.handle)\\n        cred_def_id = await do_call(\\'vcx_credentialdef_get_cred_def_id\\', c_handle, cb)\\n        return cred_def_id .decode()', 'async def create_pool_ledger_config(config_name: str,\\n                                    config: Optional[str]) -> None:\\n    \"\"\"\\n    Creates a new local pool ledger configuration that can be used later to connect pool nodes.\\n\\n    :param config_name: Name of the pool ledger configuration.\\n    :param config: (optional) Pool configuration json. if NULL, then default config will be used. Example:\\n        {\\n            \"genesis_txn\": string (optional), A path to genesis transaction file. If NULL, then a default one will be used.\\n                           If file doesn\\'t exists default one will be created.\\n        }\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"create_pool_ledger_config: >>> config_name: %r, config: %r\",\\n                 config_name,\\n                 config)\\n\\n    if not hasattr(create_pool_ledger_config, \"cb\"):\\n        logger.debug(\"create_pool_ledger_config: Creating callback\")\\n        create_pool_ledger_config.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_config_name = c_char_p(config_name.encode(\\'utf-8\\'))\\n    c_config = c_char_p(config.encode(\\'utf-8\\')) if config is not None else None\\n\\n    res = await do_call(\\'indy_create_pool_ledger_config\\',\\n                        c_config_name,\\n                        c_config,\\n                        create_pool_ledger_config.cb)\\n\\n    logger.debug(\"create_pool_ledger_config: <<< res: %r\", res)\\n    return res', 'async def refresh_pool_ledger(handle: int) -> None:\\n    \"\"\"\\n    Refreshes a local copy of a pool ledger and updates pool nodes connections.\\n\\n    :param handle: pool handle returned by indy_open_pool_ledger\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"refresh_pool_ledger: >>> config_name: %r\",\\n                 handle)\\n\\n    if not hasattr(refresh_pool_ledger, \"cb\"):\\n        logger.debug(\"refresh_pool_ledger: Creating callback\")\\n        refresh_pool_ledger.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_handle = c_int32(handle)\\n\\n    res = await do_call(\\'indy_refresh_pool_ledger\\',\\n                        c_handle,\\n                        refresh_pool_ledger.cb)\\n\\n    logger.debug(\"refresh_pool_ledger: <<< res: %r\", res)\\n    return res', 'async def list_pools() -> None:\\n    \"\"\"\\n    Lists names of created pool ledgers\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"list_pools: >>> \")\\n\\n    if not hasattr(list_pools, \"cb\"):\\n        logger.debug(\"list_pools: Creating callback\")\\n        list_pools.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    res = await do_call(\\'indy_list_pools\\',\\n                        list_pools.cb)\\n    res = json.loads(res.decode())\\n    logger.debug(\"list_pools: <<< res: %r\", res)\\n    return res', 'async def delete_pool_ledger_config(config_name: str) -> None:\\n    \"\"\"\\n    Deletes created pool ledger configuration.\\n\\n    :param config_name: Name of the pool ledger configuration to delete.\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"delete_pool_ledger_config: >>> config_name: %r\",\\n                 config_name)\\n\\n    if not hasattr(delete_pool_ledger_config, \"cb\"):\\n        logger.debug(\"delete_pool_ledger_config: Creating callback\")\\n        delete_pool_ledger_config.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_config_name = c_char_p(config_name.encode(\\'utf-8\\'))\\n\\n    res = await do_call(\\'indy_delete_pool_ledger_config\\',\\n                        c_config_name,\\n                        delete_pool_ledger_config.cb)\\n\\n    logger.debug(\"delete_pool_ledger_config: <<< res: %r\", res)\\n    return res', 'async def set_protocol_version(protocol_version: int) -> None:\\n    \"\"\"\\n    Set PROTOCOL_VERSION to specific version.\\n\\n    There is a global property PROTOCOL_VERSION that used in every request to the pool and\\n    specified version of Indy Node which Libindy works.\\n    By default PROTOCOL_VERSION=1.\\n\\n    :param protocol_version: Protocol version will be used:\\n        1 - for Indy Node 1.3\\n        2 - for Indy Node 1.4 and greater\\n    :return: Error code\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"set_protocol_version: >>> protocol_version: %r\",\\n                 protocol_version)\\n\\n    if not hasattr(set_protocol_version, \"cb\"):\\n        logger.debug(\"set_protocol_version: Creating callback\")\\n        set_protocol_version.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    res = await do_call(\\'indy_set_protocol_version\\',\\n                        protocol_version,\\n                        set_protocol_version.cb)\\n\\n    logger.debug(\"set_protocol_version: <<< res: %r\", res)\\n    return res', 'async def open_search(type_: str, query: dict, options: dict):\\n        \"\"\"\\n        Opens a search handle within the storage wallet.\\n\\n        :param type_: String\\n        :param query: dictionary\\n        :param options: dictionary\\n        Example:\\n        query_json = {\"tagName1\": \"str1\"}\\n        type_ = \\'TestType\\'\\n        search_handle = await Wallet.open_search(type_, query_json, None)\\n        :return: int\\n        \"\"\"\\n        logger = logging.getLogger(__name__)\\n\\n        if not hasattr(Wallet.open_search, \"cb\"):\\n            logger.debug(\"vcx_wallet_open_search: Creating callback\")\\n            Wallet.open_search.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_uint32))\\n\\n        c_type_ = c_char_p(type_.encode(\\'utf-8\\'))\\n        c_query = c_char_p(json.dumps(query).encode(\\'utf-8\\'))\\n        c_options = c_char_p(json.dumps(options).encode(\\'utf-8\\')) if options else None\\n\\n        data = await do_call(\\'vcx_wallet_open_search\\',\\n                             c_type_,\\n                             c_query,\\n                             c_options,\\n                             Wallet.open_search.cb)\\n\\n        logger.debug(\"vcx_wallet_open_search completed\")\\n        return data', 'async def search_next_records(handle: int, count: int):\\n        \"\"\"\\n        Searches for next n record from an open search handle\\n\\n        :param handle: int\\n        :param count: int\\n         Example:\\n        query_json = {\"tagName1\": \"str1\"}\\n        type_ = \\'TestType\\'\\n        search_handle = await Wallet.open_search(type_, query_json, None)\\n        results = await Wallet.search_next_records(search_handle, 5)\\n        :return:\\n        \"\"\"\\n        logger = logging.getLogger(__name__)\\n\\n        if not hasattr(Wallet.search_next_records, \"cb\"):\\n            logger.debug(\"vcx_wallet_search_next_records: Creating callback\")\\n            Wallet.search_next_records.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n        c_handle = c_uint32(handle)\\n        c_count = c_uint32(count)\\n\\n        data = await do_call(\\'vcx_wallet_search_next_records\\',\\n                             c_handle,\\n                             c_count,\\n                             Wallet.search_next_records.cb)\\n\\n        logger.debug(\"vcx_wallet_search_next_records completed\")\\n        return data.decode()', 'async def get_record(type_: str, id: str, options: str):\\n        \"\"\"\\n        Retrieves a record from the wallet storage.\\n        :param type_: String\\n        :param id: String\\n        :param options: String\\n        Example:\\n        import json\\n        await Wallet.add_record({\\n            \\'id\\': \\'RecordId\\',\\n            \\'tags\\': json.dumps({\\n                \\'tag1\\': \\'unencrypted value1\\',\\n                \\'~encryptedTag\\', \\'this value is encrypted,\\n                \\'integerTag\\', 1\\n                }),\\n            \\'type_\\': \\'TestType\\',\\n            \\'value\\': \\'RecordValue\\'\\n        })\\n        options = json.dumps({\"retrieveType\": True, \"retrieveValue\": True, \"retrieveTags\": True})\\n        record = await Wallet.get_record(\\'TestType\\', \\'RecordId\\', options)\\n        :return:\\n        :return:\\n        \"\"\"\\n        logger = logging.getLogger(__name__)\\n\\n        if not hasattr(Wallet.get_record, \"cb\"):\\n            logger.debug(\"vcx_wallet_get_record: Creating callback\")\\n            Wallet.get_record.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_type_ = c_char_p(type_.encode(\\'utf-8\\'))\\n        c_id = c_char_p(id.encode(\\'utf-8\\'))\\n        c_options = c_char_p(options.encode(\\'utf-8\\'))\\n        data = await do_call(\\'vcx_wallet_get_record\\',\\n                             c_type_,\\n                             c_id,\\n                             c_options,\\n                             Wallet.get_record.cb)\\n\\n        logger.debug(\"vcx_wallet_get_record completed\")\\n        return data.decode()', 'async def delete_record(type_: str, id: str):\\n        \"\"\"\\n        Delete a record from the storage wallet.\\n\\n        :param type_:\\n        :param id:\\n        Example:\\n        await Wallet.add_record({\\n            \\'id\\': \\'RecordId\\',\\n            \\'tags\\': json.dumps({\\n                \\'tag1\\': \\'unencrypted value1\\',\\n                \\'~encryptedTag\\', \\'this value is encrypted,\\n                \\'integerTag\\', 1\\n                }),\\n            \\'type_\\': \\'TestType\\',\\n            \\'value\\': \\'RecordValue\\'\\n        })\\n        await Wallet.delete_record(\\'TestType\\', \\'RecordId\\')\\n        :return:\\n        \"\"\"\\n        logger = logging.getLogger(__name__)\\n\\n        if not hasattr(Wallet.delete_record, \"cb\"):\\n            logger.debug(\"vcx_wallet_delete_record: Creating callback\")\\n            Wallet.delete_record.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n        c_type_ = c_char_p(type_.encode(\\'utf-8\\'))\\n        c_id = c_char_p(id.encode(\\'utf-8\\'))\\n        result = await do_call(\\'vcx_wallet_delete_record\\',\\n                               c_type_,\\n                               c_id,\\n                               Wallet.delete_record.cb)\\n\\n        logger.debug(\"vcx_wallet_delete_record completed\")\\n        return result', 'async def get_token_info(handle: int) -> str:\\n        \"\"\"\\n        Retrieves from the ledger token info associated with the wallet.\\n        :param handle:\\n        Example:\\n        payment_handle = 0 // payment handle is always 0, for now.\\n        info = await Wallet.get_token_info(payment_handle)\\n        :return:\\n        \"\"\"\\n        logger = logging.getLogger(__name__)\\n\\n        if not hasattr(Wallet.get_token_info, \"cb\"):\\n            logger.debug(\"vcx_wallet_get_token_info: Creating callback\")\\n            Wallet.get_token_info.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_payment = c_uint32(handle)\\n\\n        result = await do_call(\\'vcx_wallet_get_token_info\\',\\n                               c_payment,\\n                               Wallet.get_token_info.cb)\\n\\n        logger.debug(\"vcx_wallet_get_token_info completed\")\\n        return result', 'async def create_payment_address(seed: str = None) -> str:\\n        \"\"\"\\n        Creates a payment address inside the wallet.\\n        :param seed: String\\n        Example:\\n        address = await Wallet.create_payment_address(\\'00000000000000000000000001234567\\')\\n        :return: String\\n        \"\"\"\\n        logger = logging.getLogger(__name__)\\n\\n        if not hasattr(Wallet.create_payment_address, \"cb\"):\\n            logger.debug(\"vcx_wallet_create_payment_address: Creating callback\")\\n            Wallet.create_payment_address.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        if seed:\\n            c_seed = c_char_p(seed.encode(\\'utf-8\\'))\\n        else:\\n            c_seed = None\\n\\n        result = await do_call(\\'vcx_wallet_create_payment_address\\',\\n                               c_seed,\\n                               Wallet.create_payment_address.cb)\\n\\n        logger.debug(\"vcx_wallet_create_payment_address completed\")\\n        return result', 'async def validate_payment_address(address: str) -> None:\\n        \"\"\"\\n        Determines whether a payment address is valid or not\\n        :param address: String\\n        Example:\\n        address = await Wallet.create_payment_address(\\'00000000000000000000000001234567\\')\\n        b = await Wallet.validate_payment_address(address)\\n        :return: Boolean\\n        \"\"\"\\n\\n        logger = logging.getLogger(__name__)\\n\\n        if not hasattr(Wallet.validate_payment_address, \"cb\"):\\n            logger.debug(\"vcx_wallet_validate_payment_address: Creating callback\")\\n            Wallet.validate_payment_address.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n        c_address = c_char_p(address.encode(\\'utf-8\\'))\\n        result = await do_call(\\'vcx_wallet_validate_payment_address\\',\\n                               c_address,\\n                               Wallet.validate_payment_address.cb)\\n\\n        logger.debug(\"vcx_wallet_validate_payment_address completed\")\\n        return result', 'async def send_tokens(payment_handle: int, tokens: int, address: str) -> str:\\n        \"\"\"\\n        Sends tokens to an address\\n        payment_handle is always 0\\n        :param payment_handle: Integer\\n        :param tokens: Integer\\n        :param address: String\\n        Example:\\n        payment_handle = 0\\n        amount = 1000\\n        address = await Wallet.create_payment_address(\\'00000000000000000000000001234567\\')\\n        await Wallet.send_tokens(payment_handle, amount, address)\\n        :return:\\n        \"\"\"\\n        logger = logging.getLogger(__name__)\\n\\n        if not hasattr(Wallet.send_tokens, \"cb\"):\\n            logger.debug(\"vcx_wallet_send_tokens: Creating callback\")\\n            Wallet.send_tokens.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\\n\\n        c_payment_handle = c_uint32(payment_handle)\\n        c_tokens = c_char_p(str(tokens).encode(\\'utf-8\\'))\\n        c_address = c_char_p(address.encode(\\'utf-8\\'))\\n\\n        result = await do_call(\\'vcx_wallet_send_tokens\\',\\n                               c_payment_handle,\\n                               c_tokens,\\n                               c_address,\\n                               Wallet.send_tokens.cb)\\n\\n        logger.debug(\"vcx_wallet_send_tokens completed\")\\n        return result', 'async def export(path, backup_key):\\n        \"\"\"\\n        Exports opened wallet\\n        :param path: Path to export wallet to User\\'s File System.\\n        :param backupKey: String representing the User\\'s Key for securing (encrypting) the exported Wallet.\\n        :return:\\n        Error code - success indicates that the wallet was successfully exported.\\n        \"\"\"\\n        logger = logging.getLogger(__name__)\\n\\n        if not hasattr(Wallet.export, \"cb\"):\\n            logger.debug(\"vcx_wallet_export: Creating callback\")\\n            Wallet.export.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n        c_backupKey = c_char_p(backup_key.encode(\\'utf-8\\'))\\n        c_path = c_char_p(path.encode(\\'utf-8\\'))\\n\\n        result = await do_call(\\'vcx_wallet_export\\',\\n                               c_path,\\n                               c_backupKey,\\n                               Wallet.export.cb)\\n\\n        logger.debug(\"vcx_wallet_export completed\")\\n        return result', 'async def import_wallet(config):\\n        \"\"\"\\n        Imports wallet from file with given key.\\n        Cannot be used if wallet is already opened (Especially if vcx_init has already been used).\\n        :param config: Can be same config that is passed to vcx_init.\\n        Must include: \\'{\"wallet_name\":\"\",\"wallet_key\":\"\",\"exported_wallet_path\":\"\",\"backup_key\":\"\"}\\'\\n        :return:\\n        Error code - success indicates that the wallet was successfully imported.\\n        \"\"\"\\n\\n        logger = logging.getLogger(__name__)\\n\\n        if not hasattr(Wallet.import_wallet, \"cb\"):\\n            logger.debug(\"vcx_wallet_import: Creating callback\")\\n            Wallet.import_wallet.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\\n\\n        c_config = c_char_p(config.encode(\\'utf-8\\'))\\n\\n        result = await do_call(\\'vcx_wallet_import\\',\\n                               c_config,\\n                               Wallet.import_wallet.cb)\\n\\n        logger.debug(\"vcx_wallet_export completed\")\\n        return result', 'async def update_wallet_record_value(wallet_handle: int,\\n                                     type_: str,\\n                                     id_: str,\\n                                     value: str) -> None:\\n    \"\"\"\\n    Update a non-secret wallet record value\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param type_: allows to separate different record types collections\\n    :param id_: the id of record\\n    :param value: the value of record\\n    :return: None\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"update_wallet_record_value: >>> wallet_handle: %r, type_: %r, id: %r, value: %r\",\\n                 wallet_handle,\\n                 type_,\\n                 id_,\\n                 value)\\n\\n    if not hasattr(update_wallet_record_value, \"cb\"):\\n        logger.debug(\"update_wallet_record_value: Creating callback\")\\n        update_wallet_record_value.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_type = c_char_p(type_.encode(\\'utf-8\\'))\\n    c_id = c_char_p(id_.encode(\\'utf-8\\'))\\n    c_value = c_char_p(value.encode(\\'utf-8\\'))\\n\\n    res = await do_call(\\'indy_update_wallet_record_value\\',\\n                        c_wallet_handle,\\n                        c_type,\\n                        c_id,\\n                        c_value,\\n                        update_wallet_record_value.cb)\\n\\n    logger.debug(\"update_wallet_record_value: <<< res: %r\", res)\\n    return res', 'async def get_wallet_record(wallet_handle: int,\\n                            type_: str,\\n                            id: str,\\n                            options_json: str) -> str:\\n    \"\"\"\\n    Get an wallet record by id\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param type_: allows to separate different record types collections\\n    :param id: the id of record\\n    :param options_json: //TODO: FIXME: Think about replacing by bitmask\\n      {\\n        retrieveType: (optional, false by default) Retrieve record type,\\n        retrieveValue: (optional, true by default) Retrieve record value,\\n        retrieveTags: (optional, true by default) Retrieve record tags\\n      }\\n    :return: wallet record json:\\n     {\\n       id: \"Some id\",\\n       type: \"Some type\", // present only if retrieveType set to true\\n       value: \"Some value\", // present only if retrieveValue set to true\\n       tags: <tags json>, // present only if retrieveTags set to true\\n     }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"get_wallet_record: >>> wallet_handle: %r, type_: %r, id: %r, options_json: %r\",\\n                 wallet_handle,\\n                 type_,\\n                 id,\\n                 options_json)\\n\\n    if not hasattr(get_wallet_record, \"cb\"):\\n        logger.debug(\"get_wallet_record: Creating callback\")\\n        get_wallet_record.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_type = c_char_p(type_.encode(\\'utf-8\\'))\\n    c_id = c_char_p(id.encode(\\'utf-8\\'))\\n    c_options_json = c_char_p(options_json.encode(\\'utf-8\\'))\\n\\n    wallet_record = await do_call(\\'indy_get_wallet_record\\',\\n                                  c_wallet_handle,\\n                                  c_type,\\n                                  c_id,\\n                                  c_options_json,\\n                                  get_wallet_record.cb)\\n    res = wallet_record.decode()\\n\\n    logger.debug(\"get_wallet_record: <<< res: %r\", res)\\n    return res', 'async def open_wallet_search(wallet_handle: int,\\n                             type_: str,\\n                             query_json: str,\\n                             options_json: str) -> int:\\n    \"\"\"\\n    Search for wallet records\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param type_: allows to separate different record types collections\\n    :param query_json: MongoDB style query to wallet record tags:\\n      {\\n        \"tagName\": \"tagValue\",\\n        $or: {\\n          \"tagName2\": { $regex: \\'pattern\\' },\\n          \"tagName3\": { $gte: \\'123\\' },\\n        },\\n      }\\n    :param options_json: //TODO: FIXME: Think about replacing by bitmask\\n      {\\n        retrieveRecords: (optional, true by default) If false only \"counts\" will be calculated,\\n        retrieveTotalCount: (optional, false by default) Calculate total count,\\n        retrieveType: (optional, false by default) Retrieve record type,\\n        retrieveValue: (optional, true by default) Retrieve record value,\\n        retrieveTags: (optional, true by default) Retrieve record tags,\\n      }\\n    :return: search_handle: Wallet search handle that can be used later\\n             to fetch records by small batches (with fetch_wallet_search_next_records)\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"open_wallet_search: >>> wallet_handle: %r, type_: %r, query_json: %r, options_json: %r\",\\n                 wallet_handle,\\n                 type_,\\n                 query_json,\\n                 options_json)\\n\\n    if not hasattr(open_wallet_search, \"cb\"):\\n        logger.debug(\"open_wallet_search: Creating callback\")\\n        open_wallet_search.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_int32))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_type = c_char_p(type_.encode(\\'utf-8\\'))\\n    c_query_json = c_char_p(query_json.encode(\\'utf-8\\'))\\n    c_options_json = c_char_p(options_json.encode(\\'utf-8\\'))\\n\\n    search_handle = await do_call(\\'indy_open_wallet_search\\',\\n                                  c_wallet_handle,\\n                                  c_type,\\n                                  c_query_json,\\n                                  c_options_json,\\n                                  open_wallet_search.cb)\\n    res = search_handle\\n\\n    logger.debug(\"open_wallet_search: <<< res: %r\", res)\\n    return res', 'async def fetch_wallet_search_next_records(wallet_handle: int,\\n                                           wallet_search_handle: int,\\n                                           count: int) -> str:\\n    \"\"\"\\n    Fetch next records for wallet search.\\n\\n    :param wallet_handle: wallet handler (created by open_wallet).\\n    :param wallet_search_handle: wallet wallet handle (created by open_wallet_search)\\n    :param count: Count of records to fetch\\n    :return: wallet records json:\\n     {\\n       totalCount: <str>, // present only if retrieveTotalCount set to true\\n       records: [{ // present only if retrieveRecords set to true\\n           id: \"Some id\",\\n           type: \"Some type\", // present only if retrieveType set to true\\n           value: \"Some value\", // present only if retrieveValue set to true\\n           tags: <tags json>, // present only if retrieveTags set to true\\n       }],\\n     }\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"fetch_wallet_search_next_records: >>> wallet_handle: %r, wallet_search_handle: %r, count: %r\",\\n                 wallet_handle,\\n                 wallet_search_handle,\\n                 count)\\n\\n    if not hasattr(fetch_wallet_search_next_records, \"cb\"):\\n        logger.debug(\"fetch_wallet_search_next_records: Creating callback\")\\n        fetch_wallet_search_next_records.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\\n\\n    c_wallet_handle = c_int32(wallet_handle)\\n    c_wallet_search_handle = c_int32(wallet_search_handle)\\n    c_count = c_uint(count)\\n\\n    records_json = await do_call(\\'indy_fetch_wallet_search_next_records\\',\\n                                 c_wallet_handle,\\n                                 c_wallet_search_handle,\\n                                 c_count,\\n                                 fetch_wallet_search_next_records.cb)\\n    res = records_json.decode()\\n\\n    logger.debug(\"fetch_wallet_search_next_records: <<< res: %r\", res)\\n    return res', 'async def close_wallet_search(wallet_search_handle: int) -> None:\\n    \"\"\"\\n    Close wallet search (make search handle invalid)\\n\\n    :param wallet_search_handle: wallet wallet handle (created by open_wallet_search)\\n    :return: None\\n    \"\"\"\\n\\n    logger = logging.getLogger(__name__)\\n    logger.debug(\"close_wallet_search: >>> wallet_search_handle: %r\",\\n                 wallet_search_handle)\\n\\n    if not hasattr(close_wallet_search, \"cb\"):\\n        logger.debug(\"close_wallet_search: Creating callback\")\\n        close_wallet_search.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\\n\\n    c_wallet_search_handle = c_int32(wallet_search_handle)\\n\\n    res = await do_call(\\'indy_close_wallet_search\\',\\n                        c_wallet_search_handle,\\n                        close_wallet_search.cb)\\n\\n    logger.debug(\"close_wallet_search: <<< res: %r\", res)\\n    return res', 'def register_mark(key=None):\\n    \"\"\"Returns a decorator registering a mark class in the mark type registry.\\n\\n    If no key is provided, the class name is used as a key. A key is provided\\n    for each core bqplot mark so that the frontend can use\\n    this key regardless of the kernel language.\\n    \"\"\"\\n    def wrap(mark):\\n        name = key if key is not None else mark.__module__ + mark.__name__\\n        Mark.mark_types[name] = mark\\n        return mark\\n    return wrap', 'def _get_dimension_scales(self, dimension, preserve_domain=False):\\n        \"\"\"\\n        Return the list of scales corresponding to a given dimension.\\n\\n        The preserve_domain optional argument specifies whether one should\\n        filter out the scales for which preserve_domain is set to True.\\n        \"\"\"\\n        if preserve_domain:\\n            return [\\n                self.scales[k] for k in self.scales if (\\n                    k in self.scales_metadata and\\n                    self.scales_metadata[k].get(\\'dimension\\') == dimension and\\n                    not self.preserve_domain.get(k)\\n                )\\n            ]\\n        else:\\n            return [\\n                self.scales[k] for k in self.scales if (\\n                    k in self.scales_metadata and\\n                    self.scales_metadata[k].get(\\'dimension\\') == dimension\\n                )\\n            ]', 'def _validate_scales(self, proposal):\\n        \"\"\"\\n        Validates the `scales` based on the mark\\'s scaled attributes metadata.\\n\\n        First checks for missing scale and then for \\'rtype\\' compatibility.\\n        \"\"\"\\n        # Validate scales\\' \\'rtype\\' versus data attribute \\'rtype\\' decoration\\n        # At this stage it is already validated that all values in self.scales\\n        # are instances of Scale.\\n        scales = proposal.value\\n        for name in self.trait_names(scaled=True):\\n            trait = self.traits()[name]\\n            if name not in scales:\\n                # Check for missing scale\\n                if not trait.allow_none:\\n                    raise TraitError(\"Missing scale for data attribute %s.\" %\\n                                     name)\\n            else:\\n                # Check scale range type compatibility\\n                if scales[name].rtype != trait.get_metadata(\\'rtype\\'):\\n                    raise TraitError(\"Range type mismatch for scale %s.\" %\\n                                     name)\\n        return scales', 'def register_axis(key=None):\\n    \"\"\"Returns a decorator registering an axis class in the axis type registry.\\n\\n    If no key is provided, the class name is used as a key. A key is provided\\n    for each core bqplot axis so that the frontend can use this key regardless\\n    of the kernel language.\\n    \"\"\"\\n    def wrap(axis):\\n        name = key if key is not None else axis.__module__ + axis.__name__\\n        BaseAxis.axis_types[name] = axis\\n        return axis\\n    return wrap', 'def register_interaction(key=None):\\n    \"\"\"Decorator registering an interaction class in the registry.\\n\\n    If no key is provided, the class name is used as a key. A key is provided\\n    for each core bqplot interaction type so that the frontend can use this\\n    key regardless of the kernal language.\\n    \"\"\"\\n    def wrap(interaction):\\n        name = key if key is not None else interaction.__module__ + \\\\\\n            interaction.__name__\\n        interaction.types[name] = interaction\\n        return interaction\\n    return wrap', 'def panzoom(marks):\\n    \"\"\"Helper function for panning and zooming over a set of marks.\\n\\n    Creates and returns a panzoom interaction with the \\'x\\' and \\'y\\' dimension\\n    scales of the specified marks.\\n    \"\"\"\\n    return PanZoom(scales={\\n            \\'x\\': sum([mark._get_dimension_scales(\\'x\\', preserve_domain=True) for mark in marks], []),\\n            \\'y\\': sum([mark._get_dimension_scales(\\'y\\', preserve_domain=True) for mark in marks], [])\\n    })', 'def register_scale(key=None):\\n    \"\"\"Returns a decorator to register a scale type in the scale type\\n    registry.\\n\\n    If no key is provided, the class name is used as a key. A key is\\n    provided for each core bqplot scale type so that the frontend can use\\n    this key regardless of the kernal language.\\n    \"\"\"\\n    def wrap(scale):\\n        label = key if key is not None else scale.__module__ + scale.__name__\\n        Scale.scale_types[label] = scale\\n        return scale\\n    return wrap', 'def install(user=False, symlink=False, overwrite=False, **kwargs):\\n    \"\"\"Install the bqplot nbextension.\\n    \\n    Parameters\\n    ----------\\n\\n    user: bool\\n        Install for current user instead of system-wide.\\n    symlink: bool\\n        Symlink instead of copy (for development).\\n    overwrite: bool\\n        Overwrite previously-installed files for this extension\\n    **kwargs: keyword arguments\\n        Other keyword arguments passed to the install_nbextension command\\n    \"\"\"\\n    directory = join(dirname(abspath(__file__)), \\'nbextension\\')\\n    install_nbextension(directory, destination=\\'bqplot\\',\\n                        symlink=symlink, user=user, overwrite=overwrite,\\n                        **kwargs)', 'def hashable(data, v):\\n    \"\"\"Determine whether `v` can be hashed.\"\"\"\\n    try:\\n        data[v]\\n    except (TypeError, KeyError, IndexError):\\n        return False\\n    return True', 'def show(key=None, display_toolbar=True):\\n    \"\"\"Shows the current context figure in the output area.\\n\\n    Parameters\\n    ----------\\n\\n    key : hashable, optional\\n        Any variable that can be used as a key for a dictionary.\\n    display_toolbar: bool (default: True)\\n        If True, a toolbar for different mouse interaction is displayed with\\n        the figure.\\n\\n    Raises\\n    ------\\n\\n    KeyError\\n        When no context figure is associated with the provided key.\\n\\n    Examples\\n    --------\\n\\n        >>> import numpy as np\\n        >>> import pyplot as plt\\n        >>> n = 100\\n        >>> x = np.arange(n)\\n        >>> y = np.cumsum(np.random.randn(n))\\n        >>> plt.plot(x,y)\\n        >>> plt.show()\\n\\n    \"\"\"\\n    if key is None:\\n        figure = current_figure()\\n    else:\\n        figure = _context[\\'figure_registry\\'][key]\\n    if display_toolbar:\\n        if not hasattr(figure, \\'pyplot\\'):\\n            figure.pyplot = Toolbar(figure=figure)\\n        display(VBox([figure, figure.pyplot]))\\n    else:\\n        display(figure)', 'def figure(key=None, fig=None, **kwargs):\\n    \"\"\"Creates figures and switches between figures.\\n\\n    If a ``bqplot.Figure`` object is provided via the fig optional argument,\\n    this figure becomes the current context figure.\\n\\n    Otherwise:\\n\\n    - If no key is provided, a new empty context figure is created.\\n    - If a key is provided for which a context already exists, the\\n      corresponding context becomes current.\\n    - If a key is provided and no corresponding context exists, a new context\\n      is reated for that key and becomes current.\\n\\n    Besides, optional arguments allow to set or modify Attributes\\n    of the selected context figure.\\n\\n    Parameters\\n    ----------\\n    key: hashable, optional\\n        Any variable that can be used as a key for a dictionary\\n    fig: Figure, optional\\n        A bqplot Figure\\n\\n    \"\"\"\\n    scales_arg = kwargs.pop(\\'scales\\', {})\\n    _context[\\'current_key\\'] = key\\n    if fig is not None:                                     # fig provided\\n        _context[\\'figure\\'] = fig\\n        if key is not None:\\n            _context[\\'figure_registry\\'][key] = fig\\n        for arg in kwargs:\\n            setattr(_context[\\'figure\\'], arg, kwargs[arg])\\n    else:                                                   # no fig provided\\n        if key is None:                                     # no key provided\\n            _context[\\'figure\\'] = Figure(**kwargs)\\n        else:                                               # a key is provided\\n            if key not in _context[\\'figure_registry\\']:\\n                if \\'title\\' not in kwargs:\\n                    kwargs[\\'title\\'] = \\'Figure\\' + \\' \\' + str(key)\\n                _context[\\'figure_registry\\'][key] = Figure(**kwargs)\\n            _context[\\'figure\\'] = _context[\\'figure_registry\\'][key]\\n            for arg in kwargs:\\n                setattr(_context[\\'figure\\'], arg, kwargs[arg])\\n    scales(key, scales=scales_arg)\\n    # Set the axis reference dictionary. This dictionary contains the mapping\\n    # from the possible dimensions in the figure to the list of scales with\\n    # respect to which axes have been drawn for this figure.\\n    # Used to automatically generate axis.\\n    if(getattr(_context[\\'figure\\'], \\'axis_registry\\', None) is None):\\n        setattr(_context[\\'figure\\'], \\'axis_registry\\', {})\\n    return _context[\\'figure\\']', 'def close(key):\\n    \"\"\"Closes and unregister the context figure corresponding to the key.\\n\\n    Parameters\\n    ----------\\n\\n    key: hashable\\n        Any variable that can be used as a key for a dictionary\\n\\n    \"\"\"\\n    figure_registry = _context[\\'figure_registry\\']\\n    if key not in figure_registry:\\n        return\\n    if _context[\\'figure\\'] == figure_registry[key]:\\n        figure()\\n    fig = figure_registry[key]\\n    if hasattr(fig, \\'pyplot\\'):\\n        fig.pyplot.close()\\n    fig.close()\\n    del figure_registry[key]\\n    del _context[\\'scale_registry\\'][key]', 'def _process_data(*kwarg_names):\\n    \"\"\"Helper function to handle data keyword argument\\n    \"\"\"\\n    def _data_decorator(func):\\n        @functools.wraps(func)\\n        def _mark_with_data(*args, **kwargs):\\n            data = kwargs.pop(\\'data\\', None)\\n            if data is None:\\n                return func(*args, **kwargs)\\n            else:\\n                data_args = [data[i] if hashable(data, i) else i for i in args]\\n                data_kwargs = {\\n                   kw: data[kwargs[kw]] if hashable(data, kwargs[kw]) else kwargs[kw] for kw in set(kwarg_names).intersection(list(kwargs.keys()))\\n                }\\n                try:\\n                    # if any of the plots want to use the index_data, they can\\n                    # use it by referring to this attribute.\\n                    data_kwargs[\\'index_data\\'] = data.index\\n                except AttributeError as e:\\n                    pass\\n                kwargs_update = kwargs.copy()\\n                kwargs_update.update(data_kwargs)\\n                return func(*data_args, **kwargs_update)\\n\\n        return _mark_with_data\\n    return _data_decorator', 'def scales(key=None, scales={}):\\n    \"\"\"Creates and switches between context scales.\\n\\n    If no key is provided, a new blank context is created.\\n\\n    If a key is provided for which a context already exists, the existing\\n    context is set as the current context.\\n\\n    If a key is provided and no corresponding context exists, a new context is\\n    created for that key and set as the current context.\\n\\n    Parameters\\n    ----------\\n    key: hashable, optional\\n        Any variable that can be used as a key for a dictionary\\n    scales: dictionary\\n        Dictionary of scales to be used in the new context\\n\\n    Example\\n    -------\\n\\n        >>> scales(scales={\\n        >>>    \\'x\\': Keep,\\n        >>>    \\'color\\': ColorScale(min=0, max=1)\\n        >>> })\\n\\n    This creates a new scales context, where the \\'x\\' scale is kept from the\\n    previous context, the \\'color\\' scale is an instance of ColorScale\\n    provided by the user. Other scales, potentially needed such as the \\'y\\'\\n    scale in the case of a line chart will be created on the fly when\\n    needed.\\n\\n    Notes\\n    -----\\n    Every call to the function figure triggers a call to scales.\\n\\n    The `scales` parameter is ignored if the `key` argument is not Keep and\\n    context scales already exist for that key.\\n    \"\"\"\\n    old_ctxt = _context[\\'scales\\']\\n    if key is None:  # No key provided\\n        _context[\\'scales\\'] = {_get_attribute_dimension(k): scales[k] if scales[k] is not Keep\\n                              else old_ctxt[_get_attribute_dimension(k)] for k in scales}\\n    else:  # A key is provided\\n        if key not in _context[\\'scale_registry\\']:\\n            _context[\\'scale_registry\\'][key] = {\\n                _get_attribute_dimension(k): scales[k]\\n                if scales[k] is not Keep\\n                else old_ctxt[_get_attribute_dimension(k)]\\n                for k in scales\\n            }\\n        _context[\\'scales\\'] = _context[\\'scale_registry\\'][key]', 'def set_lim(min, max, name):\\n    \"\"\"Set the domain bounds of the scale associated with the provided key.\\n\\n    Parameters\\n    ----------\\n    name: hashable\\n        Any variable that can be used as a key for a dictionary\\n\\n    Raises\\n    ------\\n    KeyError\\n        When no context figure is associated with the provided key.\\n\\n    \"\"\"\\n    scale = _context[\\'scales\\'][_get_attribute_dimension(name)]\\n    scale.min = min\\n    scale.max = max\\n    return scale', 'def axes(mark=None, options={}, **kwargs):\\n    \"\"\"Draws axes corresponding to the scales of a given mark.\\n\\n    It also returns a dictionary of drawn axes. If the mark is not provided,\\n    the last drawn mark is used.\\n\\n    Parameters\\n    ----------\\n    mark: Mark or None (default: None)\\n        The mark to inspect to create axes. If None, the last mark drawn is\\n        used instead.\\n    options: dict (default: {})\\n        Options for the axes to be created. If a scale labeled \\'x\\' is required\\n        for that mark, options[\\'x\\'] contains optional keyword arguments for the\\n        constructor of the corresponding axis type.\\n    \"\"\"\\n    if mark is None:\\n        mark = _context[\\'last_mark\\']\\n    if mark is None:\\n        return {}\\n    fig = kwargs.get(\\'figure\\', current_figure())\\n    scales = mark.scales\\n    fig_axes = [axis for axis in fig.axes]\\n    axes = {}\\n    for name in scales:\\n        if name not in mark.class_trait_names(scaled=True):\\n            # The scale is not needed.\\n            continue\\n        scale_metadata = mark.scales_metadata.get(name, {})\\n        dimension = scale_metadata.get(\\'dimension\\', scales[name])\\n        axis_args = dict(scale_metadata,\\n                         **(options.get(name, {})))\\n\\n        axis = _fetch_axis(fig, dimension, scales[name])\\n        if axis is not None:\\n            # For this figure, an axis exists for the scale in the given\\n            # dimension. Apply the properties and return back the object.\\n            _apply_properties(axis, options.get(name, {}))\\n            axes[name] = axis\\n            continue\\n\\n        # An axis must be created. We fetch the type from the registry\\n        # the key being provided in the scaled attribute decoration\\n        key = mark.class_traits()[name].get_metadata(\\'atype\\')\\n        if(key is not None):\\n            axis_type = Axis.axis_types[key]\\n            axis = axis_type(scale=scales[name], **axis_args)\\n            axes[name] = axis\\n            fig_axes.append(axis)\\n            # Update the axis registry of the figure once the axis is added\\n            _update_fig_axis_registry(fig, dimension, scales[name], axis)\\n    fig.axes = fig_axes\\n    return axes', 'def _set_label(label, mark, dim, **kwargs):\\n    \"\"\"Helper function to set labels for an axis\\n    \"\"\"\\n    if mark is None:\\n        mark = _context[\\'last_mark\\']\\n    if mark is None:\\n        return {}\\n    fig = kwargs.get(\\'figure\\', current_figure())\\n    scales = mark.scales\\n    scale_metadata = mark.scales_metadata.get(dim, {})\\n    scale = scales.get(dim, None)\\n    if scale is None:\\n        return\\n    dimension = scale_metadata.get(\\'dimension\\', scales[dim])\\n    axis = _fetch_axis(fig, dimension, scales[dim])\\n\\n    if axis is not None:\\n        _apply_properties(axis, {\\'label\\': label})', 'def grids(fig=None, value=\\'solid\\'):\\n    \"\"\"Sets the value of the grid_lines for the axis to the passed value.\\n    The default value is `solid`.\\n\\n    Parameters\\n    ----------\\n    fig: Figure or None(default: None)\\n        The figure for which the axes should be edited. If the value is None,\\n        the current figure is used.\\n    value: {\\'none\\', \\'solid\\', \\'dashed\\'}\\n        The display of the grid_lines\\n    \"\"\"\\n\\n    if fig is None:\\n        fig = current_figure()\\n    for a in fig.axes:\\n        a.grid_lines = value', 'def title(label, style=None):\\n    \"\"\"Sets the title for the current figure.\\n\\n    Parameters\\n    ----------\\n    label : str\\n        The new title for the current figure.\\n    style: dict\\n        The CSS style to be applied to the figure title\\n    \"\"\"\\n    fig = current_figure()\\n    fig.title = label\\n    if style is not None:\\n        fig.title_style = style', 'def hline(level, **kwargs):\\n    \"\"\"Draws a horizontal line at the given level.\\n\\n    Parameters\\n    ----------\\n    level: float\\n        The level at which to draw the horizontal line.\\n    preserve_domain: boolean (default: False)\\n        If true, the line does not affect the domain of the \\'y\\' scale.\\n    \"\"\"\\n    kwargs.setdefault(\\'colors\\', [\\'dodgerblue\\'])\\n    kwargs.setdefault(\\'stroke_width\\', 1)\\n    scales = kwargs.pop(\\'scales\\', {})\\n    fig = kwargs.get(\\'figure\\', current_figure())\\n    scales[\\'x\\'] = fig.scale_x\\n\\n    level = array(level)\\n    if len(level.shape) == 0:\\n        x = [0, 1]\\n        y = [level, level]\\n    else:\\n        x = [0, 1]\\n        y = column_stack([level, level])\\n    return plot(x, y, scales=scales, preserve_domain={\\n        \\'x\\': True,\\n        \\'y\\': kwargs.get(\\'preserve_domain\\', False)\\n    }, axes=False, update_context=False, **kwargs)', \"def _process_cmap(cmap):\\n    '''\\n    Returns a kwarg dict suitable for a ColorScale\\n    '''\\n    option = {}\\n    if isinstance(cmap, str):\\n        option['scheme'] = cmap\\n    elif isinstance(cmap, list):\\n        option['colors'] = cmap\\n    else:\\n        raise ValueError('''`cmap` must be a string (name of a color scheme)\\n                         or a list of colors, but a value of {} was given\\n                         '''.format(cmap))\\n    return option\", \"def set_cmap(cmap):\\n    '''\\n    Set the color map of the current 'color' scale.\\n    '''\\n    scale = _context['scales']['color']\\n    for k, v in _process_cmap(cmap).items():\\n        setattr(scale, k, v)\\n    return scale\", 'def _draw_mark(mark_type, options={}, axes_options={}, **kwargs):\\n    \"\"\"Draw the mark of specified mark type.\\n\\n    Parameters\\n    ----------\\n    mark_type: type\\n        The type of mark to be drawn\\n    options: dict (default: {})\\n        Options for the scales to be created. If a scale labeled \\'x\\' is\\n        required for that mark, options[\\'x\\'] contains optional keyword\\n        arguments for the constructor of the corresponding scale type.\\n    axes_options: dict (default: {})\\n        Options for the axes to be created. If an axis labeled \\'x\\' is required\\n        for that mark, axes_options[\\'x\\'] contains optional keyword arguments\\n        for the constructor of the corresponding axis type.\\n    figure: Figure or None\\n        The figure to which the mark is to be added.\\n        If the value is None, the current figure is used.\\n    cmap: list or string\\n        List of css colors, or name of bqplot color scheme\\n    \"\"\"\\n    fig = kwargs.pop(\\'figure\\', current_figure())\\n    scales = kwargs.pop(\\'scales\\', {})\\n    update_context = kwargs.pop(\\'update_context\\', True)\\n\\n    # Set the color map of the color scale\\n    cmap = kwargs.pop(\\'cmap\\', None)\\n    if cmap is not None:\\n        # Add the colors or scheme to the color scale options\\n        options[\\'color\\'] = dict(options.get(\\'color\\', {}),\\n                                **_process_cmap(cmap))\\n\\n    # Going through the list of data attributes\\n    for name in mark_type.class_trait_names(scaled=True):\\n        dimension = _get_attribute_dimension(name, mark_type)\\n        # TODO: the following should also happen if name in kwargs and\\n        # scales[name] is incompatible.\\n        if name not in kwargs:\\n            # The scaled attribute is not being passed to the mark. So no need\\n            # create a scale for this.\\n            continue\\n        elif name in scales:\\n            if update_context:\\n                _context[\\'scales\\'][dimension] = scales[name]\\n        # Scale has to be fetched from the context or created as it has not\\n        # been passed.\\n        elif dimension not in _context[\\'scales\\']:\\n            # Creating a scale for the dimension if a matching scale is not\\n            # present in _context[\\'scales\\']\\n            traitlet = mark_type.class_traits()[name]\\n            rtype = traitlet.get_metadata(\\'rtype\\')\\n            dtype = traitlet.validate(None, kwargs[name]).dtype\\n            # Fetching the first matching scale for the rtype and dtype of the\\n            # scaled attributes of the mark.\\n            compat_scale_types = [\\n                    Scale.scale_types[key]\\n                    for key in Scale.scale_types\\n                    if Scale.scale_types[key].rtype == rtype and\\n                    issubdtype(dtype, Scale.scale_types[key].dtype)\\n                ]\\n            sorted_scales = sorted(compat_scale_types,\\n                                   key=lambda x: x.precedence)\\n            scales[name] = sorted_scales[-1](**options.get(name, {}))\\n            # Adding the scale to the context scales\\n            if update_context:\\n                _context[\\'scales\\'][dimension] = scales[name]\\n        else:\\n            scales[name] = _context[\\'scales\\'][dimension]\\n\\n    mark = mark_type(scales=scales, **kwargs)\\n    _context[\\'last_mark\\'] = mark\\n    fig.marks = [m for m in fig.marks] + [mark]\\n    if kwargs.get(\\'axes\\', True):\\n        axes(mark, options=axes_options)\\n    return mark', 'def _infer_x_for_line(y):\\n    \"\"\"\\n    Infers the x for a line if no x is provided.\\n    \"\"\"\\n    array_shape = shape(y)\\n\\n    if len(array_shape) == 0:\\n        return []\\n    if len(array_shape) == 1:\\n        return arange(array_shape[0])\\n    if len(array_shape) > 1:\\n        return arange(array_shape[1])', 'def plot(*args, **kwargs):\\n    \"\"\"Draw lines in the current context figure.\\n\\n    Signature: `plot(x, y, **kwargs)` or `plot(y, **kwargs)`, depending of the\\n    length of the list of positional arguments. In the case where the `x` array\\n    is not provided.\\n\\n    Parameters\\n    ----------\\n    x: numpy.ndarray or list, 1d or 2d (optional)\\n        The x-coordinates of the plotted line. When not provided, the function\\n        defaults to `numpy.arange(len(y))`\\n        x can be 1-dimensional or 2-dimensional.\\n    y: numpy.ndarray or list, 1d or 2d\\n        The y-coordinates of the plotted line. If argument `x` is 2-dimensional\\n        it must also be 2-dimensional.\\n    marker_str: string\\n        string representing line_style, marker and color.\\n        For e.g. \\'g--o\\', \\'sr\\' etc\\n    options: dict (default: {})\\n        Options for the scales to be created. If a scale labeled \\'x\\' is\\n        required for that mark, options[\\'x\\'] contains optional keyword\\n        arguments for the constructor of the corresponding scale type.\\n    axes_options: dict (default: {})\\n        Options for the axes to be created. If an axis labeled \\'x\\' is required\\n        for that mark, axes_options[\\'x\\'] contains optional keyword arguments\\n        for the constructor of the corresponding axis type.\\n    figure: Figure or None\\n        The figure to which the line is to be added.\\n        If the value is None, the current figure is used.\\n    \"\"\"\\n    marker_str = None\\n    if len(args) == 1:\\n        kwargs[\\'y\\'] = args[0]\\n        if kwargs.get(\\'index_data\\', None) is not None:\\n            kwargs[\\'x\\'] = kwargs[\\'index_data\\']\\n        else:\\n            kwargs[\\'x\\'] = _infer_x_for_line(args[0])\\n    elif len(args) == 2:\\n        if type(args[1]) == str:\\n            kwargs[\\'y\\'] = args[0]\\n            kwargs[\\'x\\'] = _infer_x_for_line(args[0])\\n            marker_str = args[1].strip()\\n        else:\\n            kwargs[\\'x\\'] = args[0]\\n            kwargs[\\'y\\'] = args[1]\\n    elif len(args) == 3:\\n        kwargs[\\'x\\'] = args[0]\\n        kwargs[\\'y\\'] = args[1]\\n        if type(args[2]) == str:\\n            marker_str = args[2].strip()\\n\\n    if marker_str:\\n        line_style, color, marker = _get_line_styles(marker_str)\\n\\n        # only marker specified => draw scatter\\n        if marker and not line_style:\\n            kwargs[\\'marker\\'] = marker\\n            if color:\\n                kwargs[\\'colors\\'] = [color]\\n            return _draw_mark(Scatter, **kwargs)\\n        else:  # draw lines in all other cases\\n            kwargs[\\'line_style\\'] = line_style or \\'solid\\'\\n\\n            if marker:\\n                kwargs[\\'marker\\'] = marker\\n            if color:\\n                kwargs[\\'colors\\'] = [color]\\n            return _draw_mark(Lines, **kwargs)\\n    else:\\n        return _draw_mark(Lines, **kwargs)', 'def imshow(image, format, **kwargs):\\n    \"\"\"Draw an image in the current context figure.\\n    Parameters\\n    ----------\\n    image: image data\\n        Image data, depending on the passed format, can be one of:\\n            - an instance of an ipywidgets Image\\n            - a file name\\n            - a raw byte string\\n    format: {\\'widget\\', \\'filename\\', ...}\\n        Type of the input argument.\\n        If not \\'widget\\' or \\'filename\\', must be a format supported by\\n        the ipywidgets Image.\\n    options: dict (default: {})\\n        Options for the scales to be created. If a scale labeled \\'x\\' is\\n        required for that mark, options[\\'x\\'] contains optional keyword\\n        arguments for the constructor of the corresponding scale type.\\n    axes_options: dict (default: {})\\n        Options for the axes to be created. If an axis labeled \\'x\\' is required\\n        for that mark, axes_options[\\'x\\'] contains optional keyword arguments\\n        for the constructor of the corresponding axis type.\\n    \"\"\"\\n    if format == \\'widget\\':\\n        ipyimage = image\\n    elif format == \\'filename\\':\\n        with open(image, \\'rb\\') as f:\\n            data = f.read()\\n            ipyimage = ipyImage(value=data)\\n    else:\\n        ipyimage = ipyImage(value=image, format=format)\\n    kwargs[\\'image\\'] = ipyimage\\n\\n    kwargs.setdefault(\\'x\\', [0., 1.])\\n    kwargs.setdefault(\\'y\\', [0., 1.])\\n\\n    return _draw_mark(Image, **kwargs)', 'def ohlc(*args, **kwargs):\\n    \"\"\"Draw OHLC bars or candle bars in the current context figure.\\n\\n    Signature: `ohlc(x, y, **kwargs)` or `ohlc(y, **kwargs)`, depending of the\\n    length of the list of positional arguments. In the case where the `x` array\\n    is not provided\\n\\n    Parameters\\n    ----------\\n    x: numpy.ndarray or list, 1d (optional)\\n        The x-coordinates of the plotted line. When not provided, the function\\n        defaults to `numpy.arange(len(y))`.\\n    y: numpy.ndarray or list, 2d\\n        The ohlc (open/high/low/close) information. A two dimensional array. y\\n        must have the shape (n, 4).\\n    options: dict (default: {})\\n        Options for the scales to be created. If a scale labeled \\'x\\' is\\n        required for that mark, options[\\'x\\'] contains optional keyword\\n        arguments for the constructor of the corresponding scale type.\\n    axes_options: dict (default: {})\\n        Options for the axes to be created. If an axis labeled \\'x\\' is required\\n        for that mark, axes_options[\\'x\\'] contains optional keyword arguments\\n        for the constructor of the corresponding axis type.\\n    \"\"\"\\n    if len(args) == 2:\\n        kwargs[\\'x\\'] = args[0]\\n        kwargs[\\'y\\'] = args[1]\\n    elif len(args) == 1:\\n        kwargs[\\'y\\'] = args[0]\\n        length = len(args[0])\\n        kwargs[\\'x\\'] = arange(length)\\n    return _draw_mark(OHLC, **kwargs)', 'def scatter(x, y, **kwargs):\\n    \"\"\"Draw a scatter in the current context figure.\\n\\n    Parameters\\n    ----------\\n\\n    x: numpy.ndarray, 1d\\n        The x-coordinates of the data points.\\n    y: numpy.ndarray, 1d\\n        The y-coordinates of the data points.\\n    options: dict (default: {})\\n        Options for the scales to be created. If a scale labeled \\'x\\' is\\n        required for that mark, options[\\'x\\'] contains optional keyword\\n        arguments for the constructor of the corresponding scale type.\\n    axes_options: dict (default: {})\\n        Options for the axes to be created. If an axis labeled \\'x\\' is required\\n        for that mark, axes_options[\\'x\\'] contains optional keyword arguments\\n        for the constructor of the corresponding axis type.\\n    \"\"\"\\n    kwargs[\\'x\\'] = x\\n    kwargs[\\'y\\'] = y\\n    return _draw_mark(Scatter, **kwargs)', 'def hist(sample, options={}, **kwargs):\\n    \"\"\"Draw a histogram in the current context figure.\\n\\n    Parameters\\n    ----------\\n    sample: numpy.ndarray, 1d\\n        The sample for which the histogram must be generated.\\n    options: dict (default: {})\\n        Options for the scales to be created. If a scale labeled \\'counts\\'\\n        is required for that mark, options[\\'counts\\'] contains optional keyword\\n        arguments for the constructor of the corresponding scale type.\\n    axes_options: dict (default: {})\\n        Options for the axes to be created. If an axis labeled \\'counts\\' is\\n        required for that mark, axes_options[\\'counts\\'] contains optional\\n        keyword arguments for the constructor of the corresponding axis type.\\n    \"\"\"\\n    kwargs[\\'sample\\'] = sample\\n    scales = kwargs.pop(\\'scales\\', {})\\n    if \\'count\\' not in scales:\\n        dimension = _get_attribute_dimension(\\'count\\', Hist)\\n        if dimension in _context[\\'scales\\']:\\n            scales[\\'count\\'] = _context[\\'scales\\'][dimension]\\n        else:\\n            scales[\\'count\\'] = LinearScale(**options.get(\\'count\\', {}))\\n            _context[\\'scales\\'][dimension] = scales[\\'count\\']\\n    kwargs[\\'scales\\'] = scales\\n    return _draw_mark(Hist, options=options, **kwargs)', 'def bin(sample, options={}, **kwargs):\\n    \"\"\"Draw a histogram in the current context figure.\\n    Parameters\\n    ----------\\n    sample: numpy.ndarray, 1d\\n        The sample for which the histogram must be generated.\\n    options: dict (default: {})\\n        Options for the scales to be created. If a scale labeled \\'x\\'\\n        is required for that mark, options[\\'x\\'] contains optional keyword\\n        arguments for the constructor of the corresponding scale type.\\n    axes_options: dict (default: {})\\n        Options for the axes to be created. If an axis labeled \\'x\\' is\\n        required for that mark, axes_options[\\'x\\'] contains optional\\n        keyword arguments for the constructor of the corresponding axis type.\\n    \"\"\"\\n    kwargs[\\'sample\\'] = sample\\n    scales = kwargs.pop(\\'scales\\', {})\\n    for xy in [\\'x\\', \\'y\\']:\\n        if xy not in scales:\\n            dimension = _get_attribute_dimension(xy, Bars)\\n            if dimension in _context[\\'scales\\']:\\n                scales[xy] = _context[\\'scales\\'][dimension]\\n            else:\\n                scales[xy] = LinearScale(**options.get(xy, {}))\\n                _context[\\'scales\\'][dimension] = scales[xy]\\n    kwargs[\\'scales\\'] = scales\\n    return _draw_mark(Bins, options=options, **kwargs)', 'def bar(x, y, **kwargs):\\n    \"\"\"Draws a bar chart in the current context figure.\\n\\n    Parameters\\n    ----------\\n\\n    x: numpy.ndarray, 1d\\n        The x-coordinates of the data points.\\n    y: numpy.ndarray, 1d\\n        The y-coordinates of the data pints.\\n    options: dict (default: {})\\n        Options for the scales to be created. If a scale labeled \\'x\\' is\\n        required for that mark, options[\\'x\\'] contains optional keyword\\n        arguments for the constructor of the corresponding scale type.\\n    axes_options: dict (default: {})\\n        Options for the axes to be created. If an axis labeled \\'x\\' is required\\n        for that mark, axes_options[\\'x\\'] contains optional keyword arguments\\n        for the constructor of the corresponding axis type.\\n    \"\"\"\\n    kwargs[\\'x\\'] = x\\n    kwargs[\\'y\\'] = y\\n    return _draw_mark(Bars, **kwargs)', 'def boxplot(x, y, **kwargs):\\n    \"\"\"Draws a boxplot in the current context figure.\\n\\n    Parameters\\n    ----------\\n\\n    x: numpy.ndarray, 1d\\n        The x-coordinates of the data points.\\n    y: numpy.ndarray, 2d\\n        The data from which the boxes are to be created. Each row of the data\\n        corresponds to one box drawn in the plot.\\n    options: dict (default: {})\\n        Options for the scales to be created. If a scale labeled \\'x\\' is\\n        required for that mark, options[\\'x\\'] contains optional keyword\\n        arguments for the constructor of the corresponding scale type.\\n    axes_options: dict (default: {})\\n        Options for the axes to be created. If an axis labeled \\'x\\' is required\\n        for that mark, axes_options[\\'x\\'] contains optional keyword arguments\\n        for the constructor of the corresponding axis type.\\n    \"\"\"\\n    kwargs[\\'x\\'] = x\\n    kwargs[\\'y\\'] = y\\n    return _draw_mark(Boxplot, **kwargs)', 'def geo(map_data, **kwargs):\\n    \"\"\"Draw a map in the current context figure.\\n\\n    Parameters\\n    ----------\\n    map_data: string or bqplot.map (default: WorldMap)\\n        Name of the map or json file required for the map data.\\n    options: dict (default: {})\\n        Options for the scales to be created. If a scale labeled \\'x\\' is\\n        required for that mark, options[\\'x\\'] contains optional keyword\\n        arguments for the constructor of the corresponding scale type.\\n    axes_options: dict (default: {})\\n        Options for the axes to be created. If an axis labeled \\'x\\' is required\\n        for that mark, axes_options[\\'x\\'] contains optional keyword arguments\\n        for the constructor of the corresponding axis type.\\n    \"\"\"\\n    scales = kwargs.pop(\\'scales\\', _context[\\'scales\\'])\\n    options = kwargs.get(\\'options\\', {})\\n    if \\'projection\\' not in scales:\\n        scales[\\'projection\\'] = Mercator(**options.get(\\'projection\\', {}))\\n    kwargs[\\'scales\\'] = scales\\n    if isinstance(map_data, string_types):\\n        kwargs[\\'map_data\\'] = topo_load(\\'map_data/\\' + map_data + \\'.json\\')\\n    else:\\n        kwargs[\\'map_data\\'] = map_data\\n    return _draw_mark(Map, **kwargs)', 'def _add_interaction(int_type, **kwargs):\\n    \"\"\"Add the interaction for the specified type.\\n\\n    If a figure is passed using the key-word argument `figure` it is used. Else\\n    the context figure is used.\\n    If a list of marks are passed using the key-word argument `marks` it\\n    is used. Else the latest mark that is passed is used as the only mark\\n    associated with the selector.\\n\\n    Parameters\\n    ----------\\n    int_type: type\\n        The type of interaction to be added.\\n    \"\"\"\\n\\n    fig = kwargs.pop(\\'figure\\', current_figure())\\n    marks = kwargs.pop(\\'marks\\', [_context[\\'last_mark\\']])\\n\\n    for name, traitlet in int_type.class_traits().items():\\n        dimension = traitlet.get_metadata(\\'dimension\\')\\n        if dimension is not None:\\n            # only scales have this attribute in interactions\\n            kwargs[name] = _get_context_scale(dimension)\\n    kwargs[\\'marks\\'] = marks\\n    interaction = int_type(**kwargs)\\n    if fig.interaction is not None:\\n        fig.interaction.close()\\n    fig.interaction = interaction\\n    return interaction', 'def _create_selector(int_type, func, trait, **kwargs):\\n    \"\"\"Create a selector of the specified type.\\n\\n    Also attaches the function `func` as an `on_trait_change` listener\\n    for the trait `trait` of the selector.\\n\\n    This is an internal function which should not be called by the user.\\n\\n    Parameters\\n    ----------\\n\\n    int_type: type\\n        The type of selector to be added.\\n    func: function\\n        The call back function. It should take atleast two arguments. The name\\n        of the trait and the value of the trait are passed as arguments.\\n    trait: string\\n        The name of the Selector trait whose change triggers the\\n        call back function `func`.\\n    \"\"\"\\n    interaction = _add_interaction(int_type, **kwargs)\\n    if func is not None:\\n        interaction.on_trait_change(func, trait)\\n    return interaction', 'def clear():\\n    \"\"\"Clears the current context figure of all marks axes and grid lines.\"\"\"\\n    fig = _context[\\'figure\\']\\n    if fig is not None:\\n        fig.marks = []\\n        fig.axes = []\\n        setattr(fig, \\'axis_registry\\', {})\\n        _context[\\'scales\\'] = {}\\n        key = _context[\\'current_key\\']\\n        if key is not None:\\n            _context[\\'scale_registry\\'][key] = {}', 'def set_context(context):\\n    \"\"\"Sets the current global context dictionary. All the attributes to be set\\n    should be set. Otherwise, it will result in unpredictable behavior.\"\"\"\\n    global _context\\n    _context = {k: v for k, v in context.items()}', 'def _get_attribute_dimension(trait_name, mark_type=None):\\n    \"\"\"Returns the dimension for the name of the trait for the specified mark.\\n\\n    If `mark_type` is `None`, then the `trait_name` is returned\\n    as is.\\n    Returns `None` if the `trait_name` is not valid for `mark_type`.\\n    \"\"\"\\n    if(mark_type is None):\\n        return trait_name\\n    scale_metadata = mark_type.class_traits()[\\'scales_metadata\\']\\\\\\n        .default_args[0]\\n    return scale_metadata.get(trait_name, {}).get(\\'dimension\\', None)', 'def _apply_properties(widget, properties={}):\\n    \"\"\"Applies the specified properties to the widget.\\n\\n    `properties` is a dictionary with key value pairs corresponding\\n    to the properties to be applied to the widget.\\n    \"\"\"\\n    with widget.hold_sync():\\n        for key, value in properties.items():\\n            setattr(widget, key, value)', 'def _get_line_styles(marker_str):\\n    \"\"\"Return line style, color and marker type from specified marker string.\\n\\n    For example, if ``marker_str`` is \\'g-o\\' then the method returns\\n    ``(\\'solid\\', \\'green\\', \\'circle\\')``.\\n    \"\"\"\\n    def _extract_marker_value(marker_str, code_dict):\\n        \"\"\"Extracts the marker value from a given marker string.\\n\\n        Looks up the `code_dict` and returns the corresponding marker for a\\n        specific code.\\n\\n        For example if `marker_str` is \\'g-o\\' then the method extracts\\n        - \\'green\\' if the code_dict is color_codes,\\n        - \\'circle\\' if the code_dict is marker_codes etc.\\n        \"\"\"\\n        val = None\\n        for code in code_dict:\\n            if code in marker_str:\\n                val = code_dict[code]\\n                break\\n        return val\\n\\n    return [_extract_marker_value(marker_str, code_dict) for\\n            code_dict in [LINE_STYLE_CODES, COLOR_CODES, MARKER_CODES]]', 'async def collect(self):\\n        \"\"\"\\n        Create a `self` iterator and collect it into a `TotalList`\\n        (a normal list with a `.total` attribute).\\n        \"\"\"\\n        result = helpers.TotalList()\\n        async for message in self:\\n            result.append(message)\\n\\n        result.total = self.total\\n        return result', 'async def handler(event):\\n    \"\"\"#full: Advises to read \"Accessing the full API\" in the docs.\"\"\"\\n    await asyncio.wait([\\n        event.delete(),\\n        event.respond(READ_FULL, reply_to=event.reply_to_msg_id)\\n    ])', 'async def handler(event):\\n    \"\"\"#search query: Searches for \"query\" in the method reference.\"\"\"\\n    query = urllib.parse.quote(event.pattern_match.group(1))\\n    await asyncio.wait([\\n        event.delete(),\\n        event.respond(SEARCH.format(query), reply_to=event.reply_to_msg_id)\\n    ])', 'async def handler(event):\\n    \"\"\"#docs or #ref query: Like #search but shows the query.\"\"\"\\n    q1 = event.pattern_match.group(1)\\n    q2 = urllib.parse.quote(q1)\\n    await asyncio.wait([\\n        event.delete(),\\n        event.respond(DOCS.format(q1, q2), reply_to=event.reply_to_msg_id)\\n    ])', 'async def handler(event):\\n    \"\"\"#rtd: Tells the user to please read the docs.\"\"\"\\n    rtd = RTFD if event.pattern_match.group(1) else RTD\\n    await asyncio.wait([\\n        event.delete(),\\n        event.respond(rtd, reply_to=event.reply_to_msg_id)\\n    ])', 'async def handler(event):\\n    \"\"\"#client or #msg query: Looks for the given attribute in RTD.\"\"\"\\n    await event.delete()\\n\\n    await event.respond(\\n        get_docs_message(kind=event.pattern_match.group(1),\\n                         query=event.pattern_match.group(2)),\\n        reply_to=event.reply_to_msg_id\\n    )', 'async def handler(event):\\n    \"\"\"#ot, #offtopic: Tells the user to move to @TelethonOffTopic.\"\"\"\\n    await asyncio.wait([\\n        event.delete(),\\n        event.respond(OFFTOPIC[event.chat_id], reply_to=event.reply_to_msg_id)\\n    ])', 'async def handler(event):\\n    \"\"\"#learn or #python: Tells the user to learn some Python first.\"\"\"\\n    await asyncio.wait([\\n        event.delete(),\\n        event.respond(\\n            LEARN_PYTHON, reply_to=event.reply_to_msg_id, link_preview=False)\\n    ])', 'def build_reply_markup(self, buttons, inline_only=False):\\n        \"\"\"\\n        Builds a :tl`ReplyInlineMarkup` or :tl:`ReplyKeyboardMarkup` for\\n        the given buttons, or does nothing if either no buttons are\\n        provided or the provided argument is already a reply markup.\\n\\n        This will add any event handlers defined in the\\n        buttons and delete old ones not to call them twice,\\n        so you should probably call this method manually for\\n        serious bots instead re-adding handlers every time you\\n        send a message. Magic can only go so far.\\n        \"\"\"\\n        if buttons is None:\\n            return None\\n\\n        try:\\n            if buttons.SUBCLASS_OF_ID == 0xe2e10ef2:\\n                return buttons  # crc32(b\\'ReplyMarkup\\'):\\n        except AttributeError:\\n            pass\\n\\n        if not utils.is_list_like(buttons):\\n            buttons = [[buttons]]\\n        elif not utils.is_list_like(buttons[0]):\\n            buttons = [buttons]\\n\\n        is_inline = False\\n        is_normal = False\\n        resize = None\\n        single_use = None\\n        selective = None\\n\\n        rows = []\\n        for row in buttons:\\n            current = []\\n            for button in row:\\n                if isinstance(button, custom.Button):\\n                    if button.resize is not None:\\n                        resize = button.resize\\n                    if button.single_use is not None:\\n                        single_use = button.single_use\\n                    if button.selective is not None:\\n                        selective = button.selective\\n\\n                    button = button.button\\n                elif isinstance(button, custom.MessageButton):\\n                    button = button.button\\n\\n                inline = custom.Button._is_inline(button)\\n                is_inline |= inline\\n                is_normal |= not inline\\n\\n                if button.SUBCLASS_OF_ID == 0xbad74a3:\\n                    # 0xbad74a3 == crc32(b\\'KeyboardButton\\')\\n                    current.append(button)\\n\\n            if current:\\n                rows.append(types.KeyboardButtonRow(current))\\n\\n        if inline_only and is_normal:\\n            raise ValueError(\\'You cannot use non-inline buttons here\\')\\n        elif is_inline == is_normal and is_normal:\\n            raise ValueError(\\'You cannot mix inline with normal buttons\\')\\n        elif is_inline:\\n            return types.ReplyInlineMarkup(rows)\\n        # elif is_normal:\\n        return types.ReplyKeyboardMarkup(\\n            rows, resize=resize, single_use=single_use, selective=selective)', 'def pretty_format(obj, indent=None):\\n        \"\"\"\\n        Pretty formats the given object as a string which is returned.\\n        If indent is None, a single line will be returned.\\n        \"\"\"\\n        if indent is None:\\n            if isinstance(obj, TLObject):\\n                obj = obj.to_dict()\\n\\n            if isinstance(obj, dict):\\n                return \\'{}({})\\'.format(obj.get(\\'_\\', \\'dict\\'), \\', \\'.join(\\n                    \\'{}={}\\'.format(k, TLObject.pretty_format(v))\\n                    for k, v in obj.items() if k != \\'_\\'\\n                ))\\n            elif isinstance(obj, str) or isinstance(obj, bytes):\\n                return repr(obj)\\n            elif hasattr(obj, \\'__iter__\\'):\\n                return \\'[{}]\\'.format(\\n                    \\', \\'.join(TLObject.pretty_format(x) for x in obj)\\n                )\\n            else:\\n                return repr(obj)\\n        else:\\n            result = []\\n            if isinstance(obj, TLObject):\\n                obj = obj.to_dict()\\n\\n            if isinstance(obj, dict):\\n                result.append(obj.get(\\'_\\', \\'dict\\'))\\n                result.append(\\'(\\')\\n                if obj:\\n                    result.append(\\'\\\\n\\')\\n                    indent += 1\\n                    for k, v in obj.items():\\n                        if k == \\'_\\':\\n                            continue\\n                        result.append(\\'\\\\t\\' * indent)\\n                        result.append(k)\\n                        result.append(\\'=\\')\\n                        result.append(TLObject.pretty_format(v, indent))\\n                        result.append(\\',\\\\n\\')\\n                    result.pop()  # last \\',\\\\n\\'\\n                    indent -= 1\\n                    result.append(\\'\\\\n\\')\\n                    result.append(\\'\\\\t\\' * indent)\\n                result.append(\\')\\')\\n\\n            elif isinstance(obj, str) or isinstance(obj, bytes):\\n                result.append(repr(obj))\\n\\n            elif hasattr(obj, \\'__iter__\\'):\\n                result.append(\\'[\\\\n\\')\\n                indent += 1\\n                for x in obj:\\n                    result.append(\\'\\\\t\\' * indent)\\n                    result.append(TLObject.pretty_format(x, indent))\\n                    result.append(\\',\\\\n\\')\\n                indent -= 1\\n                result.append(\\'\\\\t\\' * indent)\\n                result.append(\\']\\')\\n\\n            else:\\n                result.append(repr(obj))\\n\\n            return \\'\\'.join(result)', 'def serialize_bytes(data):\\n        \"\"\"Write bytes by using Telegram guidelines\"\"\"\\n        if not isinstance(data, bytes):\\n            if isinstance(data, str):\\n                data = data.encode(\\'utf-8\\')\\n            else:\\n                raise TypeError(\\n                    \\'bytes or str expected, not {}\\'.format(type(data)))\\n\\n        r = []\\n        if len(data) < 254:\\n            padding = (len(data) + 1) % 4\\n            if padding != 0:\\n                padding = 4 - padding\\n\\n            r.append(bytes([len(data)]))\\n            r.append(data)\\n\\n        else:\\n            padding = len(data) % 4\\n            if padding != 0:\\n                padding = 4 - padding\\n\\n            r.append(bytes([\\n                254,\\n                len(data) % 256,\\n                (len(data) >> 8) % 256,\\n                (len(data) >> 16) % 256\\n            ]))\\n            r.append(data)\\n\\n        r.append(bytes(padding))\\n        return b\\'\\'.join(r)', 'def to_json(self, fp=None, default=_json_default, **kwargs):\\n        \"\"\"\\n        Represent the current `TLObject` as JSON.\\n\\n        If ``fp`` is given, the JSON will be dumped to said\\n        file pointer, otherwise a JSON string will be returned.\\n\\n        Note that bytes and datetimes cannot be represented\\n        in JSON, so if those are found, they will be base64\\n        encoded and ISO-formatted, respectively, by default.\\n        \"\"\"\\n        d = self.to_dict()\\n        if fp:\\n            return json.dump(d, fp, default=default, **kwargs)\\n        else:\\n            return json.dumps(d, default=default, **kwargs)', 'async def _replace_with_mention(self, entities, i, user):\\n        \"\"\"\\n        Helper method to replace ``entities[i]`` to mention ``user``,\\n        or do nothing if it can\\'t be found.\\n        \"\"\"\\n        try:\\n            entities[i] = types.InputMessageEntityMentionName(\\n                entities[i].offset, entities[i].length,\\n                await self.get_input_entity(user)\\n            )\\n            return True            \\n        except (ValueError, TypeError):\\n            return False', 'async def _parse_message_text(self, message, parse_mode):\\n        \"\"\"\\n        Returns a (parsed message, entities) tuple depending on ``parse_mode``.\\n        \"\"\"\\n        if parse_mode is ():\\n            parse_mode = self._parse_mode\\n        else:\\n            parse_mode = utils.sanitize_parse_mode(parse_mode)\\n\\n        if not parse_mode:\\n            return message, []\\n\\n        message, msg_entities = parse_mode.parse(message)\\n        for i in reversed(range(len(msg_entities))):\\n            e = msg_entities[i]\\n            if isinstance(e, types.MessageEntityTextUrl):\\n                m = re.match(r\\'^@|\\\\+|tg://user\\\\?id=(\\\\d+)\\', e.url)\\n                if m:\\n                    user = int(m.group(1)) if m.group(1) else e.url\\n                    is_mention = await self._replace_with_mention(msg_entities, i, user)\\n                    if not is_mention:\\n                        del msg_entities[i]\\n            elif isinstance(e, (types.MessageEntityMentionName,\\n                                types.InputMessageEntityMentionName)):\\n                is_mention = await self._replace_with_mention(msg_entities, i, e.user_id)\\n                if not is_mention:\\n                    del msg_entities[i]\\n\\n        return message, msg_entities', 'def _get_response_message(self, request, result, input_chat):\\n        \"\"\"\\n        Extracts the response message known a request and Update result.\\n        The request may also be the ID of the message to match.\\n\\n        If ``request is None`` this method returns ``{id: message}``.\\n\\n        If ``request.random_id`` is a list, this method returns a list too.\\n        \"\"\"\\n        if isinstance(result, types.UpdateShort):\\n            updates = [result.update]\\n            entities = {}\\n        elif isinstance(result, (types.Updates, types.UpdatesCombined)):\\n            updates = result.updates\\n            entities = {utils.get_peer_id(x): x\\n                        for x in\\n                        itertools.chain(result.users, result.chats)}\\n        else:\\n            return None\\n\\n        random_to_id = {}\\n        id_to_message = {}\\n        for update in updates:\\n            if isinstance(update, types.UpdateMessageID):\\n                random_to_id[update.random_id] = update.id\\n\\n            elif isinstance(update, (\\n                    types.UpdateNewChannelMessage, types.UpdateNewMessage)):\\n                update.message._finish_init(self, entities, input_chat)\\n                id_to_message[update.message.id] = update.message\\n\\n            elif (isinstance(update, types.UpdateEditMessage)\\n                  and not isinstance(request.peer, types.InputPeerChannel)):\\n                if request.id == update.message.id:\\n                    update.message._finish_init(self, entities, input_chat)\\n                    return update.message\\n\\n            elif (isinstance(update, types.UpdateEditChannelMessage)\\n                  and utils.get_peer_id(request.peer) ==\\n                  utils.get_peer_id(update.message.to_id)):\\n                if request.id == update.message.id:\\n                    update.message._finish_init(self, entities, input_chat)\\n                    return update.message\\n\\n        if request is None:\\n            return id_to_message\\n\\n        random_id = request if isinstance(request, int) else request.random_id\\n        if not utils.is_list_like(random_id):\\n            if random_id in random_to_id:\\n                return id_to_message[random_to_id[random_id]]\\n            else:\\n                return None\\n        else:\\n            # ``rnd in random_to_id`` is needed because trying to forward only\\n            # deleted messages causes `MESSAGE_ID_INVALID`, but forwarding\\n            # valid and invalid messages in the same call makes the call\\n            # succeed, although the API won\\'t return those messages thus\\n            # `random_to_id[rnd]` would `KeyError`.\\n            return [id_to_message[random_to_id[rnd]]\\n                    if rnd in random_to_id else None\\n                    for rnd in random_id]', 'async def get_sender(self):\\n        \"\"\"\\n        Returns `sender`, but will make an API call to find the\\n        sender unless it\\'s already cached.\\n        \"\"\"\\n        # ``sender.min`` is present both in :tl:`User` and :tl:`Channel`.\\n        # It\\'s a flag that will be set if only minimal information is\\n        # available (such as display name, but username may be missing),\\n        # in which case we want to force fetch the entire thing because\\n        # the user explicitly called a method. If the user is okay with\\n        # cached information, they may use the property instead.\\n        if (self._sender is None or self._sender.min) \\\\\\n                and await self.get_input_sender():\\n            try:\\n                self._sender =\\\\\\n                    await self._client.get_entity(self._input_sender)\\n            except ValueError:\\n                await self._reload_message()\\n        return self._sender', 'def input_sender(self):\\n        \"\"\"\\n        This :tl:`InputPeer` is the input version of the user/channel who\\n        sent the message. Similarly to `input_chat`, this doesn\\'t have\\n        things like username or similar, but still useful in some cases.\\n\\n        Note that this might not be available if the library can\\'t\\n        find the input chat, or if the message a broadcast on a channel.\\n        \"\"\"\\n        if self._input_sender is None and self._sender_id:\\n            try:\\n                self._input_sender = self._client.session\\\\\\n                    .get_input_entity(self._sender_id)\\n            except ValueError:\\n                pass\\n        return self._input_sender', 'async def get_input_sender(self):\\n        \"\"\"\\n        Returns `input_sender`, but will make an API call to find the\\n        input sender unless it\\'s already cached.\\n        \"\"\"\\n        if self.input_sender is None and self._sender_id:\\n            await self._refetch_sender()\\n        return self._input_sender', 'def register(event=None):\\n    \"\"\"\\n    Decorator method to *register* event handlers. This is the client-less\\n    `add_event_handler\\n    <telethon.client.updates.UpdateMethods.add_event_handler>` variant.\\n\\n    Note that this method only registers callbacks as handlers,\\n    and does not attach them to any client. This is useful for\\n    external modules that don\\'t have access to the client, but\\n    still want to define themselves as a handler. Example:\\n\\n    >>> from telethon import events\\n    >>> @events.register(events.NewMessage)\\n    ... async def handler(event):\\n    ...     ...\\n    ...\\n    >>> # (somewhere else)\\n    ...\\n    >>> from telethon import TelegramClient\\n    >>> client = TelegramClient(...)\\n    >>> client.add_event_handler(handler)\\n\\n    Remember that you can use this as a non-decorator\\n    through ``register(event)(callback)``.\\n\\n    Args:\\n        event (`_EventBuilder` | `type`):\\n            The event builder class or instance to be used,\\n            for instance ``events.NewMessage``.\\n    \"\"\"\\n    if isinstance(event, type):\\n        event = event()\\n    elif not event:\\n        event = Raw()\\n\\n    def decorator(callback):\\n        handlers = getattr(callback, _HANDLERS_ATTRIBUTE, [])\\n        handlers.append(event)\\n        setattr(callback, _HANDLERS_ATTRIBUTE, handlers)\\n        return callback\\n\\n    return decorator', 'def unregister(callback, event=None):\\n    \"\"\"\\n    Inverse operation of `register` (though not a decorator). Client-less\\n    `remove_event_handler\\n    <telethon.client.updates.UpdateMethods.remove_event_handler>`\\n    variant. **Note that this won\\'t remove handlers from the client**,\\n    because it simply can\\'t, so you would generally use this before\\n    adding the handlers to the client.\\n\\n    This method is here for symmetry. You will rarely need to\\n    unregister events, since you can simply just not add them\\n    to any client.\\n\\n    If no event is given, all events for this callback are removed.\\n    Returns how many callbacks were removed.\\n    \"\"\"\\n    found = 0\\n    if event and not isinstance(event, type):\\n        event = type(event)\\n\\n    handlers = getattr(callback, _HANDLERS_ATTRIBUTE, [])\\n    handlers.append((event, callback))\\n    i = len(handlers)\\n    while i:\\n        i -= 1\\n        ev = handlers[i]\\n        if not event or isinstance(ev, event):\\n            del handlers[i]\\n            found += 1\\n\\n    return found', 'def parse(message, delimiters=None, url_re=None):\\n    \"\"\"\\n    Parses the given markdown message and returns its stripped representation\\n    plus a list of the MessageEntity\\'s that were found.\\n\\n    :param message: the message with markdown-like syntax to be parsed.\\n    :param delimiters: the delimiters to be used, {delimiter: type}.\\n    :param url_re: the URL bytes regex to be used. Must have two groups.\\n    :return: a tuple consisting of (clean message, [message entities]).\\n    \"\"\"\\n    if not message:\\n        return message, []\\n\\n    if url_re is None:\\n        url_re = DEFAULT_URL_RE\\n    elif isinstance(url_re, str):\\n        url_re = re.compile(url_re)\\n\\n    if not delimiters:\\n        if delimiters is not None:\\n            return message, []\\n        delimiters = DEFAULT_DELIMITERS\\n\\n    # Cannot use a for loop because we need to skip some indices\\n    i = 0\\n    result = []\\n    current = None\\n    end_delimiter = None\\n\\n    # Work on byte level with the utf-16le encoding to get the offsets right.\\n    # The offset will just be half the index we\\'re at.\\n    message = add_surrogate(message)\\n    while i < len(message):\\n        if url_re and current is None:\\n            # If we\\'re not inside a previous match since Telegram doesn\\'t allow\\n            # nested message entities, try matching the URL from the i\\'th pos.\\n            url_match = url_re.match(message, pos=i)\\n            if url_match:\\n                # Replace the whole match with only the inline URL text.\\n                message = \\'\\'.join((\\n                    message[:url_match.start()],\\n                    url_match.group(1),\\n                    message[url_match.end():]\\n                ))\\n\\n                result.append(MessageEntityTextUrl(\\n                    offset=url_match.start(), length=len(url_match.group(1)),\\n                    url=del_surrogate(url_match.group(2))\\n                ))\\n                i += len(url_match.group(1))\\n                # Next loop iteration, don\\'t check delimiters, since\\n                # a new inline URL might be right after this one.\\n                continue\\n\\n        if end_delimiter is None:\\n            # We\\'re not expecting any delimiter, so check them all\\n            for d, m in delimiters.items():\\n                # Slice the string at the current i\\'th position to see if\\n                # it matches the current delimiter d, otherwise skip it.\\n                if message[i:i + len(d)] != d:\\n                    continue\\n\\n                if message[i + len(d):i + 2 * len(d)] == d:\\n                    # The same delimiter can\\'t be right afterwards, if\\n                    # this were the case we would match empty strings\\n                    # like `` which we don\\'t want to.\\n                    continue\\n\\n                # Get rid of the delimiter by slicing it away\\n                message = message[:i] + message[i + len(d):]\\n                if m == MessageEntityPre:\\n                    # Special case, also has \\'lang\\'\\n                    current = m(i, None, \\'\\')\\n                else:\\n                    current = m(i, None)\\n\\n                end_delimiter = d  # We expect the same delimiter.\\n                break\\n\\n        elif message[i:i + len(end_delimiter)] == end_delimiter:\\n            message = message[:i] + message[i + len(end_delimiter):]\\n            current.length = i - current.offset\\n            result.append(current)\\n            current, end_delimiter = None, None\\n            # Don\\'t increment i here as we matched a delimiter,\\n            # and there may be a new one right after. This is\\n            # different than when encountering the first delimiter,\\n            # as we already know there won\\'t be the same right after.\\n            continue\\n\\n        # Next iteration\\n        i += 1\\n\\n    # We may have found some a delimiter but not its ending pair.\\n    # If this is the case, we want to insert the delimiter character back.\\n    if current is not None:\\n        message = (\\n            message[:current.offset]\\n            + end_delimiter\\n            + message[current.offset:]\\n        )\\n\\n    message = strip_text(message, result)\\n    return del_surrogate(message), result', 'def unparse(text, entities, delimiters=None, url_fmt=None):\\n    \"\"\"\\n    Performs the reverse operation to .parse(), effectively returning\\n    markdown-like syntax given a normal text and its MessageEntity\\'s.\\n\\n    :param text: the text to be reconverted into markdown.\\n    :param entities: the MessageEntity\\'s applied to the text.\\n    :return: a markdown-like text representing the combination of both inputs.\\n    \"\"\"\\n    if not text or not entities:\\n        return text\\n\\n    if not delimiters:\\n        if delimiters is not None:\\n            return text\\n        delimiters = DEFAULT_DELIMITERS\\n\\n    if url_fmt is None:\\n        url_fmt = DEFAULT_URL_FORMAT\\n\\n    if isinstance(entities, TLObject):\\n        entities = (entities,)\\n    else:\\n        entities = tuple(sorted(entities, key=lambda e: e.offset, reverse=True))\\n\\n    text = add_surrogate(text)\\n    delimiters = {v: k for k, v in delimiters.items()}\\n    for entity in entities:\\n        s = entity.offset\\n        e = entity.offset + entity.length\\n        delimiter = delimiters.get(type(entity), None)\\n        if delimiter:\\n            text = text[:s] + delimiter + text[s:e] + delimiter + text[e:]\\n        elif url_fmt:\\n            url = None\\n            if isinstance(entity, MessageEntityTextUrl):\\n                url = entity.url\\n            elif isinstance(entity, MessageEntityMentionName):\\n                url = \\'tg://user?id={}\\'.format(entity.user_id)\\n            if url:\\n                # It\\'s possible that entities are malformed and end up in the\\n                # middle of some character, like emoji, by using malformed\\n                # clients or bots. Try decoding the current one to check if\\n                # this is the case, and if it is, advance the entity.\\n                while e <= len(text):\\n                    try:\\n                        del_surrogate(text[s:e])\\n                        break\\n                    except UnicodeDecodeError:\\n                        e += 1\\n                else:\\n                    # Out of bounds, no luck going forward\\n                    while e > s:\\n                        try:\\n                            del_surrogate(text[s:e])\\n                            break\\n                        except UnicodeDecodeError:\\n                            e -= 1\\n                    else:\\n                        # No luck going backwards either, ignore entity\\n                        continue\\n\\n                text = (\\n                    text[:s] +\\n                    add_surrogate(url_fmt.format(text[s:e], url)) +\\n                    text[e:]\\n                )\\n\\n    return del_surrogate(text)', 'def calc_new_nonce_hash(self, new_nonce, number):\\n        \"\"\"\\n        Calculates the new nonce hash based on the current attributes.\\n\\n        :param new_nonce: the new nonce to be hashed.\\n        :param number: number to prepend before the hash.\\n        :return: the hash for the given new nonce.\\n        \"\"\"\\n        new_nonce = new_nonce.to_bytes(32, \\'little\\', signed=True)\\n        data = new_nonce + struct.pack(\\'<BQ\\', number, self.aux_hash)\\n\\n        # Calculates the message key from the given data\\n        return int.from_bytes(sha1(data).digest()[4:20], \\'little\\', signed=True)', 'def get_byte_array(integer):\\n    \"\"\"Return the variable length bytes corresponding to the given int\"\"\"\\n    # Operate in big endian (unlike most of Telegram API) since:\\n    # > \"...pq is a representation of a natural number\\n    #    (in binary *big endian* format)...\"\\n    # > \"...current value of dh_prime equals\\n    #    (in *big-endian* byte order)...\"\\n    # Reference: https://core.telegram.org/mtproto/auth_key\\n    return int.to_bytes(\\n        integer,\\n        (integer.bit_length() + 8 - 1) // 8,  # 8 bits per byte,\\n        byteorder=\\'big\\',\\n        signed=False\\n    )', 'def _compute_fingerprint(key):\\n    \"\"\"\\n    Given a RSA key, computes its fingerprint like Telegram does.\\n\\n    :param key: the Crypto.RSA key.\\n    :return: its 8-bytes-long fingerprint.\\n    \"\"\"\\n    n = TLObject.serialize_bytes(get_byte_array(key.n))\\n    e = TLObject.serialize_bytes(get_byte_array(key.e))\\n    # Telegram uses the last 8 bytes as the fingerprint\\n    return struct.unpack(\\'<q\\', sha1(n + e).digest()[-8:])[0]', 'def add_key(pub):\\n    \"\"\"Adds a new public key to be used when encrypting new data is needed\"\"\"\\n    global _server_keys\\n    key = rsa.PublicKey.load_pkcs1(pub)\\n    _server_keys[_compute_fingerprint(key)] = key', 'def encrypt(fingerprint, data):\\n    \"\"\"\\n    Encrypts the given data known the fingerprint to be used\\n    in the way Telegram requires us to do so (sha1(data) + data + padding)\\n\\n    :param fingerprint: the fingerprint of the RSA key.\\n    :param data: the data to be encrypted.\\n    :return:\\n        the cipher text, or None if no key matching this fingerprint is found.\\n    \"\"\"\\n    global _server_keys\\n    key = _server_keys.get(fingerprint, None)\\n    if not key:\\n        return None\\n\\n    # len(sha1.digest) is always 20, so we\\'re left with 255 - 20 - x padding\\n    to_encrypt = sha1(data).digest() + data + os.urandom(235 - len(data))\\n\\n    # rsa module rsa.encrypt adds 11 bits for padding which we don\\'t want\\n    # rsa module uses rsa.transform.bytes2int(to_encrypt), easier way:\\n    payload = int.from_bytes(to_encrypt, \\'big\\')\\n    encrypted = rsa.core.encrypt_int(payload, key.e, key.n)\\n    # rsa module uses transform.int2bytes(encrypted, keylength), easier:\\n    block = encrypted.to_bytes(256, \\'big\\')\\n    return block', 'async def send_message(self, *args, **kwargs):\\n        \"\"\"\\n        Sends a message in the context of this conversation. Shorthand\\n        for `telethon.client.messages.MessageMethods.send_message` with\\n        ``entity`` already set.\\n        \"\"\"\\n        message = await self._client.send_message(\\n            self._input_chat, *args, **kwargs)\\n\\n        self._outgoing.add(message.id)\\n        self._last_outgoing = message.id\\n        return message', 'def mark_read(self, message=None):\\n        \"\"\"\\n        Marks as read the latest received message if ``message is None``.\\n        Otherwise, marks as read until the given message (or message ID).\\n\\n        This is equivalent to calling `client.send_read_acknowledge\\n        <telethon.client.messages.MessageMethods.send_read_acknowledge>`.\\n        \"\"\"\\n        if message is None:\\n            if self._incoming:\\n                message = self._incoming[-1].id\\n            else:\\n                message = 0\\n        elif not isinstance(message, int):\\n            message = message.id\\n\\n        return self._client.send_read_acknowledge(\\n            self._input_chat, max_id=message)', 'async def get_response(self, message=None, *, timeout=None):\\n        \"\"\"\\n        Returns a coroutine that will resolve once a response arrives.\\n\\n        Args:\\n            message (`Message <telethon.tl.custom.message.Message>` | `int`, optional):\\n                The message (or the message ID) for which a response\\n                is expected. By default this is the last sent message.\\n\\n            timeout (`int` | `float`, optional):\\n                If present, this `timeout` (in seconds) will override the\\n                per-action timeout defined for the conversation.\\n        \"\"\"\\n        return await self._get_message(\\n            message, self._response_indices, self._pending_responses, timeout,\\n            lambda x, y: True\\n        )', 'async def get_reply(self, message=None, *, timeout=None):\\n        \"\"\"\\n        Returns a coroutine that will resolve once a reply\\n        (that is, a message being a reply) arrives. The\\n        arguments are the same as those for `get_response`.\\n        \"\"\"\\n        return await self._get_message(\\n            message, self._reply_indices, self._pending_replies, timeout,\\n            lambda x, y: x.reply_to_msg_id == y\\n        )', 'def _get_message(\\n            self, target_message, indices, pending, timeout, condition):\\n        \"\"\"\\n        Gets the next desired message under the desired condition.\\n\\n        Args:\\n            target_message (`object`):\\n                The target message for which we want to find another\\n                response that applies based on `condition`.\\n\\n            indices (`dict`):\\n                This dictionary remembers the last ID chosen for the\\n                input `target_message`.\\n\\n            pending (`dict`):\\n                This dictionary remembers {msg_id: Future} to be set\\n                once `condition` is met.\\n\\n            timeout (`int`):\\n                The timeout (in seconds) override to use for this operation.\\n\\n            condition (`callable`):\\n                The condition callable that checks if an incoming\\n                message is a valid response.\\n        \"\"\"\\n        start_time = time.time()\\n        target_id = self._get_message_id(target_message)\\n\\n        # If there is no last-chosen ID, make sure to pick one *after*\\n        # the input message, since we don\\'t want responses back in time\\n        if target_id not in indices:\\n            for i, incoming in enumerate(self._incoming):\\n                if incoming.id > target_id:\\n                    indices[target_id] = i\\n                    break\\n            else:\\n                indices[target_id] = len(self._incoming)\\n\\n        # We will always return a future from here, even if the result\\n        # can be set immediately. Otherwise, needing to await only\\n        # sometimes is an annoying edge case (i.e. we would return\\n        # a `Message` but `get_response()` always `await`\\'s).\\n        future = self._client.loop.create_future()\\n\\n        # If there are enough responses saved return the next one\\n        last_idx = indices[target_id]\\n        if last_idx < len(self._incoming):\\n            incoming = self._incoming[last_idx]\\n            if condition(incoming, target_id):\\n                indices[target_id] += 1\\n                future.set_result(incoming)\\n                return future\\n\\n        # Otherwise the next incoming response will be the one to use\\n        pending[target_id] = future\\n        return self._get_result(future, start_time, timeout)', 'async def get_edit(self, message=None, *, timeout=None):\\n        \"\"\"\\n        Awaits for an edit after the last message to arrive.\\n        The arguments are the same as those for `get_response`.\\n        \"\"\"\\n        start_time = time.time()\\n        target_id = self._get_message_id(message)\\n\\n        target_date = self._edit_dates.get(target_id, 0)\\n        earliest_edit = min(\\n            (x for x in self._incoming\\n             if x.edit_date\\n             and x.id > target_id\\n             and x.edit_date.timestamp() > target_date\\n             ),\\n            key=lambda x: x.edit_date.timestamp(),\\n            default=None\\n        )\\n\\n        if earliest_edit and earliest_edit.edit_date.timestamp() > target_date:\\n            self._edit_dates[target_id] = earliest_edit.edit_date.timestamp()\\n            return earliest_edit\\n\\n        # Otherwise the next incoming response will be the one to use\\n        future = asyncio.Future(loop=self._client.loop)\\n        self._pending_edits[target_id] = future\\n        return await self._get_result(future, start_time, timeout)', 'async def wait_read(self, message=None, *, timeout=None):\\n        \"\"\"\\n        Awaits for the sent message to be read. Note that receiving\\n        a response doesn\\'t imply the message was read, and this action\\n        will also trigger even without a response.\\n        \"\"\"\\n        start_time = time.time()\\n        future = self._client.loop.create_future()\\n        target_id = self._get_message_id(message)\\n\\n        if self._last_read is None:\\n            self._last_read = target_id - 1\\n\\n        if self._last_read >= target_id:\\n            return\\n\\n        self._pending_reads[target_id] = future\\n        return await self._get_result(future, start_time, timeout)', 'async def wait_event(self, event, *, timeout=None):\\n        \"\"\"\\n        Waits for a custom event to occur. Timeouts still apply.\\n\\n        Unless you\\'re certain that your code will run fast enough,\\n        generally you should get a \"handle\" of this special coroutine\\n        before acting. Generally, you should do this:\\n\\n        >>> from telethon import TelegramClient, events\\n        >>>\\n        >>> client = TelegramClient(...)\\n        >>>\\n        >>> async def main():\\n        >>>     async with client.conversation(...) as conv:\\n        >>>         response = conv.wait_event(events.NewMessage(incoming=True))\\n        >>>         await conv.send_message(\\'Hi\\')\\n        >>>         response = await response\\n\\n        This way your event can be registered before acting,\\n        since the response may arrive before your event was\\n        registered. It depends on your use case since this\\n        also means the event can arrive before you send\\n        a previous action.\\n        \"\"\"\\n        start_time = time.time()\\n        if isinstance(event, type):\\n            event = event()\\n\\n        await event.resolve(self._client)\\n\\n        counter = Conversation._custom_counter\\n        Conversation._custom_counter += 1\\n\\n        future = asyncio.Future(loop=self._client.loop)\\n\\n        # We need the `async def` here because we want to block on the future\\n        # from `_get_result` by using `await` on it. If we returned the future\\n        # immediately we would `del` from `_custom` too early.\\n\\n        async def result():\\n            try:\\n                return await self._get_result(future, start_time, timeout)\\n            finally:\\n                del self._custom[counter]\\n\\n        self._custom[counter] = (event, future)\\n        return await result()', 'def start(\\n            self,\\n            phone=lambda: input(\\'Please enter your phone (or bot token): \\'),\\n            password=lambda: getpass.getpass(\\'Please enter your password: \\'),\\n            *,\\n            bot_token=None, force_sms=False, code_callback=None,\\n            first_name=\\'New User\\', last_name=\\'\\', max_attempts=3):\\n        \"\"\"\\n        Convenience method to interactively connect and sign in if required,\\n        also taking into consideration that 2FA may be enabled in the account.\\n\\n        If the phone doesn\\'t belong to an existing account (and will hence\\n        `sign_up` for a new one),  **you are agreeing to Telegram\\'s\\n        Terms of Service. This is required and your account\\n        will be banned otherwise.** See https://telegram.org/tos\\n        and https://core.telegram.org/api/terms.\\n\\n        Example usage:\\n            >>> client = ...\\n            >>> client.start(phone)\\n            Please enter the code you received: 12345\\n            Please enter your password: *******\\n            (You are now logged in)\\n\\n        If the event loop is already running, this method returns a\\n        coroutine that you should await on your own code; otherwise\\n        the loop is ran until said coroutine completes.\\n\\n        Args:\\n            phone (`str` | `int` | `callable`):\\n                The phone (or callable without arguments to get it)\\n                to which the code will be sent. If a bot-token-like\\n                string is given, it will be used as such instead.\\n                The argument may be a coroutine.\\n\\n            password (`str`, `callable`, optional):\\n                The password for 2 Factor Authentication (2FA).\\n                This is only required if it is enabled in your account.\\n                The argument may be a coroutine.\\n\\n            bot_token (`str`):\\n                Bot Token obtained by `@BotFather <https://t.me/BotFather>`_\\n                to log in as a bot. Cannot be specified with ``phone`` (only\\n                one of either allowed).\\n\\n            force_sms (`bool`, optional):\\n                Whether to force sending the code request as SMS.\\n                This only makes sense when signing in with a `phone`.\\n\\n            code_callback (`callable`, optional):\\n                A callable that will be used to retrieve the Telegram\\n                login code. Defaults to `input()`.\\n                The argument may be a coroutine.\\n\\n            first_name (`str`, optional):\\n                The first name to be used if signing up. This has no\\n                effect if the account already exists and you sign in.\\n\\n            last_name (`str`, optional):\\n                Similar to the first name, but for the last. Optional.\\n\\n            max_attempts (`int`, optional):\\n                How many times the code/password callback should be\\n                retried or switching between signing in and signing up.\\n\\n        Returns:\\n            This `TelegramClient`, so initialization\\n            can be chained with ``.start()``.\\n        \"\"\"\\n        if code_callback is None:\\n            def code_callback():\\n                return input(\\'Please enter the code you received: \\')\\n        elif not callable(code_callback):\\n            raise ValueError(\\n                \\'The code_callback parameter needs to be a callable \\'\\n                \\'function that returns the code you received by Telegram.\\'\\n            )\\n\\n        if not phone and not bot_token:\\n            raise ValueError(\\'No phone number or bot token provided.\\')\\n\\n        if phone and bot_token and not callable(phone):\\n            raise ValueError(\\'Both a phone and a bot token provided, \\'\\n                             \\'must only provide one of either\\')\\n\\n        coro = self._start(\\n            phone=phone,\\n            password=password,\\n            bot_token=bot_token,\\n            force_sms=force_sms,\\n            code_callback=code_callback,\\n            first_name=first_name,\\n            last_name=last_name,\\n            max_attempts=max_attempts\\n        )\\n        return (\\n            coro if self.loop.is_running()\\n            else self.loop.run_until_complete(coro)\\n        )', 'def _parse_phone_and_hash(self, phone, phone_hash):\\n        \"\"\"\\n        Helper method to both parse and validate phone and its hash.\\n        \"\"\"\\n        phone = utils.parse_phone(phone) or self._phone\\n        if not phone:\\n            raise ValueError(\\n                \\'Please make sure to call send_code_request first.\\'\\n            )\\n\\n        phone_hash = phone_hash or self._phone_code_hash.get(phone, None)\\n        if not phone_hash:\\n            raise ValueError(\\'You also need to provide a phone_code_hash.\\')\\n\\n        return phone, phone_hash', 'async def sign_in(\\n            self, phone=None, code=None, *, password=None,\\n            bot_token=None, phone_code_hash=None):\\n        \"\"\"\\n        Starts or completes the sign in process with the given phone number\\n        or code that Telegram sent.\\n\\n        Args:\\n            phone (`str` | `int`):\\n                The phone to send the code to if no code was provided,\\n                or to override the phone that was previously used with\\n                these requests.\\n\\n            code (`str` | `int`):\\n                The code that Telegram sent. Note that if you have sent this\\n                code through the application itself it will immediately\\n                expire. If you want to send the code, obfuscate it somehow.\\n                If you\\'re not doing any of this you can ignore this note.\\n\\n            password (`str`):\\n                2FA password, should be used if a previous call raised\\n                SessionPasswordNeededError.\\n\\n            bot_token (`str`):\\n                Used to sign in as a bot. Not all requests will be available.\\n                This should be the hash the @BotFather gave you.\\n\\n            phone_code_hash (`str`, optional):\\n                The hash returned by `send_code_request`. This can be left as\\n                ``None`` to use the last hash known for the phone to be used.\\n\\n        Returns:\\n            The signed in user, or the information about\\n            :meth:`send_code_request`.\\n        \"\"\"\\n        me = await self.get_me()\\n        if me:\\n            return me\\n\\n        if phone and not code and not password:\\n            return await self.send_code_request(phone)\\n        elif code:\\n            phone, phone_code_hash = \\\\\\n                self._parse_phone_and_hash(phone, phone_code_hash)\\n\\n            # May raise PhoneCodeEmptyError, PhoneCodeExpiredError,\\n            # PhoneCodeHashEmptyError or PhoneCodeInvalidError.\\n            result = await self(functions.auth.SignInRequest(\\n                phone, phone_code_hash, str(code)))\\n        elif password:\\n            pwd = await self(functions.account.GetPasswordRequest())\\n            result = await self(functions.auth.CheckPasswordRequest(\\n                pwd_mod.compute_check(pwd, password)\\n            ))\\n        elif bot_token:\\n            result = await self(functions.auth.ImportBotAuthorizationRequest(\\n                flags=0, bot_auth_token=bot_token,\\n                api_id=self.api_id, api_hash=self.api_hash\\n            ))\\n        else:\\n            raise ValueError(\\n                \\'You must provide a phone and a code the first time, \\'\\n                \\'and a password only if an RPCError was raised before.\\'\\n            )\\n\\n        return self._on_login(result.user)', 'async def sign_up(self, code, first_name, last_name=\\'\\',\\n                      *, phone=None, phone_code_hash=None):\\n        \"\"\"\\n        Signs up to Telegram if you don\\'t have an account yet.\\n        You must call .send_code_request(phone) first.\\n\\n        **By using this method you\\'re agreeing to Telegram\\'s\\n        Terms of Service. This is required and your account\\n        will be banned otherwise.** See https://telegram.org/tos\\n        and https://core.telegram.org/api/terms.\\n\\n        Args:\\n            code (`str` | `int`):\\n                The code sent by Telegram\\n\\n            first_name (`str`):\\n                The first name to be used by the new account.\\n\\n            last_name (`str`, optional)\\n                Optional last name.\\n\\n            phone (`str` | `int`, optional):\\n                The phone to sign up. This will be the last phone used by\\n                default (you normally don\\'t need to set this).\\n\\n            phone_code_hash (`str`, optional):\\n                The hash returned by `send_code_request`. This can be left as\\n                ``None`` to use the last hash known for the phone to be used.\\n\\n        Returns:\\n            The new created :tl:`User`.\\n        \"\"\"\\n        me = await self.get_me()\\n        if me:\\n            return me\\n\\n        if self._tos and self._tos.text:\\n            if self.parse_mode:\\n                t = self.parse_mode.unparse(self._tos.text, self._tos.entities)\\n            else:\\n                t = self._tos.text\\n            sys.stderr.write(\"{}\\\\n\".format(t))\\n            sys.stderr.flush()\\n\\n        phone, phone_code_hash = \\\\\\n            self._parse_phone_and_hash(phone, phone_code_hash)\\n\\n        result = await self(functions.auth.SignUpRequest(\\n            phone_number=phone,\\n            phone_code_hash=phone_code_hash,\\n            phone_code=str(code),\\n            first_name=first_name,\\n            last_name=last_name\\n        ))\\n\\n        if self._tos:\\n            await self(\\n                functions.help.AcceptTermsOfServiceRequest(self._tos.id))\\n\\n        return self._on_login(result.user)', 'def _on_login(self, user):\\n        \"\"\"\\n        Callback called whenever the login or sign up process completes.\\n\\n        Returns the input user parameter.\\n        \"\"\"\\n        self._bot = bool(user.bot)\\n        self._self_input_peer = utils.get_input_peer(user, allow_self=False)\\n        self._authorized = True\\n\\n        return user', 'async def send_code_request(self, phone, *, force_sms=False):\\n        \"\"\"\\n        Sends a code request to the specified phone number.\\n\\n        Args:\\n            phone (`str` | `int`):\\n                The phone to which the code will be sent.\\n\\n            force_sms (`bool`, optional):\\n                Whether to force sending as SMS.\\n\\n        Returns:\\n            An instance of :tl:`SentCode`.\\n        \"\"\"\\n        phone = utils.parse_phone(phone) or self._phone\\n        phone_hash = self._phone_code_hash.get(phone)\\n\\n        if not phone_hash:\\n            try:\\n                result = await self(functions.auth.SendCodeRequest(\\n                    phone, self.api_id, self.api_hash, types.CodeSettings()))\\n            except errors.AuthRestartError:\\n                return self.send_code_request(phone, force_sms=force_sms)\\n\\n            self._tos = result.terms_of_service\\n            self._phone_code_hash[phone] = phone_hash = result.phone_code_hash\\n        else:\\n            force_sms = True\\n\\n        self._phone = phone\\n\\n        if force_sms:\\n            result = await self(\\n                functions.auth.ResendCodeRequest(phone, phone_hash))\\n\\n            self._phone_code_hash[phone] = result.phone_code_hash\\n\\n        return result', 'async def log_out(self):\\n        \"\"\"\\n        Logs out Telegram and deletes the current ``*.session`` file.\\n\\n        Returns:\\n            ``True`` if the operation was successful.\\n        \"\"\"\\n        try:\\n            await self(functions.auth.LogOutRequest())\\n        except errors.RPCError:\\n            return False\\n\\n        self._bot = None\\n        self._self_input_peer = None\\n        self._authorized = False\\n        self._state_cache.reset()\\n\\n        await self.disconnect()\\n        self.session.delete()\\n        return True', 'async def edit_2fa(\\n            self, current_password=None, new_password=None,\\n            *, hint=\\'\\', email=None, email_code_callback=None):\\n        \"\"\"\\n        Changes the 2FA settings of the logged in user, according to the\\n        passed parameters. Take note of the parameter explanations.\\n\\n        Note that this method may be *incredibly* slow depending on the\\n        prime numbers that must be used during the process to make sure\\n        that everything is safe.\\n\\n        Has no effect if both current and new password are omitted.\\n\\n        current_password (`str`, optional):\\n            The current password, to authorize changing to ``new_password``.\\n            Must be set if changing existing 2FA settings.\\n            Must **not** be set if 2FA is currently disabled.\\n            Passing this by itself will remove 2FA (if correct).\\n\\n        new_password (`str`, optional):\\n            The password to set as 2FA.\\n            If 2FA was already enabled, ``current_password`` **must** be set.\\n            Leaving this blank or ``None`` will remove the password.\\n\\n        hint (`str`, optional):\\n            Hint to be displayed by Telegram when it asks for 2FA.\\n            Leaving unspecified is highly discouraged.\\n            Has no effect if ``new_password`` is not set.\\n\\n        email (`str`, optional):\\n            Recovery and verification email. If present, you must also\\n            set `email_code_callback`, else it raises ``ValueError``.\\n\\n        email_code_callback (`callable`, optional):\\n            If an email is provided, a callback that returns the code sent\\n            to it must also be set. This callback may be asynchronous.\\n            It should return a string with the code. The length of the\\n            code will be passed to the callback as an input parameter.\\n\\n            If the callback returns an invalid code, it will raise\\n            ``CodeInvalidError``.\\n\\n        Returns:\\n            ``True`` if successful, ``False`` otherwise.\\n        \"\"\"\\n        if new_password is None and current_password is None:\\n            return False\\n\\n        if email and not callable(email_code_callback):\\n            raise ValueError(\\'email present without email_code_callback\\')\\n\\n        pwd = await self(functions.account.GetPasswordRequest())\\n        pwd.new_algo.salt1 += os.urandom(32)\\n        assert isinstance(pwd, types.account.Password)\\n        if not pwd.has_password and current_password:\\n            current_password = None\\n\\n        if current_password:\\n            password = pwd_mod.compute_check(pwd, current_password)\\n        else:\\n            password = types.InputCheckPasswordEmpty()\\n\\n        if new_password:\\n            new_password_hash = pwd_mod.compute_digest(\\n                pwd.new_algo, new_password)\\n        else:\\n            new_password_hash = b\\'\\'\\n\\n        try:\\n            await self(functions.account.UpdatePasswordSettingsRequest(\\n                password=password,\\n                new_settings=types.account.PasswordInputSettings(\\n                    new_algo=pwd.new_algo,\\n                    new_password_hash=new_password_hash,\\n                    hint=hint,\\n                    email=email,\\n                    new_secure_settings=None\\n                )\\n            ))\\n        except errors.EmailUnconfirmedError as e:\\n            code = email_code_callback(e.code_length)\\n            if inspect.isawaitable(code):\\n                code = await code\\n\\n            code = str(code)\\n            await self(functions.account.ConfirmPasswordEmailRequest(code))\\n\\n        return True', 'def _get_class_name(error_code):\\n    \"\"\"\\n    Gets the corresponding class name for the given error code,\\n    this either being an integer (thus base error name) or str.\\n    \"\"\"\\n    if isinstance(error_code, int):\\n        return KNOWN_BASE_CLASSES.get(\\n            error_code, \\'RPCError\\' + str(error_code).replace(\\'-\\', \\'Neg\\')\\n        )\\n\\n    return snake_to_camel_case(\\n        error_code.replace(\\'FIRSTNAME\\', \\'FIRST_NAME\\').lower(), suffix=\\'Error\\')', 'def parse_errors(csv_file):\\n    \"\"\"\\n    Parses the input CSV file with columns (name, error codes, description)\\n    and yields `Error` instances as a result.\\n    \"\"\"\\n    with csv_file.open(newline=\\'\\') as f:\\n        f = csv.reader(f)\\n        next(f, None)  # header\\n        for line, tup in enumerate(f, start=2):\\n            try:\\n                name, codes, description = tup\\n            except ValueError:\\n                raise ValueError(\\'Columns count mismatch, unquoted comma in \\'\\n                                 \\'desc? (line {})\\'.format(line)) from None\\n\\n            try:\\n                codes = [int(x) for x in codes.split()] or [400]\\n            except ValueError:\\n                raise ValueError(\\'Not all codes are integers \\'\\n                                 \\'(line {})\\'.format(line)) from None\\n\\n            yield Error([int(x) for x in codes], name, description)', 'async def do_authentication(sender):\\n    \"\"\"\\n    Executes the authentication process with the Telegram servers.\\n\\n    :param sender: a connected `MTProtoPlainSender`.\\n    :return: returns a (authorization key, time offset) tuple.\\n    \"\"\"\\n    # Step 1 sending: PQ Request, endianness doesn\\'t matter since it\\'s random\\n    nonce = int.from_bytes(os.urandom(16), \\'big\\', signed=True)\\n    res_pq = await sender.send(ReqPqMultiRequest(nonce))\\n    assert isinstance(res_pq, ResPQ), \\'Step 1 answer was %s\\' % res_pq\\n\\n    if res_pq.nonce != nonce:\\n        raise SecurityError(\\'Step 1 invalid nonce from server\\')\\n\\n    pq = get_int(res_pq.pq)\\n\\n    # Step 2 sending: DH Exchange\\n    p, q = Factorization.factorize(pq)\\n    p, q = rsa.get_byte_array(p), rsa.get_byte_array(q)\\n    new_nonce = int.from_bytes(os.urandom(32), \\'little\\', signed=True)\\n\\n    pq_inner_data = bytes(PQInnerData(\\n        pq=rsa.get_byte_array(pq), p=p, q=q,\\n        nonce=res_pq.nonce,\\n        server_nonce=res_pq.server_nonce,\\n        new_nonce=new_nonce\\n    ))\\n\\n    # sha_digest + data + random_bytes\\n    cipher_text, target_fingerprint = None, None\\n    for fingerprint in res_pq.server_public_key_fingerprints:\\n        cipher_text = rsa.encrypt(fingerprint, pq_inner_data)\\n        if cipher_text is not None:\\n            target_fingerprint = fingerprint\\n            break\\n\\n    if cipher_text is None:\\n        raise SecurityError(\\n            \\'Step 2 could not find a valid key for fingerprints: {}\\'\\n            .format(\\', \\'.join(\\n                [str(f) for f in res_pq.server_public_key_fingerprints])\\n            )\\n        )\\n\\n    server_dh_params = await sender.send(ReqDHParamsRequest(\\n        nonce=res_pq.nonce,\\n        server_nonce=res_pq.server_nonce,\\n        p=p, q=q,\\n        public_key_fingerprint=target_fingerprint,\\n        encrypted_data=cipher_text\\n    ))\\n\\n    assert isinstance(\\n        server_dh_params, (ServerDHParamsOk, ServerDHParamsFail)),\\\\\\n        \\'Step 2.1 answer was %s\\' % server_dh_params\\n\\n    if server_dh_params.nonce != res_pq.nonce:\\n        raise SecurityError(\\'Step 2 invalid nonce from server\\')\\n\\n    if server_dh_params.server_nonce != res_pq.server_nonce:\\n        raise SecurityError(\\'Step 2 invalid server nonce from server\\')\\n\\n    if isinstance(server_dh_params, ServerDHParamsFail):\\n        nnh = int.from_bytes(\\n            sha1(new_nonce.to_bytes(32, \\'little\\', signed=True)).digest()[4:20],\\n            \\'little\\', signed=True\\n        )\\n        if server_dh_params.new_nonce_hash != nnh:\\n            raise SecurityError(\\'Step 2 invalid DH fail nonce from server\\')\\n\\n    assert isinstance(server_dh_params, ServerDHParamsOk),\\\\\\n        \\'Step 2.2 answer was %s\\' % server_dh_params\\n\\n    # Step 3 sending: Complete DH Exchange\\n    key, iv = helpers.generate_key_data_from_nonce(\\n        res_pq.server_nonce, new_nonce\\n    )\\n    if len(server_dh_params.encrypted_answer) % 16 != 0:\\n        # See PR#453\\n        raise SecurityError(\\'Step 3 AES block size mismatch\\')\\n\\n    plain_text_answer = AES.decrypt_ige(\\n        server_dh_params.encrypted_answer, key, iv\\n    )\\n\\n    with BinaryReader(plain_text_answer) as reader:\\n        reader.read(20)  # hash sum\\n        server_dh_inner = reader.tgread_object()\\n        assert isinstance(server_dh_inner, ServerDHInnerData),\\\\\\n            \\'Step 3 answer was %s\\' % server_dh_inner\\n\\n    if server_dh_inner.nonce != res_pq.nonce:\\n        raise SecurityError(\\'Step 3 Invalid nonce in encrypted answer\\')\\n\\n    if server_dh_inner.server_nonce != res_pq.server_nonce:\\n        raise SecurityError(\\'Step 3 Invalid server nonce in encrypted answer\\')\\n\\n    dh_prime = get_int(server_dh_inner.dh_prime, signed=False)\\n    g_a = get_int(server_dh_inner.g_a, signed=False)\\n    time_offset = server_dh_inner.server_time - int(time.time())\\n\\n    b = get_int(os.urandom(256), signed=False)\\n    gb = pow(server_dh_inner.g, b, dh_prime)\\n    gab = pow(g_a, b, dh_prime)\\n\\n    # Prepare client DH Inner Data\\n    client_dh_inner = bytes(ClientDHInnerData(\\n        nonce=res_pq.nonce,\\n        server_nonce=res_pq.server_nonce,\\n        retry_id=0,  # TODO Actual retry ID\\n        g_b=rsa.get_byte_array(gb)\\n    ))\\n\\n    client_dh_inner_hashed = sha1(client_dh_inner).digest() + client_dh_inner\\n\\n    # Encryption\\n    client_dh_encrypted = AES.encrypt_ige(client_dh_inner_hashed, key, iv)\\n\\n    # Prepare Set client DH params\\n    dh_gen = await sender.send(SetClientDHParamsRequest(\\n        nonce=res_pq.nonce,\\n        server_nonce=res_pq.server_nonce,\\n        encrypted_data=client_dh_encrypted,\\n    ))\\n\\n    nonce_types = (DhGenOk, DhGenRetry, DhGenFail)\\n    assert isinstance(dh_gen, nonce_types), \\'Step 3.1 answer was %s\\' % dh_gen\\n    name = dh_gen.__class__.__name__\\n    if dh_gen.nonce != res_pq.nonce:\\n        raise SecurityError(\\'Step 3 invalid {} nonce from server\\'.format(name))\\n\\n    if dh_gen.server_nonce != res_pq.server_nonce:\\n        raise SecurityError(\\n            \\'Step 3 invalid {} server nonce from server\\'.format(name))\\n\\n    auth_key = AuthKey(rsa.get_byte_array(gab))\\n    nonce_number = 1 + nonce_types.index(type(dh_gen))\\n    new_nonce_hash = auth_key.calc_new_nonce_hash(new_nonce, nonce_number)\\n\\n    dh_hash = getattr(dh_gen, \\'new_nonce_hash{}\\'.format(nonce_number))\\n    if dh_hash != new_nonce_hash:\\n        raise SecurityError(\\'Step 3 invalid new nonce hash\\')\\n\\n    if not isinstance(dh_gen, DhGenOk):\\n        raise AssertionError(\\'Step 3.2 answer was %s\\' % dh_gen)\\n\\n    return auth_key, time_offset', 'def get_int(byte_array, signed=True):\\n    \"\"\"\\n    Gets the specified integer from its byte array.\\n    This should be used by this module alone, as it works with big endian.\\n\\n    :param byte_array: the byte array representing th integer.\\n    :param signed: whether the number is signed or not.\\n    :return: the integer representing the given byte array.\\n    \"\"\"\\n    return int.from_bytes(byte_array, byteorder=\\'big\\', signed=signed)', 'def callback(func):\\n    \"\"\"\\n    This decorator turns `func` into a callback for Tkinter\\n    to be able to use, even if `func` is an awaitable coroutine.\\n    \"\"\"\\n    @functools.wraps(func)\\n    def wrapped(*args, **kwargs):\\n        result = func(*args, **kwargs)\\n        if inspect.iscoroutine(result):\\n            aio_loop.create_task(result)\\n\\n    return wrapped', 'async def post_init(self):\\n        \"\"\"\\n        Completes the initialization of our application.\\n        Since `__init__` cannot be `async` we use this.\\n        \"\"\"\\n        if await self.cl.is_user_authorized():\\n            self.set_signed_in(await self.cl.get_me())\\n        else:\\n            # User is not logged in, configure the button to ask them to login\\n            self.sign_in_button.configure(text=\\'Sign in\\')\\n            self.sign_in_label.configure(\\n                text=\\'Sign in (phone/token):\\')', 'async def on_message(self, event):\\n        \"\"\"\\n        Event handler that will add new messages to the message log.\\n        \"\"\"\\n        # We want to show only messages sent to this chat\\n        if event.chat_id != self.chat_id:\\n            return\\n\\n        # Save the message ID so we know which to reply to\\n        self.message_ids.append(event.id)\\n\\n        # Decide a prefix (\">> \" for our messages, \"<user>\" otherwise)\\n        if event.out:\\n            text = \\'>> \\'\\n        else:\\n            sender = await event.get_sender()\\n            text = \\'<{}> \\'.format(sanitize_str(\\n                utils.get_display_name(sender)))\\n\\n        # If the message has media show \"(MediaType) \"\\n        if event.media:\\n            text += \\'({}) \\'.format(event.media.__class__.__name__)\\n\\n        text += sanitize_str(event.text)\\n        text += \\'\\\\n\\'\\n\\n        # Append the text to the end with a newline, and scroll to the end\\n        self.log.insert(tkinter.END, text)\\n        self.log.yview(tkinter.END)', 'async def sign_in(self, event=None):\\n        \"\"\"\\n        Note the `event` argument. This is required since this callback\\n        may be called from a ``widget.bind`` (such as ``\\'<Return>\\'``),\\n        which sends information about the event we don\\'t care about.\\n\\n        This callback logs out if authorized, signs in if a code was\\n        sent or a bot token is input, or sends the code otherwise.\\n        \"\"\"\\n        self.sign_in_label.configure(text=\\'Working...\\')\\n        self.sign_in_entry.configure(state=tkinter.DISABLED)\\n        if await self.cl.is_user_authorized():\\n            await self.cl.log_out()\\n            self.destroy()\\n            return\\n\\n        value = self.sign_in_entry.get().strip()\\n        if self.code:\\n            self.set_signed_in(await self.cl.sign_in(code=value))\\n        elif \\':\\' in value:\\n            self.set_signed_in(await self.cl.sign_in(bot_token=value))\\n        else:\\n            self.code = await self.cl.send_code_request(value)\\n            self.sign_in_label.configure(text=\\'Code:\\')\\n            self.sign_in_entry.configure(state=tkinter.NORMAL)\\n            self.sign_in_entry.delete(0, tkinter.END)\\n            self.sign_in_entry.focus()\\n            return', 'def set_signed_in(self, me):\\n        \"\"\"\\n        Configures the application as \"signed in\" (displays user\\'s\\n        name and disables the entry to input phone/bot token/code).\\n        \"\"\"\\n        self.me = me\\n        self.sign_in_label.configure(text=\\'Signed in\\')\\n        self.sign_in_entry.configure(state=tkinter.NORMAL)\\n        self.sign_in_entry.delete(0, tkinter.END)\\n        self.sign_in_entry.insert(tkinter.INSERT, utils.get_display_name(me))\\n        self.sign_in_entry.configure(state=tkinter.DISABLED)\\n        self.sign_in_button.configure(text=\\'Log out\\')\\n        self.chat.focus()', 'async def send_message(self, event=None):\\n        \"\"\"\\n        Sends a message. Does nothing if the client is not connected.\\n        \"\"\"\\n        if not self.cl.is_connected():\\n            return\\n\\n        # The user needs to configure a chat where the message should be sent.\\n        #\\n        # If the chat ID does not exist, it was not valid and the user must\\n        # configure one; hint them by changing the background to red.\\n        if not self.chat_id:\\n            self.chat.configure(bg=\\'red\\')\\n            self.chat.focus()\\n            return\\n\\n        # Get the message, clear the text field and focus it again\\n        text = self.message.get().strip()\\n        self.message.delete(0, tkinter.END)\\n        self.message.focus()\\n        if not text:\\n            return\\n\\n        # NOTE: This part is optional but supports editing messages\\n        #       You can remove it if you find it too complicated.\\n        #\\n        # Check if the edit matches any text\\n        m = EDIT.match(text)\\n        if m:\\n            find = re.compile(m.group(1).lstrip())\\n            # Cannot reversed(enumerate(...)), use index\\n            for i in reversed(range(len(self.sent_text))):\\n                msg_id, msg_text = self.sent_text[i]\\n                if find.search(msg_text):\\n                    # Found text to replace, so replace it and edit\\n                    new = find.sub(m.group(2), msg_text)\\n                    self.sent_text[i] = (msg_id, new)\\n                    await self.cl.edit_message(self.chat_id, msg_id, new)\\n\\n                    # Notify that a replacement was made\\n                    self.log.insert(tkinter.END, \\'(message edited: {} -> {})\\\\n\\'\\n                                    .format(msg_text, new))\\n                    self.log.yview(tkinter.END)\\n                    return\\n\\n        # Check if we want to delete the message\\n        m = DELETE.match(text)\\n        if m:\\n            try:\\n                delete = self.message_ids.pop(-int(m.group(1)))\\n            except IndexError:\\n                pass\\n            else:\\n                await self.cl.delete_messages(self.chat_id, delete)\\n                # Notify that a message was deleted\\n                self.log.insert(tkinter.END, \\'(message deleted)\\\\n\\')\\n                self.log.yview(tkinter.END)\\n                return\\n\\n        # Check if we want to reply to some message\\n        reply_to = None\\n        m = REPLY.match(text)\\n        if m:\\n            text = m.group(2)\\n            try:\\n                reply_to = self.message_ids[-int(m.group(1))]\\n            except IndexError:\\n                pass\\n\\n        # NOTE: This part is no longer optional. It sends the message.\\n        # Send the message text and get back the sent message object\\n        message = await self.cl.send_message(self.chat_id, text,\\n                                             reply_to=reply_to)\\n\\n        # Save the sent message ID and text to allow edits\\n        self.sent_text.append((message.id, text))\\n\\n        # Process the sent message as if it were an event\\n        await self.on_message(message)', 'async def check_chat(self, event=None):\\n        \"\"\"\\n        Checks the input chat where to send and listen messages from.\\n        \"\"\"\\n        if self.me is None:\\n            return  # Not logged in yet\\n\\n        chat = self.chat.get().strip()\\n        try:\\n            chat = int(chat)\\n        except ValueError:\\n            pass\\n\\n        try:\\n            old = self.chat_id\\n            # Valid chat ID, set it and configure the colour back to white\\n            self.chat_id = await self.cl.get_peer_id(chat)\\n            self.chat.configure(bg=\\'white\\')\\n\\n            # If the chat ID changed, clear the\\n            # messages that we could edit or reply\\n            if self.chat_id != old:\\n                self.message_ids.clear()\\n                self.sent_text.clear()\\n                self.log.delete(\\'1.0\\', tkinter.END)\\n                if not self.me.bot:\\n                    for msg in reversed(\\n                            await self.cl.get_messages(self.chat_id, 100)):\\n                        await self.on_message(msg)\\n        except ValueError:\\n            # Invalid chat ID, let the user know with a yellow background\\n            self.chat_id = None\\n            self.chat.configure(bg=\\'yellow\\')', 'def old(self):\\n        \"\"\"\\n        The old value from the event.\\n        \"\"\"\\n        ori = self.original.action\\n        if isinstance(ori, (\\n                types.ChannelAdminLogEventActionChangeAbout,\\n                types.ChannelAdminLogEventActionChangeTitle,\\n                types.ChannelAdminLogEventActionChangeUsername\\n        )):\\n            return ori.prev_value\\n        elif isinstance(ori, types.ChannelAdminLogEventActionChangePhoto):\\n            return ori.prev_photo\\n        elif isinstance(ori, types.ChannelAdminLogEventActionChangeStickerSet):\\n            return ori.prev_stickerset\\n        elif isinstance(ori, types.ChannelAdminLogEventActionEditMessage):\\n            return ori.prev_message\\n        elif isinstance(ori, (\\n                types.ChannelAdminLogEventActionParticipantToggleAdmin,\\n                types.ChannelAdminLogEventActionParticipantToggleBan\\n        )):\\n            return ori.prev_participant\\n        elif isinstance(ori, (\\n                types.ChannelAdminLogEventActionToggleInvites,\\n                types.ChannelAdminLogEventActionTogglePreHistoryHidden,\\n                types.ChannelAdminLogEventActionToggleSignatures\\n        )):\\n            return not ori.new_value\\n        elif isinstance(ori, types.ChannelAdminLogEventActionDeleteMessage):\\n            return ori.message\\n        elif isinstance(ori, types.ChannelAdminLogEventActionDefaultBannedRights):\\n            return ori.prev_banned_rights', 'def new(self):\\n        \"\"\"\\n        The new value present in the event.\\n        \"\"\"\\n        ori = self.original.action\\n        if isinstance(ori, (\\n                types.ChannelAdminLogEventActionChangeAbout,\\n                types.ChannelAdminLogEventActionChangeTitle,\\n                types.ChannelAdminLogEventActionChangeUsername,\\n                types.ChannelAdminLogEventActionToggleInvites,\\n                types.ChannelAdminLogEventActionTogglePreHistoryHidden,\\n                types.ChannelAdminLogEventActionToggleSignatures\\n        )):\\n            return ori.new_value\\n        elif isinstance(ori, types.ChannelAdminLogEventActionChangePhoto):\\n            return ori.new_photo\\n        elif isinstance(ori, types.ChannelAdminLogEventActionChangeStickerSet):\\n            return ori.new_stickerset\\n        elif isinstance(ori, types.ChannelAdminLogEventActionEditMessage):\\n            return ori.new_message\\n        elif isinstance(ori, (\\n                types.ChannelAdminLogEventActionParticipantToggleAdmin,\\n                types.ChannelAdminLogEventActionParticipantToggleBan\\n        )):\\n            return ori.new_participant\\n        elif isinstance(ori, types.ChannelAdminLogEventActionParticipantInvite):\\n            return ori.participant\\n        elif isinstance(ori, types.ChannelAdminLogEventActionDefaultBannedRights):\\n            return ori.new_banned_rights\\n        elif isinstance(ori, types.ChannelAdminLogEventActionStopPoll):\\n            return ori.message', 'def decrypt_ige(cipher_text, key, iv):\\n        \"\"\"\\n        Decrypts the given text in 16-bytes blocks by using the\\n        given key and 32-bytes initialization vector.\\n        \"\"\"\\n        if cryptg:\\n            return cryptg.decrypt_ige(cipher_text, key, iv)\\n        if libssl.decrypt_ige:\\n            return libssl.decrypt_ige(cipher_text, key, iv)\\n\\n        iv1 = iv[:len(iv) // 2]\\n        iv2 = iv[len(iv) // 2:]\\n\\n        aes = pyaes.AES(key)\\n\\n        plain_text = []\\n        blocks_count = len(cipher_text) // 16\\n\\n        cipher_text_block = [0] * 16\\n        for block_index in range(blocks_count):\\n            for i in range(16):\\n                cipher_text_block[i] = \\\\\\n                    cipher_text[block_index * 16 + i] ^ iv2[i]\\n\\n            plain_text_block = aes.decrypt(cipher_text_block)\\n\\n            for i in range(16):\\n                plain_text_block[i] ^= iv1[i]\\n\\n            iv1 = cipher_text[block_index * 16:block_index * 16 + 16]\\n            iv2 = plain_text_block\\n\\n            plain_text.extend(plain_text_block)\\n\\n        return bytes(plain_text)', 'def encrypt_ige(plain_text, key, iv):\\n        \"\"\"\\n        Encrypts the given text in 16-bytes blocks by using the\\n        given key and 32-bytes initialization vector.\\n        \"\"\"\\n        padding = len(plain_text) % 16\\n        if padding:\\n            plain_text += os.urandom(16 - padding)\\n\\n        if cryptg:\\n            return cryptg.encrypt_ige(plain_text, key, iv)\\n        if libssl.encrypt_ige:\\n            return libssl.encrypt_ige(plain_text, key, iv)\\n\\n        iv1 = iv[:len(iv) // 2]\\n        iv2 = iv[len(iv) // 2:]\\n\\n        aes = pyaes.AES(key)\\n\\n        cipher_text = []\\n        blocks_count = len(plain_text) // 16\\n\\n        for block_index in range(blocks_count):\\n            plain_text_block = list(\\n                plain_text[block_index * 16:block_index * 16 + 16]\\n            )\\n            for i in range(16):\\n                plain_text_block[i] ^= iv1[i]\\n\\n            cipher_text_block = aes.encrypt(plain_text_block)\\n\\n            for i in range(16):\\n                cipher_text_block[i] ^= iv2[i]\\n\\n            iv1 = cipher_text_block\\n            iv2 = plain_text[block_index * 16:block_index * 16 + 16]\\n\\n            cipher_text.extend(cipher_text_block)\\n\\n        return bytes(cipher_text)', 'def _finish_init(self, client, entities, input_chat):\\n        \"\"\"\\n        Finishes the initialization of this message by setting\\n        the client that sent the message and making use of the\\n        known entities.\\n        \"\"\"\\n        self._client = client\\n        self._sender = entities.get(self._sender_id)\\n        if self._sender:\\n            try:\\n                self._input_sender = utils.get_input_peer(self._sender)\\n            except TypeError:\\n                self._input_sender = None\\n\\n        self._chat = entities.get(self.chat_id)\\n        self._input_chat = input_chat\\n        if not self._input_chat and self._chat:\\n            try:\\n                self._input_chat = utils.get_input_peer(self._chat)\\n            except TypeError:\\n                self._input_chat = None\\n\\n        self._via_bot = entities.get(self.via_bot_id)\\n        if self._via_bot:\\n            try:\\n                self._via_input_bot = utils.get_input_peer(self._via_bot)\\n            except TypeError:\\n                self._via_input_bot = None\\n\\n        if self.fwd_from:\\n            self._forward = Forward(self._client, self.fwd_from, entities)\\n\\n        if self.action:\\n            if isinstance(self.action, (types.MessageActionChatAddUser,\\n                                        types.MessageActionChatCreate)):\\n                self._action_entities = [entities.get(i)\\n                                         for i in self.action.users]\\n            elif isinstance(self.action, types.MessageActionChatDeleteUser):\\n                self._action_entities = [entities.get(self.action.user_id)]\\n            elif isinstance(self.action, types.MessageActionChatJoinedByLink):\\n                self._action_entities = [entities.get(self.action.inviter_id)]\\n            elif isinstance(self.action, types.MessageActionChatMigrateTo):\\n                self._action_entities = [entities.get(utils.get_peer_id(\\n                    types.PeerChannel(self.action.channel_id)))]\\n            elif isinstance(\\n                    self.action, types.MessageActionChannelMigrateFrom):\\n                self._action_entities = [entities.get(utils.get_peer_id(\\n                    types.PeerChat(self.action.chat_id)))]', 'def text(self):\\n        \"\"\"\\n        The message text, formatted using the client\\'s default\\n        parse mode. Will be ``None`` for :tl:`MessageService`.\\n        \"\"\"\\n        if self._text is None and self._client:\\n            self._text = self._client.parse_mode.unparse(\\n                self.message, self.entities)\\n\\n        return self._text', 'def buttons(self):\\n        \"\"\"\\n        Returns a matrix (list of lists) containing all buttons of the message\\n        as `MessageButton <telethon.tl.custom.messagebutton.MessageButton>`\\n        instances.\\n        \"\"\"\\n        if self._buttons is None and self.reply_markup:\\n            if not self.input_chat:\\n                return\\n            try:\\n                bot = self._needed_markup_bot()\\n            except ValueError:\\n                return\\n            else:\\n                self._set_buttons(self._input_chat, bot)\\n\\n        return self._buttons', 'async def get_buttons(self):\\n        \"\"\"\\n        Returns `buttons`, but will make an API call to find the\\n        input chat (needed for the buttons) unless it\\'s already cached.\\n        \"\"\"\\n        if not self.buttons and self.reply_markup:\\n            chat = await self.get_input_chat()\\n            if not chat:\\n                return\\n            try:\\n                bot = self._needed_markup_bot()\\n            except ValueError:\\n                await self._reload_message()\\n                bot = self._needed_markup_bot()  # TODO use via_input_bot\\n\\n            self._set_buttons(chat, bot)\\n\\n        return self._buttons', 'def button_count(self):\\n        \"\"\"\\n        Returns the total button count.\\n        \"\"\"\\n        if self._buttons_count is None:\\n            if isinstance(self.reply_markup, (\\n                    types.ReplyInlineMarkup, types.ReplyKeyboardMarkup)):\\n                self._buttons_count = sum(\\n                    len(row.buttons) for row in self.reply_markup.rows)\\n            else:\\n                self._buttons_count = 0\\n\\n        return self._buttons_count', 'def photo(self):\\n        \"\"\"\\n        If the message media is a photo, this returns the :tl:`Photo` object.\\n        This will also return the photo for :tl:`MessageService` if their\\n        action is :tl:`MessageActionChatEditPhoto`.\\n        \"\"\"\\n        if isinstance(self.media, types.MessageMediaPhoto):\\n            if isinstance(self.media.photo, types.Photo):\\n                return self.media.photo\\n        elif isinstance(self.action, types.MessageActionChatEditPhoto):\\n            return self.action.photo\\n        else:\\n            web = self.web_preview\\n            if web and isinstance(web.photo, types.Photo):\\n                return web.photo', 'def document(self):\\n        \"\"\"\\n        If the message media is a document,\\n        this returns the :tl:`Document` object.\\n        \"\"\"\\n        if isinstance(self.media, types.MessageMediaDocument):\\n            if isinstance(self.media.document, types.Document):\\n                return self.media.document\\n        else:\\n            web = self.web_preview\\n            if web and isinstance(web.photo, types.Document):\\n                return web.photo', 'def web_preview(self):\\n        \"\"\"\\n        If the message has a loaded web preview,\\n        this returns the :tl:`WebPage` object.\\n        \"\"\"\\n        if isinstance(self.media, types.MessageMediaWebPage):\\n            if isinstance(self.media.webpage, types.WebPage):\\n                return self.media.webpage', 'def game(self):\\n        \"\"\"\\n        If the message media is a game, this returns the :tl:`Game`.\\n        \"\"\"\\n        if isinstance(self.media, types.MessageMediaGame):\\n            return self.media.game', 'def geo(self):\\n        \"\"\"\\n        If the message media is geo, geo live or a venue,\\n        this returns the :tl:`GeoPoint`.\\n        \"\"\"\\n        if isinstance(self.media, (types.MessageMediaGeo,\\n                                   types.MessageMediaGeoLive,\\n                                   types.MessageMediaVenue)):\\n            return self.media.geo', 'def get_entities_text(self, cls=None):\\n        \"\"\"\\n        Returns a list of tuples [(:tl:`MessageEntity`, `str`)], the string\\n        being the inner text of the message entity (like bold, italics, etc).\\n\\n        Args:\\n            cls (`type`):\\n                Returns entities matching this type only. For example,\\n                the following will print the text for all ``code`` entities:\\n\\n                >>> from telethon.tl.types import MessageEntityCode\\n                >>>\\n                >>> m = ...  # get the message\\n                >>> for _, inner_text in m.get_entities_text(MessageEntityCode):\\n                >>>     print(inner_text)\\n        \"\"\"\\n        ent = self.entities\\n        if not ent:\\n            return []\\n\\n        if cls:\\n            ent = [c for c in ent if isinstance(c, cls)]\\n\\n        texts = utils.get_inner_text(self.message, ent)\\n        return list(zip(ent, texts))', 'async def get_reply_message(self):\\n        \"\"\"\\n        The `Message` that this message is replying to, or ``None``.\\n\\n        The result will be cached after its first use.\\n        \"\"\"\\n        if self._reply_message is None:\\n            if not self.reply_to_msg_id:\\n                return None\\n\\n            # Bots cannot access other bots\\' messages by their ID.\\n            # However they can access them through replies...\\n            self._reply_message = await self._client.get_messages(\\n                await self.get_input_chat() if self.is_channel else None,\\n                ids=types.InputMessageReplyTo(self.id)\\n            )\\n            if not self._reply_message:\\n                # ...unless the current message got deleted.\\n                #\\n                # If that\\'s the case, give it a second chance accessing\\n                # directly by its ID.\\n                self._reply_message = await self._client.get_messages(\\n                    self._input_chat if self.is_channel else None,\\n                    ids=self.reply_to_msg_id\\n                )\\n\\n        return self._reply_message', 'async def respond(self, *args, **kwargs):\\n        \"\"\"\\n        Responds to the message (not as a reply). Shorthand for\\n        `telethon.client.messages.MessageMethods.send_message`\\n        with ``entity`` already set.\\n        \"\"\"\\n        return await self._client.send_message(\\n            await self.get_input_chat(), *args, **kwargs)', 'async def reply(self, *args, **kwargs):\\n        \"\"\"\\n        Replies to the message (as a reply). Shorthand for\\n        `telethon.client.messages.MessageMethods.send_message`\\n        with both ``entity`` and ``reply_to`` already set.\\n        \"\"\"\\n        kwargs[\\'reply_to\\'] = self.id\\n        return await self._client.send_message(\\n            await self.get_input_chat(), *args, **kwargs)', 'async def forward_to(self, *args, **kwargs):\\n        \"\"\"\\n        Forwards the message. Shorthand for\\n        `telethon.client.messages.MessageMethods.forward_messages`\\n        with both ``messages`` and ``from_peer`` already set.\\n\\n        If you need to forward more than one message at once, don\\'t use\\n        this `forward_to` method. Use a\\n        `telethon.client.telegramclient.TelegramClient` instance directly.\\n        \"\"\"\\n        kwargs[\\'messages\\'] = self.id\\n        kwargs[\\'from_peer\\'] = await self.get_input_chat()\\n        return await self._client.forward_messages(*args, **kwargs)', 'async def edit(self, *args, **kwargs):\\n        \"\"\"\\n        Edits the message iff it\\'s outgoing. Shorthand for\\n        `telethon.client.messages.MessageMethods.edit_message`\\n        with both ``entity`` and ``message`` already set.\\n\\n        Returns ``None`` if the message was incoming,\\n        or the edited `Message` otherwise.\\n\\n        .. note::\\n\\n            This is different from `client.edit_message\\n            <telethon.client.messages.MessageMethods.edit_message>`\\n            and **will respect** the previous state of the message.\\n            For example, if the message didn\\'t have a link preview,\\n            the edit won\\'t add one by default, and you should force\\n            it by setting it to ``True`` if you want it.\\n\\n            This is generally the most desired and convenient behaviour,\\n            and will work for link previews and message buttons.\\n        \"\"\"\\n        if self.fwd_from or not self.out:\\n            return None  # We assume self.out was patched for our chat\\n\\n        if \\'link_preview\\' not in kwargs:\\n            kwargs[\\'link_preview\\'] = bool(self.web_preview)\\n\\n        if \\'buttons\\' not in kwargs:\\n            kwargs[\\'buttons\\'] = self.reply_markup\\n\\n        return await self._client.edit_message(\\n            await self.get_input_chat(), self.id,\\n            *args, **kwargs\\n        )', 'async def delete(self, *args, **kwargs):\\n        \"\"\"\\n        Deletes the message. You\\'re responsible for checking whether you\\n        have the permission to do so, or to except the error otherwise.\\n        Shorthand for\\n        `telethon.client.messages.MessageMethods.delete_messages` with\\n        ``entity`` and ``message_ids`` already set.\\n\\n        If you need to delete more than one message at once, don\\'t use\\n        this `delete` method. Use a\\n        `telethon.client.telegramclient.TelegramClient` instance directly.\\n        \"\"\"\\n        return await self._client.delete_messages(\\n            await self.get_input_chat(), [self.id],\\n            *args, **kwargs\\n        )', 'async def download_media(self, *args, **kwargs):\\n        \"\"\"\\n        Downloads the media contained in the message, if any. Shorthand\\n        for `telethon.client.downloads.DownloadMethods.download_media`\\n        with the ``message`` already set.\\n        \"\"\"\\n        return await self._client.download_media(self, *args, **kwargs)', 'async def click(self, i=None, j=None,\\n                    *, text=None, filter=None, data=None):\\n        \"\"\"\\n        Calls `telethon.tl.custom.messagebutton.MessageButton.click`\\n        for the specified button.\\n\\n        Does nothing if the message has no buttons.\\n\\n        Args:\\n            i (`int`):\\n                Clicks the i\\'th button (starting from the index 0).\\n                Will ``raise IndexError`` if out of bounds. Example:\\n\\n                >>> message = ...  # get the message somehow\\n                >>> # Clicking the 3rd button\\n                >>> # [button1] [button2]\\n                >>> # [     button3     ]\\n                >>> # [button4] [button5]\\n                >>> message.click(2)  # index\\n\\n            j (`int`):\\n                Clicks the button at position (i, j), these being the\\n                indices for the (row, column) respectively. Example:\\n\\n                >>> # Clicking the 2nd button on the 1st row.\\n                >>> # [button1] [button2]\\n                >>> # [     button3     ]\\n                >>> # [button4] [button5]\\n                >>> message.click(0, 1)  # (row, column)\\n\\n                This is equivalent to ``message.buttons[0][1].click()``.\\n\\n            text (`str` | `callable`):\\n                Clicks the first button with the text \"text\". This may\\n                also be a callable, like a ``re.compile(...).match``,\\n                and the text will be passed to it.\\n\\n            filter (`callable`):\\n                Clicks the first button for which the callable\\n                returns ``True``. The callable should accept a single\\n                `telethon.tl.custom.messagebutton.MessageButton` argument.\\n\\n            data (`bytes`):\\n                This argument overrides the rest and will not search any\\n                buttons. Instead, it will directly send the request to\\n                behave as if it clicked a button with said data. Note\\n                that if the message does not have this data, it will\\n                ``raise DataInvalidError``.\\n        \"\"\"\\n        if data:\\n            if not await self.get_input_chat():\\n                return None\\n\\n            try:\\n                return await self._client(\\n                    functions.messages.GetBotCallbackAnswerRequest(\\n                        peer=self._input_chat,\\n                        msg_id=self.id,\\n                        data=data\\n                    )\\n                )\\n            except errors.BotTimeout:\\n                return None\\n\\n        if sum(int(x is not None) for x in (i, text, filter)) >= 2:\\n            raise ValueError(\\'You can only set either of i, text or filter\\')\\n\\n        if not await self.get_buttons():\\n            return  # Accessing the property sets self._buttons[_flat]\\n\\n        if text is not None:\\n            if callable(text):\\n                for button in self._buttons_flat:\\n                    if text(button.text):\\n                        return await button.click()\\n            else:\\n                for button in self._buttons_flat:\\n                    if button.text == text:\\n                        return await button.click()\\n            return\\n\\n        if filter is not None:\\n            for button in self._buttons_flat:\\n                if filter(button):\\n                    return await button.click()\\n            return\\n\\n        if i is None:\\n            i = 0\\n        if j is None:\\n            return await self._buttons_flat[i].click()\\n        else:\\n            return await self._buttons[i][j].click()', 'async def _reload_message(self):\\n        \"\"\"\\n        Re-fetches this message to reload the sender and chat entities,\\n        along with their input versions.\\n        \"\"\"\\n        try:\\n            chat = await self.get_input_chat() if self.is_channel else None\\n            msg = await self._client.get_messages(chat, ids=self.id)\\n        except ValueError:\\n            return  # We may not have the input chat/get message failed\\n        if not msg:\\n            return  # The message may be deleted and it will be None\\n\\n        self._sender = msg._sender\\n        self._input_sender = msg._input_sender\\n        self._chat = msg._chat\\n        self._input_chat = msg._input_chat\\n        self._via_bot = msg._via_bot\\n        self._via_input_bot = msg._via_input_bot\\n        self._forward = msg._forward\\n        self._action_entities = msg._action_entities', 'def _set_buttons(self, chat, bot):\\n        \"\"\"\\n        Helper methods to set the buttons given the input sender and chat.\\n        \"\"\"\\n        if isinstance(self.reply_markup, (\\n                types.ReplyInlineMarkup, types.ReplyKeyboardMarkup)):\\n            self._buttons = [[\\n                MessageButton(self._client, button, chat, bot, self.id)\\n                for button in row.buttons\\n            ] for row in self.reply_markup.rows]\\n            self._buttons_flat = [x for row in self._buttons for x in row]', 'def _needed_markup_bot(self):\\n        \"\"\"\\n        Returns the input peer of the bot that\\'s needed for the reply markup.\\n\\n        This is necessary for :tl:`KeyboardButtonSwitchInline` since we need\\n        to know what bot we want to start. Raises ``ValueError`` if the bot\\n        cannot be found but is needed. Returns ``None`` if it\\'s not needed.\\n        \"\"\"\\n        if not isinstance(self.reply_markup, (\\n                types.ReplyInlineMarkup, types.ReplyKeyboardMarkup)):\\n            return None\\n\\n        for row in self.reply_markup.rows:\\n            for button in row.buttons:\\n                if isinstance(button, types.KeyboardButtonSwitchInline):\\n                    if button.same_peer:\\n                        bot = self.input_sender\\n                        if not bot:\\n                            raise ValueError(\\'No input sender\\')\\n                    else:\\n                        try:\\n                            return self._client._entity_cache[self.via_bot_id]\\n                        except KeyError:\\n                            raise ValueError(\\'No input sender\\') from None', 'def _document_by_attribute(self, kind, condition=None):\\n        \"\"\"\\n        Helper method to return the document only if it has an attribute\\n        that\\'s an instance of the given kind, and passes the condition.\\n        \"\"\"\\n        doc = self.document\\n        if doc:\\n            for attr in doc.attributes:\\n                if isinstance(attr, kind):\\n                    if not condition or condition(attr):\\n                        return doc\\n                    return None', 'def make_link_node(rawtext, app, name, options):\\n    \"\"\"\\n    Create a link to the TL reference.\\n\\n    :param rawtext: Text being replaced with link node.\\n    :param app: Sphinx application context\\n    :param name: Name of the object to link to\\n    :param options: Options dictionary passed to role func.\\n    \"\"\"\\n    try:\\n        base = app.config.tl_ref_url\\n        if not base:\\n            raise AttributeError\\n    except AttributeError as e:\\n        raise ValueError(\\'tl_ref_url config value is not set\\') from e\\n\\n    if base[-1] != \\'/\\':\\n        base += \\'/\\'\\n\\n    set_classes(options)\\n    node = nodes.reference(rawtext, utils.unescape(name),\\n                           refuri=\\'{}?q={}\\'.format(base, name),\\n                           **options)\\n    return node', 'def tl_role(name, rawtext, text, lineno, inliner, options=None, content=None):\\n    \"\"\"\\n    Link to the TL reference.\\n\\n    Returns 2 part tuple containing list of nodes to insert into the\\n    document and a list of system messages. Both are allowed to be empty.\\n\\n    :param name: The role name used in the document.\\n    :param rawtext: The entire markup snippet, with role.\\n    :param text: The text marked with the role.\\n    :param lineno: The line number where rawtext appears in the input.\\n    :param inliner: The inliner instance that called us.\\n    :param options: Directive options for customization.\\n    :param content: The directive content for customization.\\n    \"\"\"\\n    if options is None:\\n        options = {}\\n    if content is None:\\n        content = []\\n\\n    # TODO Report error on type not found?\\n    # Usage:\\n    #   msg = inliner.reporter.error(..., line=lineno)\\n    #   return [inliner.problematic(rawtext, rawtext, msg)], [msg]\\n    app = inliner.document.settings.env.app\\n    node = make_link_node(rawtext, app, text, options)\\n    return [node], []', 'async def inline_query(self, bot, query, *, offset=None, geo_point=None):\\n        \"\"\"\\n        Makes the given inline query to the specified bot\\n        i.e. ``@vote My New Poll`` would be as follows:\\n\\n        >>> client = ...\\n        >>> client.inline_query(\\'vote\\', \\'My New Poll\\')\\n\\n        Args:\\n            bot (`entity`):\\n                The bot entity to which the inline query should be made.\\n\\n            query (`str`):\\n                The query that should be made to the bot.\\n\\n            offset (`str`, optional):\\n                The string offset to use for the bot.\\n\\n            geo_point (:tl:`GeoPoint`, optional)\\n                The geo point location information to send to the bot\\n                for localised results. Available under some bots.\\n\\n        Returns:\\n            A list of `custom.InlineResult\\n            <telethon.tl.custom.inlineresult.InlineResult>`.\\n        \"\"\"\\n        bot = await self.get_input_entity(bot)\\n        result = await self(functions.messages.GetInlineBotResultsRequest(\\n            bot=bot,\\n            peer=types.InputPeerEmpty(),\\n            query=query,\\n            offset=offset or \\'\\',\\n            geo_point=geo_point\\n        ))\\n\\n        return custom.InlineResults(self, result)', 'async def get_me(self, input_peer=False):\\n        \"\"\"\\n        Gets \"me\" (the self user) which is currently authenticated,\\n        or None if the request fails (hence, not authenticated).\\n\\n        Args:\\n            input_peer (`bool`, optional):\\n                Whether to return the :tl:`InputPeerUser` version or the normal\\n                :tl:`User`. This can be useful if you just need to know the ID\\n                of yourself.\\n\\n        Returns:\\n            Your own :tl:`User`.\\n        \"\"\"\\n        if input_peer and self._self_input_peer:\\n            return self._self_input_peer\\n\\n        try:\\n            me = (await self(\\n                functions.users.GetUsersRequest([types.InputUserSelf()])))[0]\\n\\n            self._bot = me.bot\\n            if not self._self_input_peer:\\n                self._self_input_peer = utils.get_input_peer(\\n                    me, allow_self=False\\n                )\\n\\n            return self._self_input_peer if input_peer else me\\n        except errors.UnauthorizedError:\\n            return None', 'async def is_bot(self):\\n        \"\"\"\\n        Return ``True`` if the signed-in user is a bot, ``False`` otherwise.\\n        \"\"\"\\n        if self._bot is None:\\n            self._bot = (await self.get_me()).bot\\n\\n        return self._bot', 'async def is_user_authorized(self):\\n        \"\"\"\\n        Returns ``True`` if the user is authorized.\\n        \"\"\"\\n        if self._authorized is None:\\n            try:\\n                # Any request that requires authorization will work\\n                await self(functions.updates.GetStateRequest())\\n                self._authorized = True\\n            except errors.RPCError:\\n                self._authorized = False\\n\\n        return self._authorized', 'async def get_entity(self, entity):\\n        \"\"\"\\n        Turns the given entity into a valid Telegram :tl:`User`, :tl:`Chat`\\n        or :tl:`Channel`. You can also pass a list or iterable of entities,\\n        and they will be efficiently fetched from the network.\\n\\n        entity (`str` | `int` | :tl:`Peer` | :tl:`InputPeer`):\\n            If a username is given, **the username will be resolved** making\\n            an API call every time. Resolving usernames is an expensive\\n            operation and will start hitting flood waits around 50 usernames\\n            in a short period of time.\\n\\n            If you want to get the entity for a *cached* username, you should\\n            first `get_input_entity(username) <get_input_entity>` which will\\n            use the cache), and then use `get_entity` with the result of the\\n            previous call.\\n\\n            Similar limits apply to invite links, and you should use their\\n            ID instead.\\n\\n            Using phone numbers (from people in your contact list), exact\\n            names, integer IDs or :tl:`Peer` rely on a `get_input_entity`\\n            first, which in turn needs the entity to be in cache, unless\\n            a :tl:`InputPeer` was passed.\\n\\n            Unsupported types will raise ``TypeError``.\\n\\n            If the entity can\\'t be found, ``ValueError`` will be raised.\\n\\n        Returns:\\n            :tl:`User`, :tl:`Chat` or :tl:`Channel` corresponding to the\\n            input entity. A list will be returned if more than one was given.\\n        \"\"\"\\n        single = not utils.is_list_like(entity)\\n        if single:\\n            entity = (entity,)\\n\\n        # Group input entities by string (resolve username),\\n        # input users (get users), input chat (get chats) and\\n        # input channels (get channels) to get the most entities\\n        # in the less amount of calls possible.\\n        inputs = []\\n        for x in entity:\\n            if isinstance(x, str):\\n                inputs.append(x)\\n            else:\\n                inputs.append(await self.get_input_entity(x))\\n\\n        users = [x for x in inputs\\n                 if isinstance(x, (types.InputPeerUser, types.InputPeerSelf))]\\n        chats = [x.chat_id for x in inputs\\n                 if isinstance(x, types.InputPeerChat)]\\n        channels = [x for x in inputs\\n                    if isinstance(x, types.InputPeerChannel)]\\n        if users:\\n            # GetUsersRequest has a limit of 200 per call\\n            tmp = []\\n            while users:\\n                curr, users = users[:200], users[200:]\\n                tmp.extend(await self(functions.users.GetUsersRequest(curr)))\\n            users = tmp\\n        if chats:  # TODO Handle chats slice?\\n            chats = (await self(\\n                functions.messages.GetChatsRequest(chats))).chats\\n        if channels:\\n            channels = (await self(\\n                functions.channels.GetChannelsRequest(channels))).chats\\n\\n        # Merge users, chats and channels into a single dictionary\\n        id_entity = {\\n            utils.get_peer_id(x): x\\n            for x in itertools.chain(users, chats, channels)\\n        }\\n\\n        # We could check saved usernames and put them into the users,\\n        # chats and channels list from before. While this would reduce\\n        # the amount of ResolveUsername calls, it would fail to catch\\n        # username changes.\\n        result = []\\n        for x in inputs:\\n            if isinstance(x, str):\\n                result.append(await self._get_entity_from_string(x))\\n            elif not isinstance(x, types.InputPeerSelf):\\n                result.append(id_entity[utils.get_peer_id(x)])\\n            else:\\n                result.append(next(\\n                    u for u in id_entity.values()\\n                    if isinstance(u, types.User) and u.is_self\\n                ))\\n\\n        return result[0] if single else result', 'async def get_input_entity(self, peer):\\n        \"\"\"\\n        Turns the given peer into its input entity version. Most requests\\n        use this kind of :tl:`InputPeer`, so this is the most suitable call\\n        to make for those cases. **Generally you should let the library do\\n        its job** and don\\'t worry about getting the input entity first, but\\n        if you\\'re going to use an entity often, consider making the call:\\n\\n        >>> import asyncio\\n        >>> rc = asyncio.get_event_loop().run_until_complete\\n        >>>\\n        >>> from telethon import TelegramClient\\n        >>> client = TelegramClient(...)\\n        >>> # If you\\'re going to use \"username\" often in your code\\n        >>> # (make a lot of calls), consider getting its input entity\\n        >>> # once, and then using the \"user\" everywhere instead.\\n        >>> user = rc(client.get_input_entity(\\'username\\'))\\n        >>> # The same applies to IDs, chats or channels.\\n        >>> chat = rc(client.get_input_entity(-123456789))\\n\\n        entity (`str` | `int` | :tl:`Peer` | :tl:`InputPeer`):\\n            If a username or invite link is given, **the library will\\n            use the cache**. This means that it\\'s possible to be using\\n            a username that *changed* or an old invite link (this only\\n            happens if an invite link for a small group chat is used\\n            after it was upgraded to a mega-group).\\n\\n            If the username or ID from the invite link is not found in\\n            the cache, it will be fetched. The same rules apply to phone\\n            numbers (``\\'+34 123456789\\'``) from people in your contact list.\\n\\n            If an exact name is given, it must be in the cache too. This\\n            is not reliable as different people can share the same name\\n            and which entity is returned is arbitrary, and should be used\\n            only for quick tests.\\n\\n            If a positive integer ID is given, the entity will be searched\\n            in cached users, chats or channels, without making any call.\\n\\n            If a negative integer ID is given, the entity will be searched\\n            exactly as either a chat (prefixed with ``-``) or as a channel\\n            (prefixed with ``-100``).\\n\\n            If a :tl:`Peer` is given, it will be searched exactly in the\\n            cache as either a user, chat or channel.\\n\\n            If the given object can be turned into an input entity directly,\\n            said operation will be done.\\n\\n            Unsupported types will raise ``TypeError``.\\n\\n            If the entity can\\'t be found, ``ValueError`` will be raised.\\n\\n        Returns:\\n            :tl:`InputPeerUser`, :tl:`InputPeerChat` or :tl:`InputPeerChannel`\\n            or :tl:`InputPeerSelf` if the parameter is ``\\'me\\'`` or ``\\'self\\'``.\\n\\n            If you need to get the ID of yourself, you should use\\n            `get_me` with ``input_peer=True``) instead.\\n        \"\"\"\\n        # Short-circuit if the input parameter directly maps to an InputPeer\\n        try:\\n            return utils.get_input_peer(peer)\\n        except TypeError:\\n            pass\\n\\n        # Next in priority is having a peer (or its ID) cached in-memory\\n        try:\\n            # 0x2d45687 == crc32(b\\'Peer\\')\\n            if isinstance(peer, int) or peer.SUBCLASS_OF_ID == 0x2d45687:\\n                return self._entity_cache[peer]\\n        except (AttributeError, KeyError):\\n            pass\\n\\n        # Then come known strings that take precedence\\n        if peer in (\\'me\\', \\'self\\'):\\n            return types.InputPeerSelf()\\n\\n        # No InputPeer, cached peer, or known string. Fetch from disk cache\\n        try:\\n            return self.session.get_input_entity(peer)\\n        except ValueError:\\n            pass\\n\\n        # Only network left to try\\n        if isinstance(peer, str):\\n            return utils.get_input_peer(\\n                await self._get_entity_from_string(peer))\\n\\n        # If we\\'re a bot and the user has messaged us privately users.getUsers\\n        # will work with access_hash = 0. Similar for channels.getChannels.\\n        # If we\\'re not a bot but the user is in our contacts, it seems to work\\n        # regardless. These are the only two special-cased requests.\\n        peer = utils.get_peer(peer)\\n        if isinstance(peer, types.PeerUser):\\n            users = await self(functions.users.GetUsersRequest([\\n                types.InputUser(peer.user_id, access_hash=0)]))\\n            if users and not isinstance(users[0], types.UserEmpty):\\n                # If the user passed a valid ID they expect to work for\\n                # channels but would be valid for users, we get UserEmpty.\\n                # Avoid returning the invalid empty input peer for that.\\n                #\\n                # We *could* try to guess if it\\'s a channel first, and if\\n                # it\\'s not, work as a chat and try to validate it through\\n                # another request, but that becomes too much work.\\n                return utils.get_input_peer(users[0])\\n        elif isinstance(peer, types.PeerChat):\\n            return types.InputPeerChat(peer.chat_id)\\n        elif isinstance(peer, types.PeerChannel):\\n            try:\\n                channels = await self(functions.channels.GetChannelsRequest([\\n                    types.InputChannel(peer.channel_id, access_hash=0)]))\\n                return utils.get_input_peer(channels.chats[0])\\n            except errors.ChannelInvalidError:\\n                pass\\n\\n        raise ValueError(\\n            \\'Could not find the input entity for {!r}. Please read https://\\'\\n            \\'telethon.readthedocs.io/en/latest/extra/basic/entities.html to\\'\\n            \\' find out more details.\\'\\n            .format(peer)\\n        )', 'async def get_peer_id(self, peer, add_mark=True):\\n        \"\"\"\\n        Gets the ID for the given peer, which may be anything entity-like.\\n\\n        This method needs to be ``async`` because `peer` supports usernames,\\n        invite-links, phone numbers (from people in your contact list), etc.\\n\\n        If ``add_mark is False``, then a positive ID will be returned\\n        instead. By default, bot-API style IDs (signed) are returned.\\n        \"\"\"\\n        if isinstance(peer, int):\\n            return utils.get_peer_id(peer, add_mark=add_mark)\\n\\n        try:\\n            if peer.SUBCLASS_OF_ID not in (0x2d45687, 0xc91c90b6):\\n                # 0x2d45687, 0xc91c90b6 == crc32(b\\'Peer\\') and b\\'InputPeer\\'\\n                peer = await self.get_input_entity(peer)\\n        except AttributeError:\\n            peer = await self.get_input_entity(peer)\\n\\n        if isinstance(peer, types.InputPeerSelf):\\n            peer = await self.get_me(input_peer=True)\\n\\n        return utils.get_peer_id(peer, add_mark=add_mark)', 'async def _get_entity_from_string(self, string):\\n        \"\"\"\\n        Gets a full entity from the given string, which may be a phone or\\n        a username, and processes all the found entities on the session.\\n        The string may also be a user link, or a channel/chat invite link.\\n\\n        This method has the side effect of adding the found users to the\\n        session database, so it can be queried later without API calls,\\n        if this option is enabled on the session.\\n\\n        Returns the found entity, or raises TypeError if not found.\\n        \"\"\"\\n        phone = utils.parse_phone(string)\\n        if phone:\\n            try:\\n                for user in (await self(\\n                        functions.contacts.GetContactsRequest(0))).users:\\n                    if user.phone == phone:\\n                        return user\\n            except errors.BotMethodInvalidError:\\n                raise ValueError(\\'Cannot get entity by phone number as a \\'\\n                                 \\'bot (try using integer IDs, not strings)\\')\\n        elif string.lower() in (\\'me\\', \\'self\\'):\\n            return await self.get_me()\\n        else:\\n            username, is_join_chat = utils.parse_username(string)\\n            if is_join_chat:\\n                invite = await self(\\n                    functions.messages.CheckChatInviteRequest(username))\\n\\n                if isinstance(invite, types.ChatInvite):\\n                    raise ValueError(\\n                        \\'Cannot get entity from a channel (or group) \\'\\n                        \\'that you are not part of. Join the group and retry\\'\\n                    )\\n                elif isinstance(invite, types.ChatInviteAlready):\\n                    return invite.chat\\n            elif username:\\n                try:\\n                    result = await self(\\n                        functions.contacts.ResolveUsernameRequest(username))\\n                except errors.UsernameNotOccupiedError as e:\\n                    raise ValueError(\\'No user has \"{}\" as username\\'\\n                                     .format(username)) from e\\n\\n                try:\\n                    pid = utils.get_peer_id(result.peer, add_mark=False)\\n                    if isinstance(result.peer, types.PeerUser):\\n                        return next(x for x in result.users if x.id == pid)\\n                    else:\\n                        return next(x for x in result.chats if x.id == pid)\\n                except StopIteration:\\n                    pass\\n            try:\\n                # Nobody with this username, maybe it\\'s an exact name/title\\n                return await self.get_entity(\\n                    self.session.get_input_entity(string))\\n            except ValueError:\\n                pass\\n\\n        raise ValueError(\\n            \\'Cannot find any entity corresponding to \"{}\"\\'.format(string)\\n        )', 'async def _get_input_dialog(self, dialog):\\n        \"\"\"\\n        Returns a :tl:`InputDialogPeer`. This is a bit tricky because\\n        it may or not need access to the client to convert what\\'s given\\n        into an input entity.\\n        \"\"\"\\n        try:\\n            if dialog.SUBCLASS_OF_ID == 0xa21c9795:  # crc32(b\\'InputDialogPeer\\')\\n                dialog.peer = await self.get_input_entity(dialog.peer)\\n                return dialog\\n            elif dialog.SUBCLASS_OF_ID == 0xc91c90b6:  # crc32(b\\'InputPeer\\')\\n                return types.InputDialogPeer(dialog)\\n        except AttributeError:\\n            pass\\n\\n        return types.InputDialogPeer(await self.get_input_entity(dialog))', 'async def _get_input_notify(self, notify):\\n        \"\"\"\\n        Returns a :tl:`InputNotifyPeer`. This is a bit tricky because\\n        it may or not need access to the client to convert what\\'s given\\n        into an input entity.\\n        \"\"\"\\n        try:\\n            if notify.SUBCLASS_OF_ID == 0x58981615:\\n                if isinstance(notify, types.InputNotifyPeer):\\n                    notify.peer = await self.get_input_entity(notify.peer)\\n                return notify\\n        except AttributeError:\\n            return types.InputNotifyPeer(await self.get_input_entity(notify))', 'def input_entity(self):\\n        \"\"\"\\n        Input version of the entity.\\n        \"\"\"\\n        if not self._input_entity:\\n            try:\\n                self._input_entity = self._client._entity_cache[self._peer]\\n            except KeyError:\\n                pass\\n\\n        return self._input_entity', 'async def get_entity(self):\\n        \"\"\"\\n        Returns `entity` but will make an API call if necessary.\\n        \"\"\"\\n        if not self.entity and await self.get_input_entity():\\n            try:\\n                self._entity =\\\\\\n                    await self._client.get_entity(self._input_entity)\\n            except ValueError:\\n                pass\\n\\n        return self._entity', 'async def set_message(\\n            self, text=None, reply_to=0, parse_mode=(),\\n            link_preview=None):\\n        \"\"\"\\n        Changes the draft message on the Telegram servers. The changes are\\n        reflected in this object.\\n\\n        :param str text: New text of the draft.\\n                         Preserved if left as None.\\n\\n        :param int reply_to: Message ID to reply to.\\n                             Preserved if left as 0, erased if set to None.\\n\\n        :param bool link_preview: Whether to attach a web page preview.\\n                                  Preserved if left as None.\\n\\n        :param str parse_mode: The parse mode to be used for the text.\\n        :return bool: ``True`` on success.\\n        \"\"\"\\n        if text is None:\\n            text = self._text\\n\\n        if reply_to == 0:\\n            reply_to = self.reply_to_msg_id\\n\\n        if link_preview is None:\\n            link_preview = self.link_preview\\n\\n        raw_text, entities =\\\\\\n            await self._client._parse_message_text(text, parse_mode)\\n\\n        result = await self._client(SaveDraftRequest(\\n            peer=self._peer,\\n            message=raw_text,\\n            no_webpage=not link_preview,\\n            reply_to_msg_id=reply_to,\\n            entities=entities\\n        ))\\n\\n        if result:\\n            self._text = text\\n            self._raw_text = raw_text\\n            self.link_preview = link_preview\\n            self.reply_to_msg_id = reply_to\\n            self.date = datetime.datetime.now(tz=datetime.timezone.utc)\\n\\n        return result', 'async def send(self, clear=True, parse_mode=()):\\n        \"\"\"\\n        Sends the contents of this draft to the dialog. This is just a\\n        wrapper around ``send_message(dialog.input_entity, *args, **kwargs)``.\\n        \"\"\"\\n        await self._client.send_message(\\n            self._peer, self.text, reply_to=self.reply_to_msg_id,\\n            link_preview=self.link_preview, parse_mode=parse_mode,\\n            clear_draft=clear\\n        )', 'async def download_profile_photo(\\n            self, entity, file=None, *, download_big=True):\\n        \"\"\"\\n        Downloads the profile photo of the given entity (user/chat/channel).\\n\\n        Args:\\n            entity (`entity`):\\n                From who the photo will be downloaded.\\n\\n                .. note::\\n\\n                    This method expects the full entity (which has the data\\n                    to download the photo), not an input variant.\\n\\n                    It\\'s possible that sometimes you can\\'t fetch the entity\\n                    from its input (since you can get errors like\\n                    ``ChannelPrivateError``) but you already have it through\\n                    another call, like getting a forwarded message from it.\\n\\n            file (`str` | `file`, optional):\\n                The output file path, directory, or stream-like object.\\n                If the path exists and is a file, it will be overwritten.\\n                If file is the type `bytes`, it will be downloaded in-memory\\n                as a bytestring (e.g. ``file=bytes``).\\n\\n            download_big (`bool`, optional):\\n                Whether to use the big version of the available photos.\\n\\n        Returns:\\n            ``None`` if no photo was provided, or if it was Empty. On success\\n            the file path is returned since it may differ from the one given.\\n        \"\"\"\\n        # hex(crc32(x.encode(\\'ascii\\'))) for x in\\n        # (\\'User\\', \\'Chat\\', \\'UserFull\\', \\'ChatFull\\')\\n        ENTITIES = (0x2da17977, 0xc5af5d94, 0x1f4661b9, 0xd49a2697)\\n        # (\\'InputPeer\\', \\'InputUser\\', \\'InputChannel\\')\\n        INPUTS = (0xc91c90b6, 0xe669bf46, 0x40f202fd)\\n        if not isinstance(entity, TLObject) or entity.SUBCLASS_OF_ID in INPUTS:\\n            entity = await self.get_entity(entity)\\n\\n        possible_names = []\\n        if entity.SUBCLASS_OF_ID not in ENTITIES:\\n            photo = entity\\n        else:\\n            if not hasattr(entity, \\'photo\\'):\\n                # Special case: may be a ChatFull with photo:Photo\\n                # This is different from a normal UserProfilePhoto and Chat\\n                if not hasattr(entity, \\'chat_photo\\'):\\n                    return None\\n\\n                return await self._download_photo(\\n                    entity.chat_photo, file, date=None, progress_callback=None)\\n\\n            for attr in (\\'username\\', \\'first_name\\', \\'title\\'):\\n                possible_names.append(getattr(entity, attr, None))\\n\\n            photo = entity.photo\\n\\n        if isinstance(photo, (types.UserProfilePhoto, types.ChatPhoto)):\\n            dc_id = photo.dc_id\\n            which = photo.photo_big if download_big else photo.photo_small\\n            loc = types.InputPeerPhotoFileLocation(\\n                peer=await self.get_input_entity(entity),\\n                local_id=which.local_id,\\n                volume_id=which.volume_id,\\n                big=download_big\\n            )\\n        else:\\n            # It doesn\\'t make any sense to check if `photo` can be used\\n            # as input location, because then this method would be able\\n            # to \"download the profile photo of a message\", i.e. its\\n            # media which should be done with `download_media` instead.\\n            return None\\n\\n        file = self._get_proper_filename(\\n            file, \\'profile_photo\\', \\'.jpg\\',\\n            possible_names=possible_names\\n        )\\n\\n        try:\\n            result = await self.download_file(loc, file, dc_id=dc_id)\\n            return result if file is bytes else file\\n        except errors.LocationInvalidError:\\n            # See issue #500, Android app fails as of v4.6.0 (1155).\\n            # The fix seems to be using the full channel chat photo.\\n            ie = await self.get_input_entity(entity)\\n            if isinstance(ie, types.InputPeerChannel):\\n                full = await self(functions.channels.GetFullChannelRequest(ie))\\n                return await self._download_photo(\\n                    full.full_chat.chat_photo, file,\\n                    date=None, progress_callback=None,\\n                    thumb=-1 if download_big else 0\\n                )\\n            else:\\n                # Until there\\'s a report for chats, no need to.\\n                return None', 'async def download_media(self, message, file=None,\\n                             *, thumb=None, progress_callback=None):\\n        \"\"\"\\n        Downloads the given media, or the media from a specified Message.\\n\\n        Note that if the download is too slow, you should consider installing\\n        ``cryptg`` (through ``pip install cryptg``) so that decrypting the\\n        received data is done in C instead of Python (much faster).\\n\\n        message (`Message <telethon.tl.custom.message.Message>` | :tl:`Media`):\\n            The media or message containing the media that will be downloaded.\\n\\n        file (`str` | `file`, optional):\\n            The output file path, directory, or stream-like object.\\n            If the path exists and is a file, it will be overwritten.\\n            If file is the type `bytes`, it will be downloaded in-memory\\n            as a bytestring (e.g. ``file=bytes``).\\n\\n        progress_callback (`callable`, optional):\\n            A callback function accepting two parameters:\\n            ``(received bytes, total)``.\\n\\n        thumb (`int` | :tl:`PhotoSize`, optional):\\n            Which thumbnail size from the document or photo to download,\\n            instead of downloading the document or photo itself.\\n\\n            If it\\'s specified but the file does not have a thumbnail,\\n            this method will return ``None``.\\n\\n            The parameter should be an integer index between ``0`` and\\n            ``len(sizes)``. ``0`` will download the smallest thumbnail,\\n            and ``len(sizes) - 1`` will download the largest thumbnail.\\n            You can also use negative indices.\\n\\n            You can also pass the :tl:`PhotoSize` instance to use.\\n\\n            In short, use ``thumb=0`` if you want the smallest thumbnail\\n            and ``thumb=-1`` if you want the largest thumbnail.\\n\\n        Returns:\\n            ``None`` if no media was provided, or if it was Empty. On success\\n            the file path is returned since it may differ from the one given.\\n        \"\"\"\\n        # TODO This won\\'t work for messageService\\n        if isinstance(message, types.Message):\\n            date = message.date\\n            media = message.media\\n        else:\\n            date = datetime.datetime.now()\\n            media = message\\n\\n        if isinstance(media, str):\\n            media = utils.resolve_bot_file_id(media)\\n\\n        if isinstance(media, types.MessageMediaWebPage):\\n            if isinstance(media.webpage, types.WebPage):\\n                media = media.webpage.document or media.webpage.photo\\n\\n        if isinstance(media, (types.MessageMediaPhoto, types.Photo)):\\n            return await self._download_photo(\\n                media, file, date, thumb, progress_callback\\n            )\\n        elif isinstance(media, (types.MessageMediaDocument, types.Document)):\\n            return await self._download_document(\\n                media, file, date, thumb, progress_callback\\n            )\\n        elif isinstance(media, types.MessageMediaContact) and thumb is None:\\n            return self._download_contact(\\n                media, file\\n            )\\n        elif isinstance(media, (types.WebDocument, types.WebDocumentNoProxy)) and thumb is None:\\n            return await self._download_web_document(\\n                media, file, progress_callback\\n            )', 'async def download_file(\\n            self, input_location, file=None, *, part_size_kb=None,\\n            file_size=None, progress_callback=None, dc_id=None):\\n        \"\"\"\\n        Downloads the given input location to a file.\\n\\n        Args:\\n            input_location (:tl:`InputFileLocation`):\\n                The file location from which the file will be downloaded.\\n                See `telethon.utils.get_input_location` source for a complete\\n                list of supported types.\\n\\n            file (`str` | `file`, optional):\\n                The output file path, directory, or stream-like object.\\n                If the path exists and is a file, it will be overwritten.\\n\\n                If the file path is ``None`` or ``bytes``, then the result\\n                will be saved in memory and returned as `bytes`.\\n\\n            part_size_kb (`int`, optional):\\n                Chunk size when downloading files. The larger, the less\\n                requests will be made (up to 512KB maximum).\\n\\n            file_size (`int`, optional):\\n                The file size that is about to be downloaded, if known.\\n                Only used if ``progress_callback`` is specified.\\n\\n            progress_callback (`callable`, optional):\\n                A callback function accepting two parameters:\\n                ``(downloaded bytes, total)``. Note that the\\n                ``total`` is the provided ``file_size``.\\n\\n            dc_id (`int`, optional):\\n                The data center the library should connect to in order\\n                to download the file. You shouldn\\'t worry about this.\\n        \"\"\"\\n        if not part_size_kb:\\n            if not file_size:\\n                part_size_kb = 64  # Reasonable default\\n            else:\\n                part_size_kb = utils.get_appropriated_part_size(file_size)\\n\\n        part_size = int(part_size_kb * 1024)\\n        # https://core.telegram.org/api/files says:\\n        # > part_size % 1024 = 0 (divisible by 1KB)\\n        #\\n        # But https://core.telegram.org/cdn (more recent) says:\\n        # > limit must be divisible by 4096 bytes\\n        # So we just stick to the 4096 limit.\\n        if part_size % 4096 != 0:\\n            raise ValueError(\\n                \\'The part size must be evenly divisible by 4096.\\')\\n\\n        in_memory = file is None or file is bytes\\n        if in_memory:\\n            f = io.BytesIO()\\n        elif isinstance(file, str):\\n            # Ensure that we\\'ll be able to download the media\\n            helpers.ensure_parent_dir_exists(file)\\n            f = open(file, \\'wb\\')\\n        else:\\n            f = file\\n\\n        old_dc = dc_id\\n        dc_id, input_location = utils.get_input_location(input_location)\\n        if dc_id is None:\\n            dc_id = old_dc\\n\\n        exported = dc_id and self.session.dc_id != dc_id\\n        if exported:\\n            try:\\n                sender = await self._borrow_exported_sender(dc_id)\\n            except errors.DcIdInvalidError:\\n                # Can\\'t export a sender for the ID we are currently in\\n                config = await self(functions.help.GetConfigRequest())\\n                for option in config.dc_options:\\n                    if option.ip_address == self.session.server_address:\\n                        self.session.set_dc(\\n                            option.id, option.ip_address, option.port)\\n                        self.session.save()\\n                        break\\n\\n                # TODO Figure out why the session may have the wrong DC ID\\n                sender = self._sender\\n                exported = False\\n        else:\\n            # The used sender will also change if ``FileMigrateError`` occurs\\n            sender = self._sender\\n\\n        self._log[__name__].info(\\'Downloading file in chunks of %d bytes\\',\\n                                 part_size)\\n        try:\\n            offset = 0\\n            while True:\\n                try:\\n                    result = await sender.send(functions.upload.GetFileRequest(\\n                        input_location, offset, part_size\\n                    ))\\n                    if isinstance(result, types.upload.FileCdnRedirect):\\n                        # TODO Implement\\n                        raise NotImplementedError\\n                except errors.FileMigrateError as e:\\n                    self._log[__name__].info(\\'File lives in another DC\\')\\n                    sender = await self._borrow_exported_sender(e.new_dc)\\n                    exported = True\\n                    continue\\n\\n                offset += part_size\\n                if not result.bytes:\\n                    if in_memory:\\n                        f.flush()\\n                        return f.getvalue()\\n                    else:\\n                        return getattr(result, \\'type\\', \\'\\')\\n\\n                self._log[__name__].debug(\\'Saving %d more bytes\\',\\n                                          len(result.bytes))\\n                f.write(result.bytes)\\n                if progress_callback:\\n                    progress_callback(f.tell(), file_size)\\n        finally:\\n            if exported:\\n                await self._return_exported_sender(sender)\\n            elif sender != self._sender:\\n                await sender.disconnect()\\n            if isinstance(file, str) or in_memory:\\n                f.close()', 'async def _download_photo(self, photo, file, date, thumb, progress_callback):\\n        \"\"\"Specialized version of .download_media() for photos\"\"\"\\n        # Determine the photo and its largest size\\n        if isinstance(photo, types.MessageMediaPhoto):\\n            photo = photo.photo\\n        if not isinstance(photo, types.Photo):\\n            return\\n\\n        size = self._get_thumb(photo.sizes, thumb)\\n        if not size or isinstance(size, types.PhotoSizeEmpty):\\n            return\\n\\n        file = self._get_proper_filename(file, \\'photo\\', \\'.jpg\\', date=date)\\n        if isinstance(size, (types.PhotoCachedSize, types.PhotoStrippedSize)):\\n            return self._download_cached_photo_size(size, file)\\n\\n        result = await self.download_file(\\n            types.InputPhotoFileLocation(\\n                id=photo.id,\\n                access_hash=photo.access_hash,\\n                file_reference=photo.file_reference,\\n                thumb_size=size.type\\n            ),\\n            file,\\n            file_size=size.size,\\n            progress_callback=progress_callback\\n        )\\n        return result if file is bytes else file', 'def _get_kind_and_names(attributes):\\n        \"\"\"Gets kind and possible names for :tl:`DocumentAttribute`.\"\"\"\\n        kind = \\'document\\'\\n        possible_names = []\\n        for attr in attributes:\\n            if isinstance(attr, types.DocumentAttributeFilename):\\n                possible_names.insert(0, attr.file_name)\\n\\n            elif isinstance(attr, types.DocumentAttributeAudio):\\n                kind = \\'audio\\'\\n                if attr.performer and attr.title:\\n                    possible_names.append(\\'{} - {}\\'.format(\\n                        attr.performer, attr.title\\n                    ))\\n                elif attr.performer:\\n                    possible_names.append(attr.performer)\\n                elif attr.title:\\n                    possible_names.append(attr.title)\\n                elif attr.voice:\\n                    kind = \\'voice\\'\\n\\n        return kind, possible_names', 'async def _download_document(\\n            self, document, file, date, thumb, progress_callback):\\n        \"\"\"Specialized version of .download_media() for documents.\"\"\"\\n        if isinstance(document, types.MessageMediaDocument):\\n            document = document.document\\n        if not isinstance(document, types.Document):\\n            return\\n\\n        kind, possible_names = self._get_kind_and_names(document.attributes)\\n        file = self._get_proper_filename(\\n            file, kind, utils.get_extension(document),\\n            date=date, possible_names=possible_names\\n        )\\n\\n        if thumb is None:\\n            size = None\\n        else:\\n            size = self._get_thumb(document.thumbs, thumb)\\n            if isinstance(size, (types.PhotoCachedSize, types.PhotoStrippedSize)):\\n                return self._download_cached_photo_size(size, file)\\n\\n        result = await self.download_file(\\n            types.InputDocumentFileLocation(\\n                id=document.id,\\n                access_hash=document.access_hash,\\n                file_reference=document.file_reference,\\n                thumb_size=size.type if size else \\'\\'\\n            ),\\n            file,\\n            file_size=size.size if size else document.size,\\n            progress_callback=progress_callback\\n        )\\n\\n        return result if file is bytes else file', 'def _download_contact(cls, mm_contact, file):\\n        \"\"\"\\n        Specialized version of .download_media() for contacts.\\n        Will make use of the vCard 4.0 format.\\n        \"\"\"\\n        first_name = mm_contact.first_name\\n        last_name = mm_contact.last_name\\n        phone_number = mm_contact.phone_number\\n\\n        # Remove these pesky characters\\n        first_name = first_name.replace(\\';\\', \\'\\')\\n        last_name = (last_name or \\'\\').replace(\\';\\', \\'\\')\\n        result = (\\n            \\'BEGIN:VCARD\\\\n\\'\\n            \\'VERSION:4.0\\\\n\\'\\n            \\'N:{f};{l};;;\\\\n\\'\\n            \\'FN:{f} {l}\\\\n\\'\\n            \\'TEL;TYPE=cell;VALUE=uri:tel:+{p}\\\\n\\'\\n            \\'END:VCARD\\\\n\\'\\n        ).format(f=first_name, l=last_name, p=phone_number).encode(\\'utf-8\\')\\n\\n        if file is bytes:\\n            return result\\n        elif isinstance(file, str):\\n            file = cls._get_proper_filename(\\n                file, \\'contact\\', \\'.vcard\\',\\n                possible_names=[first_name, phone_number, last_name]\\n            )\\n            f = open(file, \\'wb\\', encoding=\\'utf-8\\')\\n        else:\\n            f = file\\n\\n        try:\\n            f.write(result)\\n        finally:\\n            # Only close the stream if we opened it\\n            if isinstance(file, str):\\n                f.close()\\n\\n        return file', 'async def _download_web_document(cls, web, file, progress_callback):\\n        \"\"\"\\n        Specialized version of .download_media() for web documents.\\n        \"\"\"\\n        if not aiohttp:\\n            raise ValueError(\\n                \\'Cannot download web documents without the aiohttp \\'\\n                \\'dependency install it (pip install aiohttp)\\'\\n            )\\n\\n        # TODO Better way to get opened handles of files and auto-close\\n        in_memory = file is bytes\\n        if in_memory:\\n            f = io.BytesIO()\\n        elif isinstance(file, str):\\n            kind, possible_names = cls._get_kind_and_names(web.attributes)\\n            file = cls._get_proper_filename(\\n                file, kind, utils.get_extension(web),\\n                possible_names=possible_names\\n            )\\n            f = open(file, \\'wb\\')\\n        else:\\n            f = file\\n\\n        try:\\n            with aiohttp.ClientSession() as session:\\n                # TODO Use progress_callback; get content length from response\\n                # https://github.com/telegramdesktop/tdesktop/blob/c7e773dd9aeba94e2be48c032edc9a78bb50234e/Telegram/SourceFiles/ui/images.cpp#L1318-L1319\\n                async with session.get(web.url) as response:\\n                    while True:\\n                        chunk = await response.content.read(128 * 1024)\\n                        if not chunk:\\n                            break\\n                        f.write(chunk)\\n        finally:\\n            if isinstance(file, str) or file is bytes:\\n                f.close()\\n\\n        return f.getvalue() if in_memory else file', 'def _get_proper_filename(file, kind, extension,\\n                             date=None, possible_names=None):\\n        \"\"\"Gets a proper filename for \\'file\\', if this is a path.\\n\\n           \\'kind\\' should be the kind of the output file (photo, document...)\\n           \\'extension\\' should be the extension to be added to the file if\\n                       the filename doesn\\'t have any yet\\n           \\'date\\' should be when this file was originally sent, if known\\n           \\'possible_names\\' should be an ordered list of possible names\\n\\n           If no modification is made to the path, any existing file\\n           will be overwritten.\\n           If any modification is made to the path, this method will\\n           ensure that no existing file will be overwritten.\\n        \"\"\"\\n        if isinstance(file, pathlib.Path):\\n            file = str(file.absolute())\\n\\n        if file is not None and not isinstance(file, str):\\n            # Probably a stream-like object, we cannot set a filename here\\n            return file\\n\\n        if file is None:\\n            file = \\'\\'\\n        elif os.path.isfile(file):\\n            # Make no modifications to valid existing paths\\n            return file\\n\\n        if os.path.isdir(file) or not file:\\n            try:\\n                name = None if possible_names is None else next(\\n                    x for x in possible_names if x\\n                )\\n            except StopIteration:\\n                name = None\\n\\n            if not name:\\n                if not date:\\n                    date = datetime.datetime.now()\\n                name = \\'{}_{}-{:02}-{:02}_{:02}-{:02}-{:02}\\'.format(\\n                    kind,\\n                    date.year, date.month, date.day,\\n                    date.hour, date.minute, date.second,\\n                )\\n            file = os.path.join(file, name)\\n\\n        directory, name = os.path.split(file)\\n        name, ext = os.path.splitext(name)\\n        if not ext:\\n            ext = extension\\n\\n        result = os.path.join(directory, name + ext)\\n        if not os.path.isfile(result):\\n            return result\\n\\n        i = 1\\n        while True:\\n            result = os.path.join(directory, \\'{} ({}){}\\'.format(name, i, ext))\\n            if not os.path.isfile(result):\\n                return result\\n            i += 1', 'def _message_in_range(self, message):\\n        \"\"\"\\n        Determine whether the given message is in the range or\\n        it should be ignored (and avoid loading more chunks).\\n        \"\"\"\\n        # No entity means message IDs between chats may vary\\n        if self.entity:\\n            if self.reverse:\\n                if message.id <= self.last_id or message.id >= self.max_id:\\n                    return False\\n            else:\\n                if message.id >= self.last_id or message.id <= self.min_id:\\n                    return False\\n\\n        return True', 'def _update_offset(self, last_message):\\n        \"\"\"\\n        After making the request, update its offset with the last message.\\n        \"\"\"\\n        self.request.offset_id = last_message.id\\n        if self.reverse:\\n            # We want to skip the one we already have\\n            self.request.offset_id += 1\\n\\n        if isinstance(self.request, functions.messages.SearchRequest):\\n            # Unlike getHistory and searchGlobal that use *offset* date,\\n            # this is *max* date. This means that doing a search in reverse\\n            # will break it. Since it\\'s not really needed once we\\'re going\\n            # (only for the first request), it\\'s safe to just clear it off.\\n            self.request.max_date = None\\n        else:\\n            # getHistory and searchGlobal call it offset_date\\n            self.request.offset_date = last_message.date\\n\\n        if isinstance(self.request, functions.messages.SearchGlobalRequest):\\n            self.request.offset_peer = last_message.input_chat', 'def iter_messages(\\n            self, entity, limit=None, *, offset_date=None, offset_id=0,\\n            max_id=0, min_id=0, add_offset=0, search=None, filter=None,\\n            from_user=None, wait_time=None, ids=None, reverse=False\\n    ):\\n        \"\"\"\\n        Iterator over the message history for the specified entity.\\n        If either `search`, `filter` or `from_user` are provided,\\n        :tl:`messages.Search` will be used instead of :tl:`messages.getHistory`.\\n\\n        Args:\\n            entity (`entity`):\\n                The entity from whom to retrieve the message history.\\n\\n                It may be ``None`` to perform a global search, or\\n                to get messages by their ID from no particular chat.\\n                Note that some of the offsets will not work if this\\n                is the case.\\n\\n                Note that if you want to perform a global search,\\n                you **must** set a non-empty `search` string.\\n\\n            limit (`int` | `None`, optional):\\n                Number of messages to be retrieved. Due to limitations with\\n                the API retrieving more than 3000 messages will take longer\\n                than half a minute (or even more based on previous calls).\\n\\n                The limit may also be ``None``, which would eventually return\\n                the whole history.\\n\\n            offset_date (`datetime`):\\n                Offset date (messages *previous* to this date will be\\n                retrieved). Exclusive.\\n\\n            offset_id (`int`):\\n                Offset message ID (only messages *previous* to the given\\n                ID will be retrieved). Exclusive.\\n\\n            max_id (`int`):\\n                All the messages with a higher (newer) ID or equal to this will\\n                be excluded.\\n\\n            min_id (`int`):\\n                All the messages with a lower (older) ID or equal to this will\\n                be excluded.\\n\\n            add_offset (`int`):\\n                Additional message offset (all of the specified offsets +\\n                this offset = older messages).\\n\\n            search (`str`):\\n                The string to be used as a search query.\\n\\n            filter (:tl:`MessagesFilter` | `type`):\\n                The filter to use when returning messages. For instance,\\n                :tl:`InputMessagesFilterPhotos` would yield only messages\\n                containing photos.\\n\\n            from_user (`entity`):\\n                Only messages from this user will be returned.\\n                This parameter will be ignored if it is not an user.\\n\\n            wait_time (`int`):\\n                Wait time (in seconds) between different\\n                :tl:`GetHistoryRequest`. Use this parameter to avoid hitting\\n                the ``FloodWaitError`` as needed. If left to ``None``, it will\\n                default to 1 second only if the limit is higher than 3000.\\n\\n            ids (`int`, `list`):\\n                A single integer ID (or several IDs) for the message that\\n                should be returned. This parameter takes precedence over\\n                the rest (which will be ignored if this is set). This can\\n                for instance be used to get the message with ID 123 from\\n                a channel. Note that if the message doesn\\'t exist, ``None``\\n                will appear in its place, so that zipping the list of IDs\\n                with the messages can match one-to-one.\\n\\n                .. note::\\n\\n                    At the time of writing, Telegram will **not** return\\n                    :tl:`MessageEmpty` for :tl:`InputMessageReplyTo` IDs that\\n                    failed (i.e. the message is not replying to any, or is\\n                    replying to a deleted message). This means that it is\\n                    **not** possible to match messages one-by-one, so be\\n                    careful if you use non-integers in this parameter.\\n\\n            reverse (`bool`, optional):\\n                If set to ``True``, the messages will be returned in reverse\\n                order (from oldest to newest, instead of the default newest\\n                to oldest). This also means that the meaning of `offset_id`\\n                and `offset_date` parameters is reversed, although they will\\n                still be exclusive. `min_id` becomes equivalent to `offset_id`\\n                instead of being `max_id` as well since messages are returned\\n                in ascending order.\\n\\n                You cannot use this if both `entity` and `ids` are ``None``.\\n\\n        Yields:\\n            Instances of `telethon.tl.custom.message.Message`.\\n\\n        Notes:\\n            Telegram\\'s flood wait limit for :tl:`GetHistoryRequest` seems to\\n            be around 30 seconds per 10 requests, therefore a sleep of 1\\n            second is the default for this limit (or above).\\n        \"\"\"\\n\\n        if ids is not None:\\n            return _IDsIter(self, limit, entity=entity, ids=ids)\\n\\n        return _MessagesIter(\\n            client=self,\\n            reverse=reverse,\\n            wait_time=wait_time,\\n            limit=limit,\\n            entity=entity,\\n            offset_id=offset_id,\\n            min_id=min_id,\\n            max_id=max_id,\\n            from_user=from_user,\\n            offset_date=offset_date,\\n            add_offset=add_offset,\\n            filter=filter,\\n            search=search\\n        )', 'async def get_messages(self, *args, **kwargs):\\n        \"\"\"\\n        Same as `iter_messages`, but returns a\\n        `TotalList <telethon.helpers.TotalList>` instead.\\n\\n        If the `limit` is not set, it will be 1 by default unless both\\n        `min_id` **and** `max_id` are set (as *named* arguments), in\\n        which case the entire range will be returned.\\n\\n        This is so because any integer limit would be rather arbitrary and\\n        it\\'s common to only want to fetch one message, but if a range is\\n        specified it makes sense that it should return the entirety of it.\\n\\n        If `ids` is present in the *named* arguments and is not a list,\\n        a single `Message <telethon.tl.custom.message.Message>` will be\\n        returned for convenience instead of a list.\\n        \"\"\"\\n        if len(args) == 1 and \\'limit\\' not in kwargs:\\n            if \\'min_id\\' in kwargs and \\'max_id\\' in kwargs:\\n                kwargs[\\'limit\\'] = None\\n            else:\\n                kwargs[\\'limit\\'] = 1\\n\\n        it = self.iter_messages(*args, **kwargs)\\n\\n        ids = kwargs.get(\\'ids\\')\\n        if ids and not utils.is_list_like(ids):\\n            async for message in it:\\n                return message\\n            else:\\n                # Iterator exhausted = empty, to handle InputMessageReplyTo\\n                return None\\n\\n        return await it.collect()', 'async def send_message(\\n            self, entity, message=\\'\\', *, reply_to=None,\\n            parse_mode=(), link_preview=True, file=None,\\n            force_document=False, clear_draft=False, buttons=None,\\n            silent=None):\\n        \"\"\"\\n        Sends the given message to the specified entity (user/chat/channel).\\n\\n        The default parse mode is the same as the official applications\\n        (a custom flavour of markdown). ``**bold**, `code` or __italic__``\\n        are available. In addition you can send ``[links](https://example.com)``\\n        and ``[mentions](@username)`` (or using IDs like in the Bot API:\\n        ``[mention](tg://user?id=123456789)``) and ``pre`` blocks with three\\n        backticks.\\n\\n        Sending a ``/start`` command with a parameter (like ``?start=data``)\\n        is also done through this method. Simply send ``\\'/start data\\'`` to\\n        the bot.\\n\\n        Args:\\n            entity (`entity`):\\n                To who will it be sent.\\n\\n            message (`str` | `Message <telethon.tl.custom.message.Message>`):\\n                The message to be sent, or another message object to resend.\\n\\n                The maximum length for a message is 35,000 bytes or 4,096\\n                characters. Longer messages will not be sliced automatically,\\n                and you should slice them manually if the text to send is\\n                longer than said length.\\n\\n            reply_to (`int` | `Message <telethon.tl.custom.message.Message>`, optional):\\n                Whether to reply to a message or not. If an integer is provided,\\n                it should be the ID of the message that it should reply to.\\n\\n            parse_mode (`object`, optional):\\n                See the `TelegramClient.parse_mode\\n                <telethon.client.messageparse.MessageParseMethods.parse_mode>`\\n                property for allowed values. Markdown parsing will be used by\\n                default.\\n\\n            link_preview (`bool`, optional):\\n                Should the link preview be shown?\\n\\n            file (`file`, optional):\\n                Sends a message with a file attached (e.g. a photo,\\n                video, audio or document). The ``message`` may be empty.\\n\\n            force_document (`bool`, optional):\\n                Whether to send the given file as a document or not.\\n\\n            clear_draft (`bool`, optional):\\n                Whether the existing draft should be cleared or not.\\n                Has no effect when sending a file.\\n\\n            buttons (`list`, `custom.Button <telethon.tl.custom.button.Button>`, :tl:`KeyboardButton`):\\n                The matrix (list of lists), row list or button to be shown\\n                after sending the message. This parameter will only work if\\n                you have signed in as a bot. You can also pass your own\\n                :tl:`ReplyMarkup` here.\\n\\n                All the following limits apply together:\\n\\n                * There can be 100 buttons at most (any more are ignored).\\n                * There can be 8 buttons per row at most (more are ignored).\\n                * The maximum callback data per button is 64 bytes.\\n                * The maximum data that can be embedded in total is just\\n                  over 4KB, shared between inline callback data and text.\\n\\n            silent (`bool`, optional):\\n                Whether the message should notify people in a broadcast\\n                channel or not. Defaults to ``False``, which means it will\\n                notify them. Set it to ``True`` to alter this behaviour.\\n\\n        Returns:\\n            The sent `custom.Message <telethon.tl.custom.message.Message>`.\\n        \"\"\"\\n        if file is not None:\\n            return await self.send_file(\\n                entity, file, caption=message, reply_to=reply_to,\\n                parse_mode=parse_mode, force_document=force_document,\\n                buttons=buttons\\n            )\\n        elif not message:\\n            raise ValueError(\\n                \\'The message cannot be empty unless a file is provided\\'\\n            )\\n\\n        entity = await self.get_input_entity(entity)\\n        if isinstance(message, types.Message):\\n            if buttons is None:\\n                markup = message.reply_markup\\n            else:\\n                markup = self.build_reply_markup(buttons)\\n\\n            if silent is None:\\n                silent = message.silent\\n\\n            if (message.media and not isinstance(\\n                    message.media, types.MessageMediaWebPage)):\\n                return await self.send_file(\\n                    entity,\\n                    message.media,\\n                    caption=message.message,\\n                    silent=silent,\\n                    reply_to=reply_to,\\n                    buttons=markup,\\n                    entities=message.entities\\n                )\\n\\n            request = functions.messages.SendMessageRequest(\\n                peer=entity,\\n                message=message.message or \\'\\',\\n                silent=silent,\\n                reply_to_msg_id=utils.get_message_id(reply_to),\\n                reply_markup=markup,\\n                entities=message.entities,\\n                clear_draft=clear_draft,\\n                no_webpage=not isinstance(\\n                    message.media, types.MessageMediaWebPage)\\n            )\\n            message = message.message\\n        else:\\n            message, msg_ent = await self._parse_message_text(message,\\n                                                              parse_mode)\\n            request = functions.messages.SendMessageRequest(\\n                peer=entity,\\n                message=message,\\n                entities=msg_ent,\\n                no_webpage=not link_preview,\\n                reply_to_msg_id=utils.get_message_id(reply_to),\\n                clear_draft=clear_draft,\\n                silent=silent,\\n                reply_markup=self.build_reply_markup(buttons)\\n            )\\n\\n        result = await self(request)\\n        if isinstance(result, types.UpdateShortSentMessage):\\n            message = types.Message(\\n                id=result.id,\\n                to_id=utils.get_peer(entity),\\n                message=message,\\n                date=result.date,\\n                out=result.out,\\n                media=result.media,\\n                entities=result.entities,\\n                reply_markup=request.reply_markup\\n            )\\n            message._finish_init(self, {}, entity)\\n            return message\\n\\n        return self._get_response_message(request, result, entity)', 'async def forward_messages(self, entity, messages, from_peer=None,\\n                               *, silent=None, as_album=None):\\n        \"\"\"\\n        Forwards the given message(s) to the specified entity.\\n\\n        Args:\\n            entity (`entity`):\\n                To which entity the message(s) will be forwarded.\\n\\n            messages (`list` | `int` | `Message <telethon.tl.custom.message.Message>`):\\n                The message(s) to forward, or their integer IDs.\\n\\n            from_peer (`entity`):\\n                If the given messages are integer IDs and not instances\\n                of the ``Message`` class, this *must* be specified in\\n                order for the forward to work. This parameter indicates\\n                the entity from which the messages should be forwarded.\\n\\n            silent (`bool`, optional):\\n                Whether the message should notify people in a broadcast\\n                channel or not. Defaults to ``False``, which means it will\\n                notify them. Set it to ``True`` to alter this behaviour.\\n\\n            as_album (`bool`, optional):\\n                Whether several image messages should be forwarded as an\\n                album (grouped) or not. The default behaviour is to treat\\n                albums specially and send outgoing requests with\\n                ``as_album=True`` only for the albums if message objects\\n                are used. If IDs are used it will group by default.\\n\\n                In short, the default should do what you expect,\\n                ``True`` will group always (even converting separate\\n                images into albums), and ``False`` will never group.\\n\\n        Returns:\\n            The list of forwarded `telethon.tl.custom.message.Message`,\\n            or a single one if a list wasn\\'t provided as input.\\n\\n            Note that if all messages are invalid (i.e. deleted) the call\\n            will fail with ``MessageIdInvalidError``. If only some are\\n            invalid, the list will have ``None`` instead of those messages.\\n        \"\"\"\\n        single = not utils.is_list_like(messages)\\n        if single:\\n            messages = (messages,)\\n\\n        entity = await self.get_input_entity(entity)\\n\\n        if from_peer:\\n            from_peer = await self.get_input_entity(from_peer)\\n            from_peer_id = await self.get_peer_id(from_peer)\\n        else:\\n            from_peer_id = None\\n\\n        def _get_key(m):\\n            if isinstance(m, int):\\n                if from_peer_id is not None:\\n                    return from_peer_id, None\\n\\n                raise ValueError(\\'from_peer must be given if integer IDs are used\\')\\n            elif isinstance(m, types.Message):\\n                return m.chat_id, m.grouped_id\\n            else:\\n                raise TypeError(\\'Cannot forward messages of type {}\\'.format(type(m)))\\n\\n        # We want to group outgoing chunks differently if we are \"smart\"\\n        # about sending as album.\\n        #\\n        # Why? We need separate requests for ``as_album=True/False``, so\\n        # if we want that behaviour, when we group messages to create the\\n        # chunks, we need to consider the grouped ID too. But if we don\\'t\\n        # care about that, we don\\'t need to consider it for creating the\\n        # chunks, so we can make less requests.\\n        if as_album is None:\\n            get_key = _get_key\\n        else:\\n            def get_key(m):\\n                return _get_key(m)[0]  # Ignore grouped_id\\n\\n        sent = []\\n        for chat_id, chunk in itertools.groupby(messages, key=get_key):\\n            chunk = list(chunk)\\n            if isinstance(chunk[0], int):\\n                chat = from_peer\\n                grouped = True if as_album is None else as_album\\n            else:\\n                chat = await chunk[0].get_input_chat()\\n                if as_album is None:\\n                    grouped = any(m.grouped_id is not None for m in chunk)\\n                else:\\n                    grouped = as_album\\n\\n                chunk = [m.id for m in chunk]\\n\\n            req = functions.messages.ForwardMessagesRequest(\\n                from_peer=chat,\\n                id=chunk,\\n                to_peer=entity,\\n                silent=silent,\\n                # Trying to send a single message as grouped will cause\\n                # GROUPED_MEDIA_INVALID. If more than one message is forwarded\\n                # (even without media...), this error goes away.\\n                grouped=len(chunk) > 1 and grouped\\n            )\\n            result = await self(req)\\n            sent.extend(self._get_response_message(req, result, entity))\\n\\n        return sent[0] if single else sent', 'async def edit_message(\\n            self, entity, message=None, text=None,\\n            *, parse_mode=(), link_preview=True, file=None,\\n            buttons=None):\\n        \"\"\"\\n        Edits the given message ID (to change its contents or disable preview).\\n\\n        Args:\\n            entity (`entity` | `Message <telethon.tl.custom.message.Message>`):\\n                From which chat to edit the message. This can also be\\n                the message to be edited, and the entity will be inferred\\n                from it, so the next parameter will be assumed to be the\\n                message text.\\n\\n                You may also pass a :tl:`InputBotInlineMessageID`,\\n                which is the only way to edit messages that were sent\\n                after the user selects an inline query result.\\n\\n            message (`int` | `Message <telethon.tl.custom.message.Message>` | `str`):\\n                The ID of the message (or `Message\\n                <telethon.tl.custom.message.Message>` itself) to be edited.\\n                If the `entity` was a `Message\\n                <telethon.tl.custom.message.Message>`, then this message\\n                will be treated as the new text.\\n\\n            text (`str`, optional):\\n                The new text of the message. Does nothing if the `entity`\\n                was a `Message <telethon.tl.custom.message.Message>`.\\n\\n            parse_mode (`object`, optional):\\n                See the `TelegramClient.parse_mode\\n                <telethon.client.messageparse.MessageParseMethods.parse_mode>`\\n                property for allowed values. Markdown parsing will be used by\\n                default.\\n\\n            link_preview (`bool`, optional):\\n                Should the link preview be shown?\\n\\n            file (`str` | `bytes` | `file` | `media`, optional):\\n                The file object that should replace the existing media\\n                in the message.\\n\\n            buttons (`list`, `custom.Button <telethon.tl.custom.button.Button>`, :tl:`KeyboardButton`):\\n                The matrix (list of lists), row list or button to be shown\\n                after sending the message. This parameter will only work if\\n                you have signed in as a bot. You can also pass your own\\n                :tl:`ReplyMarkup` here.\\n\\n        Examples:\\n\\n            >>> client = ...\\n            >>> message = client.send_message(\\'username\\', \\'hello\\')\\n            >>>\\n            >>> client.edit_message(\\'username\\', message, \\'hello!\\')\\n            >>> # or\\n            >>> client.edit_message(\\'username\\', message.id, \\'Hello\\')\\n            >>> # or\\n            >>> client.edit_message(message, \\'Hello!\\')\\n\\n        Raises:\\n            ``MessageAuthorRequiredError`` if you\\'re not the author of the\\n            message but tried editing it anyway.\\n\\n            ``MessageNotModifiedError`` if the contents of the message were\\n            not modified at all.\\n\\n        Returns:\\n            The edited `telethon.tl.custom.message.Message`, unless\\n            `entity` was a :tl:`InputBotInlineMessageID` in which\\n            case this method returns a boolean.\\n        \"\"\"\\n        if isinstance(entity, types.InputBotInlineMessageID):\\n            text = message\\n            message = entity\\n        elif isinstance(entity, types.Message):\\n            text = message  # Shift the parameters to the right\\n            message = entity\\n            entity = entity.to_id\\n\\n        text, msg_entities = await self._parse_message_text(text, parse_mode)\\n        file_handle, media, image = await self._file_to_media(file)\\n\\n        if isinstance(entity, types.InputBotInlineMessageID):\\n            return await self(functions.messages.EditInlineBotMessageRequest(\\n                id=entity,\\n                message=text,\\n                no_webpage=not link_preview,\\n                entities=msg_entities,\\n                media=media,\\n                reply_markup=self.build_reply_markup(buttons)\\n            ))\\n\\n        entity = await self.get_input_entity(entity)\\n        request = functions.messages.EditMessageRequest(\\n            peer=entity,\\n            id=utils.get_message_id(message),\\n            message=text,\\n            no_webpage=not link_preview,\\n            entities=msg_entities,\\n            media=media,\\n            reply_markup=self.build_reply_markup(buttons)\\n        )\\n        msg = self._get_response_message(request, await self(request), entity)\\n        await self._cache_media(msg, file, file_handle, image=image)\\n        return msg', 'async def delete_messages(self, entity, message_ids, *, revoke=True):\\n        \"\"\"\\n        Deletes a message from a chat, optionally \"for everyone\".\\n\\n        Args:\\n            entity (`entity`):\\n                From who the message will be deleted. This can actually\\n                be ``None`` for normal chats, but **must** be present\\n                for channels and megagroups.\\n\\n            message_ids (`list` | `int` | `Message <telethon.tl.custom.message.Message>`):\\n                The IDs (or ID) or messages to be deleted.\\n\\n            revoke (`bool`, optional):\\n                Whether the message should be deleted for everyone or not.\\n                By default it has the opposite behaviour of official clients,\\n                and it will delete the message for everyone.\\n\\n                `Since 24 March 2019\\n                <https://telegram.org/blog/unsend-privacy-emoji>`_, you can\\n                also revoke messages of any age (i.e. messages sent long in\\n                the past) the *other* person sent in private conversations\\n                (and of course your messages too).\\n\\n                Disabling this has no effect on channels or megagroups,\\n                since it will unconditionally delete the message for everyone.\\n\\n        Returns:\\n            A list of :tl:`AffectedMessages`, each item being the result\\n            for the delete calls of the messages in chunks of 100 each.\\n        \"\"\"\\n        if not utils.is_list_like(message_ids):\\n            message_ids = (message_ids,)\\n\\n        message_ids = (\\n            m.id if isinstance(m, (\\n                types.Message, types.MessageService, types.MessageEmpty))\\n            else int(m) for m in message_ids\\n        )\\n\\n        entity = await self.get_input_entity(entity) if entity else None\\n        if isinstance(entity, types.InputPeerChannel):\\n            return await self([functions.channels.DeleteMessagesRequest(\\n                         entity, list(c)) for c in utils.chunks(message_ids)])\\n        else:\\n            return await self([functions.messages.DeleteMessagesRequest(\\n                         list(c), revoke) for c in utils.chunks(message_ids)])', 'async def send_read_acknowledge(\\n            self, entity, message=None, *, max_id=None, clear_mentions=False):\\n        \"\"\"\\n        Sends a \"read acknowledge\" (i.e., notifying the given peer that we\\'ve\\n        read their messages, also known as the \"double check\").\\n\\n        This effectively marks a message as read (or more than one) in the\\n        given conversation.\\n\\n        If neither message nor maximum ID are provided, all messages will be\\n        marked as read by assuming that ``max_id = 0``.\\n\\n        Args:\\n            entity (`entity`):\\n                The chat where these messages are located.\\n\\n            message (`list` | `Message <telethon.tl.custom.message.Message>`):\\n                Either a list of messages or a single message.\\n\\n            max_id (`int`):\\n                Overrides messages, until which message should the\\n                acknowledge should be sent.\\n\\n            clear_mentions (`bool`):\\n                Whether the mention badge should be cleared (so that\\n                there are no more mentions) or not for the given entity.\\n\\n                If no message is provided, this will be the only action\\n                taken.\\n        \"\"\"\\n        if max_id is None:\\n            if not message:\\n                max_id = 0\\n            else:\\n                if utils.is_list_like(message):\\n                    max_id = max(msg.id for msg in message)\\n                else:\\n                    max_id = message.id\\n\\n        entity = await self.get_input_entity(entity)\\n        if clear_mentions:\\n            await self(functions.messages.ReadMentionsRequest(entity))\\n            if max_id is None:\\n                return True\\n\\n        if max_id is not None:\\n            if isinstance(entity, types.InputPeerChannel):\\n                return await self(functions.channels.ReadHistoryRequest(\\n                    entity, max_id=max_id))\\n            else:\\n                return await self(functions.messages.ReadHistoryRequest(\\n                    entity, max_id=max_id))\\n\\n        return False', 'async def send_file(\\n            self, entity, file, *, caption=None, force_document=False,\\n            progress_callback=None, reply_to=None, attributes=None,\\n            thumb=None, allow_cache=True, parse_mode=(),\\n            voice_note=False, video_note=False, buttons=None, silent=None,\\n            supports_streaming=False, **kwargs):\\n        \"\"\"\\n        Sends a file to the specified entity.\\n\\n        Args:\\n            entity (`entity`):\\n                Who will receive the file.\\n\\n            file (`str` | `bytes` | `file` | `media`):\\n                The file to send, which can be one of:\\n\\n                * A local file path to an in-disk file. The file name\\n                  will be the path\\'s base name.\\n\\n                * A `bytes` byte array with the file\\'s data to send\\n                  (for example, by using ``text.encode(\\'utf-8\\')``).\\n                  A default file name will be used.\\n\\n                * A bytes `io.IOBase` stream over the file to send\\n                  (for example, by using ``open(file, \\'rb\\')``).\\n                  Its ``.name`` property will be used for the file name,\\n                  or a default if it doesn\\'t have one.\\n\\n                * An external URL to a file over the internet. This will\\n                  send the file as \"external\" media, and Telegram is the\\n                  one that will fetch the media and send it.\\n\\n                * A Bot API-like ``file_id``. You can convert previously\\n                  sent media to file IDs for later reusing with\\n                  `telethon.utils.pack_bot_file_id`.\\n\\n                * A handle to an existing file (for example, if you sent a\\n                  message with media before, you can use its ``message.media``\\n                  as a file here).\\n\\n                * A handle to an uploaded file (from `upload_file`).\\n\\n                To send an album, you should provide a list in this parameter.\\n\\n                If a list or similar is provided, the files in it will be\\n                sent as an album in the order in which they appear, sliced\\n                in chunks of 10 if more than 10 are given.\\n\\n            caption (`str`, optional):\\n                Optional caption for the sent media message. When sending an\\n                album, the caption may be a list of strings, which will be\\n                assigned to the files pairwise.\\n\\n            force_document (`bool`, optional):\\n                If left to ``False`` and the file is a path that ends with\\n                the extension of an image file or a video file, it will be\\n                sent as such. Otherwise always as a document.\\n\\n            progress_callback (`callable`, optional):\\n                A callback function accepting two parameters:\\n                ``(sent bytes, total)``.\\n\\n            reply_to (`int` | `Message <telethon.tl.custom.message.Message>`):\\n                Same as `reply_to` from `send_message`.\\n\\n            attributes (`list`, optional):\\n                Optional attributes that override the inferred ones, like\\n                :tl:`DocumentAttributeFilename` and so on.\\n\\n            thumb (`str` | `bytes` | `file`, optional):\\n                Optional JPEG thumbnail (for documents). **Telegram will\\n                ignore this parameter** unless you pass a ``.jpg`` file!\\n\\n                The file must also be small in dimensions and in-disk size.\\n                Successful thumbnails were files below 20kb and 200x200px.\\n                Width/height and dimensions/size ratios may be important.\\n\\n            allow_cache (`bool`, optional):\\n                Whether to allow using the cached version stored in the\\n                database or not. Defaults to ``True`` to avoid re-uploads.\\n                Must be ``False`` if you wish to use different attributes\\n                or thumb than those that were used when the file was cached.\\n\\n            parse_mode (`object`, optional):\\n                See the `TelegramClient.parse_mode\\n                <telethon.client.messageparse.MessageParseMethods.parse_mode>`\\n                property for allowed values. Markdown parsing will be used by\\n                default.\\n\\n            voice_note (`bool`, optional):\\n                If ``True`` the audio will be sent as a voice note.\\n\\n                Set `allow_cache` to ``False`` if you sent the same file\\n                without this setting before for it to work.\\n\\n            video_note (`bool`, optional):\\n                If ``True`` the video will be sent as a video note,\\n                also known as a round video message.\\n\\n                Set `allow_cache` to ``False`` if you sent the same file\\n                without this setting before for it to work.\\n\\n            buttons (`list`, `custom.Button <telethon.tl.custom.button.Button>`, :tl:`KeyboardButton`):\\n                The matrix (list of lists), row list or button to be shown\\n                after sending the message. This parameter will only work if\\n                you have signed in as a bot. You can also pass your own\\n                :tl:`ReplyMarkup` here.\\n\\n            silent (`bool`, optional):\\n                Whether the message should notify people in a broadcast\\n                channel or not. Defaults to ``False``, which means it will\\n                notify them. Set it to ``True`` to alter this behaviour.\\n\\n            supports_streaming (`bool`, optional):\\n                Whether the sent video supports streaming or not. Note that\\n                Telegram only recognizes as streamable some formats like MP4,\\n                and others like AVI or MKV will not work. You should convert\\n                these to MP4 before sending if you want them to be streamable.\\n                Unsupported formats will result in ``VideoContentTypeError``.\\n\\n        Notes:\\n            If the ``hachoir3`` package (``hachoir`` module) is installed,\\n            it will be used to determine metadata from audio and video files.\\n\\n            If the `pillow` package is installed and you are sending a photo,\\n            it will be resized to fit within the maximum dimensions allowed\\n            by Telegram to avoid ``errors.PhotoInvalidDimensionsError``. This\\n            cannot be done if you are sending :tl:`InputFile`, however.\\n\\n        Returns:\\n            The `telethon.tl.custom.message.Message` (or messages) containing\\n            the sent file, or messages if a list of them was passed.\\n        \"\"\"\\n        # i.e. ``None`` was used\\n        if not file:\\n            raise TypeError(\\'Cannot use {!r} as file\\'.format(file))\\n\\n        if not caption:\\n            caption = \\'\\'\\n\\n        # First check if the user passed an iterable, in which case\\n        # we may want to send as an album if all are photo files.\\n        if utils.is_list_like(file):\\n            # TODO Fix progress_callback\\n            images = []\\n            if force_document:\\n                documents = file\\n            else:\\n                documents = []\\n                for x in file:\\n                    if utils.is_image(x):\\n                        images.append(x)\\n                    else:\\n                        documents.append(x)\\n\\n            result = []\\n            while images:\\n                result += await self._send_album(\\n                    entity, images[:10], caption=caption,\\n                    progress_callback=progress_callback, reply_to=reply_to,\\n                    parse_mode=parse_mode, silent=silent\\n                )\\n                images = images[10:]\\n\\n            for x in documents:\\n                result.append(await self.send_file(\\n                    entity, x, allow_cache=allow_cache,\\n                    caption=caption, force_document=force_document,\\n                    progress_callback=progress_callback, reply_to=reply_to,\\n                    attributes=attributes, thumb=thumb, voice_note=voice_note,\\n                    video_note=video_note, buttons=buttons, silent=silent,\\n                    supports_streaming=supports_streaming,\\n                    **kwargs\\n                ))\\n\\n            return result\\n\\n        entity = await self.get_input_entity(entity)\\n        reply_to = utils.get_message_id(reply_to)\\n\\n        # Not document since it\\'s subject to change.\\n        # Needed when a Message is passed to send_message and it has media.\\n        if \\'entities\\' in kwargs:\\n            msg_entities = kwargs[\\'entities\\']\\n        else:\\n            caption, msg_entities =\\\\\\n                await self._parse_message_text(caption, parse_mode)\\n\\n        file_handle, media, image = await self._file_to_media(\\n            file, force_document=force_document,\\n            progress_callback=progress_callback,\\n            attributes=attributes,  allow_cache=allow_cache, thumb=thumb,\\n            voice_note=voice_note, video_note=video_note,\\n            supports_streaming=supports_streaming\\n        )\\n\\n        # e.g. invalid cast from :tl:`MessageMediaWebPage`\\n        if not media:\\n            raise TypeError(\\'Cannot use {!r} as file\\'.format(file))\\n\\n        markup = self.build_reply_markup(buttons)\\n        request = functions.messages.SendMediaRequest(\\n            entity, media, reply_to_msg_id=reply_to, message=caption,\\n            entities=msg_entities, reply_markup=markup, silent=silent\\n        )\\n        msg = self._get_response_message(request, await self(request), entity)\\n        await self._cache_media(msg, file, file_handle, image=image)\\n\\n        return msg', 'async def _send_album(self, entity, files, caption=\\'\\',\\n                          progress_callback=None, reply_to=None,\\n                          parse_mode=(), silent=None):\\n        \"\"\"Specialized version of .send_file for albums\"\"\"\\n        # We don\\'t care if the user wants to avoid cache, we will use it\\n        # anyway. Why? The cached version will be exactly the same thing\\n        # we need to produce right now to send albums (uploadMedia), and\\n        # cache only makes a difference for documents where the user may\\n        # want the attributes used on them to change.\\n        #\\n        # In theory documents can be sent inside the albums but they appear\\n        # as different messages (not inside the album), and the logic to set\\n        # the attributes/avoid cache is already written in .send_file().\\n        entity = await self.get_input_entity(entity)\\n        if not utils.is_list_like(caption):\\n            caption = (caption,)\\n\\n        captions = []\\n        for c in reversed(caption):  # Pop from the end (so reverse)\\n            captions.append(await self._parse_message_text(c or \\'\\', parse_mode))\\n\\n        reply_to = utils.get_message_id(reply_to)\\n\\n        # Need to upload the media first, but only if they\\'re not cached yet\\n        media = []\\n        for file in files:\\n            # Albums want :tl:`InputMedia` which, in theory, includes\\n            # :tl:`InputMediaUploadedPhoto`. However using that will\\n            # make it `raise MediaInvalidError`, so we need to upload\\n            # it as media and then convert that to :tl:`InputMediaPhoto`.\\n            fh, fm, _ = await self._file_to_media(file)\\n            if isinstance(fm, types.InputMediaUploadedPhoto):\\n                r = await self(functions.messages.UploadMediaRequest(\\n                    entity, media=fm\\n                ))\\n                self.session.cache_file(\\n                    fh.md5, fh.size, utils.get_input_photo(r.photo))\\n\\n                fm = utils.get_input_media(r.photo)\\n\\n            if captions:\\n                caption, msg_entities = captions.pop()\\n            else:\\n                caption, msg_entities = \\'\\', None\\n            media.append(types.InputSingleMedia(\\n                fm,\\n                message=caption,\\n                entities=msg_entities\\n            ))\\n\\n        # Now we can construct the multi-media request\\n        result = await self(functions.messages.SendMultiMediaRequest(\\n            entity, reply_to_msg_id=reply_to, multi_media=media, silent=silent\\n        ))\\n\\n        # We never sent a `random_id` for the messages that resulted from\\n        # the request so we can\\'t pair them up with the `Updates` that we\\n        # get from Telegram. However, the sent messages have a photo and\\n        # the photo IDs match with those we did send.\\n        #\\n        # Updates -> {_: message}\\n        messages = self._get_response_message(None, result, entity)\\n        # {_: message} -> {photo ID: message}\\n        messages = {m.photo.id: m for m in messages.values()}\\n        # Sent photo IDs -> messages\\n        return [messages[m.media.id.id] for m in media]', 'async def upload_file(\\n            self, file, *, part_size_kb=None, file_name=None, use_cache=None,\\n            progress_callback=None):\\n        \"\"\"\\n        Uploads the specified file and returns a handle (an instance of\\n        :tl:`InputFile` or :tl:`InputFileBig`, as required) which can be\\n        later used before it expires (they are usable during less than a day).\\n\\n        Uploading a file will simply return a \"handle\" to the file stored\\n        remotely in the Telegram servers, which can be later used on. This\\n        will **not** upload the file to your own chat or any chat at all.\\n\\n        Args:\\n            file (`str` | `bytes` | `file`):\\n                The path of the file, byte array, or stream that will be sent.\\n                Note that if a byte array or a stream is given, a filename\\n                or its type won\\'t be inferred, and it will be sent as an\\n                \"unnamed application/octet-stream\".\\n\\n            part_size_kb (`int`, optional):\\n                Chunk size when uploading files. The larger, the less\\n                requests will be made (up to 512KB maximum).\\n\\n            file_name (`str`, optional):\\n                The file name which will be used on the resulting InputFile.\\n                If not specified, the name will be taken from the ``file``\\n                and if this is not a ``str``, it will be ``\"unnamed\"``.\\n\\n            use_cache (`type`, optional):\\n                The type of cache to use (currently either :tl:`InputDocument`\\n                or :tl:`InputPhoto`). If present and the file is small enough\\n                to need the MD5, it will be checked against the database,\\n                and if a match is found, the upload won\\'t be made. Instead,\\n                an instance of type ``use_cache`` will be returned.\\n\\n            progress_callback (`callable`, optional):\\n                A callback function accepting two parameters:\\n                ``(sent bytes, total)``.\\n\\n        Returns:\\n            :tl:`InputFileBig` if the file size is larger than 10MB,\\n            `telethon.tl.custom.inputsizedfile.InputSizedFile`\\n            (subclass of :tl:`InputFile`) otherwise.\\n        \"\"\"\\n        if isinstance(file, (types.InputFile, types.InputFileBig)):\\n            return file  # Already uploaded\\n\\n        if not file_name and getattr(file, \\'name\\', None):\\n            file_name = file.name\\n\\n        if isinstance(file, str):\\n            file_size = os.path.getsize(file)\\n        elif isinstance(file, bytes):\\n            file_size = len(file)\\n        else:\\n            if isinstance(file, io.IOBase) and file.seekable():\\n                pos = file.tell()\\n            else:\\n                pos = None\\n\\n            # TODO Don\\'t load the entire file in memory always\\n            data = file.read()\\n            if pos is not None:\\n                file.seek(pos)\\n\\n            file = data\\n            file_size = len(file)\\n\\n        # File will now either be a string or bytes\\n        if not part_size_kb:\\n            part_size_kb = utils.get_appropriated_part_size(file_size)\\n\\n        if part_size_kb > 512:\\n            raise ValueError(\\'The part size must be less or equal to 512KB\\')\\n\\n        part_size = int(part_size_kb * 1024)\\n        if part_size % 1024 != 0:\\n            raise ValueError(\\n                \\'The part size must be evenly divisible by 1024\\')\\n\\n        # Set a default file name if None was specified\\n        file_id = helpers.generate_random_long()\\n        if not file_name:\\n            if isinstance(file, str):\\n                file_name = os.path.basename(file)\\n            else:\\n                file_name = str(file_id)\\n\\n        # If the file name lacks extension, add it if possible.\\n        # Else Telegram complains with `PHOTO_EXT_INVALID_ERROR`\\n        # even if the uploaded image is indeed a photo.\\n        if not os.path.splitext(file_name)[-1]:\\n            file_name += utils._get_extension(file)\\n\\n        # Determine whether the file is too big (over 10MB) or not\\n        # Telegram does make a distinction between smaller or larger files\\n        is_large = file_size > 10 * 1024 * 1024\\n        hash_md5 = hashlib.md5()\\n        if not is_large:\\n            # Calculate the MD5 hash before anything else.\\n            # As this needs to be done always for small files,\\n            # might as well do it before anything else and\\n            # check the cache.\\n            if isinstance(file, str):\\n                with open(file, \\'rb\\') as stream:\\n                    file = stream.read()\\n            hash_md5.update(file)\\n            if use_cache:\\n                cached = self.session.get_file(\\n                    hash_md5.digest(), file_size, cls=_CacheType(use_cache)\\n                )\\n                if cached:\\n                    return cached\\n\\n        part_count = (file_size + part_size - 1) // part_size\\n        self._log[__name__].info(\\'Uploading file of %d bytes in %d chunks of %d\\',\\n                                 file_size, part_count, part_size)\\n\\n        with open(file, \\'rb\\') if isinstance(file, str) else BytesIO(file)\\\\\\n                as stream:\\n            for part_index in range(part_count):\\n                # Read the file by in chunks of size part_size\\n                part = stream.read(part_size)\\n\\n                # The SavePartRequest is different depending on whether\\n                # the file is too large or not (over or less than 10MB)\\n                if is_large:\\n                    request = functions.upload.SaveBigFilePartRequest(\\n                        file_id, part_index, part_count, part)\\n                else:\\n                    request = functions.upload.SaveFilePartRequest(\\n                        file_id, part_index, part)\\n\\n                result = await self(request)\\n                if result:\\n                    self._log[__name__].debug(\\'Uploaded %d/%d\\',\\n                                              part_index + 1, part_count)\\n                    if progress_callback:\\n                        progress_callback(stream.tell(), file_size)\\n                else:\\n                    raise RuntimeError(\\n                        \\'Failed to upload file part {}.\\'.format(part_index))\\n\\n        if is_large:\\n            return types.InputFileBig(file_id, part_count, file_name)\\n        else:\\n            return custom.InputSizedFile(\\n                file_id, part_count, file_name, md5=hash_md5, size=file_size\\n            )', 'def add(self, entities):\\n        \"\"\"\\n        Adds the given entities to the cache, if they weren\\'t saved before.\\n        \"\"\"\\n        if not utils.is_list_like(entities):\\n            # Invariant: all \"chats\" and \"users\" are always iterables,\\n            # and \"user\" never is (so we wrap it inside a list).\\n            entities = itertools.chain(\\n                getattr(entities, \\'chats\\', []),\\n                getattr(entities, \\'users\\', []),\\n                (hasattr(entities, \\'user\\') and [entities.user]) or []\\n            )\\n\\n        for entity in entities:\\n            try:\\n                pid = utils.get_peer_id(entity)\\n                if pid not in self.__dict__:\\n                    # Note: `get_input_peer` already checks for `access_hash`\\n                    self.__dict__[pid] = utils.get_input_peer(entity)\\n            except TypeError:\\n                pass', 'def run_until_disconnected(self):\\n        \"\"\"\\n        Runs the event loop until `disconnect` is called or if an error\\n        while connecting/sending/receiving occurs in the background. In\\n        the latter case, said error will ``raise`` so you have a chance\\n        to ``except`` it on your own code.\\n\\n        If the loop is already running, this method returns a coroutine\\n        that you should await on your own code.\\n        \"\"\"\\n        if self.loop.is_running():\\n            return self._run_until_disconnected()\\n        try:\\n            return self.loop.run_until_complete(self.disconnected)\\n        except KeyboardInterrupt:\\n            pass\\n        finally:\\n            # No loop.run_until_complete; it\\'s already syncified\\n            self.disconnect()', 'def on(self, event):\\n        \"\"\"\\n        Decorator helper method around `add_event_handler`. Example:\\n\\n        >>> from telethon import TelegramClient, events\\n        >>> client = TelegramClient(...)\\n        >>>\\n        >>> @client.on(events.NewMessage)\\n        ... async def handler(event):\\n        ...     ...\\n        ...\\n        >>>\\n\\n        Args:\\n            event (`_EventBuilder` | `type`):\\n                The event builder class or instance to be used,\\n                for instance ``events.NewMessage``.\\n        \"\"\"\\n        def decorator(f):\\n            self.add_event_handler(f, event)\\n            return f\\n\\n        return decorator', 'def add_event_handler(self, callback, event=None):\\n        \"\"\"\\n        Registers the given callback to be called on the specified event.\\n\\n        Args:\\n            callback (`callable`):\\n                The callable function accepting one parameter to be used.\\n\\n                Note that if you have used `telethon.events.register` in\\n                the callback, ``event`` will be ignored, and instead the\\n                events you previously registered will be used.\\n\\n            event (`_EventBuilder` | `type`, optional):\\n                The event builder class or instance to be used,\\n                for instance ``events.NewMessage``.\\n\\n                If left unspecified, `telethon.events.raw.Raw` (the\\n                :tl:`Update` objects with no further processing) will\\n                be passed instead.\\n        \"\"\"\\n        builders = events._get_handlers(callback)\\n        if builders is not None:\\n            for event in builders:\\n                self._event_builders.append((event, callback))\\n            return\\n\\n        if isinstance(event, type):\\n            event = event()\\n        elif not event:\\n            event = events.Raw()\\n\\n        self._event_builders.append((event, callback))', 'def remove_event_handler(self, callback, event=None):\\n        \"\"\"\\n        Inverse operation of :meth:`add_event_handler`.\\n\\n        If no event is given, all events for this callback are removed.\\n        Returns how many callbacks were removed.\\n        \"\"\"\\n        found = 0\\n        if event and not isinstance(event, type):\\n            event = type(event)\\n\\n        i = len(self._event_builders)\\n        while i:\\n            i -= 1\\n            ev, cb = self._event_builders[i]\\n            if cb == callback and (not event or isinstance(ev, event)):\\n                del self._event_builders[i]\\n                found += 1\\n\\n        return found', 'async def catch_up(self):\\n        \"\"\"\\n        \"Catches up\" on the missed updates while the client was offline.\\n        You should call this method after registering the event handlers\\n        so that the updates it loads can by processed by your script.\\n\\n        This can also be used to forcibly fetch new updates if there are any.\\n        \"\"\"\\n        pts, date = self._state_cache[None]\\n        self.session.catching_up = True\\n        try:\\n            while True:\\n                d = await self(functions.updates.GetDifferenceRequest(\\n                    pts, date, 0\\n                ))\\n                if isinstance(d, (types.updates.DifferenceSlice,\\n                                  types.updates.Difference)):\\n                    if isinstance(d, types.updates.Difference):\\n                        state = d.state\\n                    else:\\n                        state = d.intermediate_state\\n\\n                    pts, date = state.pts, state.date\\n                    self._handle_update(types.Updates(\\n                        users=d.users,\\n                        chats=d.chats,\\n                        date=state.date,\\n                        seq=state.seq,\\n                        updates=d.other_updates + [\\n                            types.UpdateNewMessage(m, 0, 0)\\n                            for m in d.new_messages\\n                        ]\\n                    ))\\n\\n                    # TODO Implement upper limit (max_pts)\\n                    # We don\\'t want to fetch updates we already know about.\\n                    #\\n                    # We may still get duplicates because the Difference\\n                    # contains a lot of updates and presumably only has\\n                    # the state for the last one, but at least we don\\'t\\n                    # unnecessarily fetch too many.\\n                    #\\n                    # updates.getDifference\\'s pts_total_limit seems to mean\\n                    # \"how many pts is the request allowed to return\", and\\n                    # if there is more than that, it returns \"too long\" (so\\n                    # there would be duplicate updates since we know about\\n                    # some). This can be used to detect collisions (i.e.\\n                    # it would return an update we have already seen).\\n                else:\\n                    if isinstance(d, types.updates.DifferenceEmpty):\\n                        date = d.date\\n                    elif isinstance(d, types.updates.DifferenceTooLong):\\n                        pts = d.pts\\n                    break\\n        except (ConnectionError, asyncio.CancelledError):\\n            pass\\n        finally:\\n            # TODO Save new pts to session\\n            self._state_cache._pts_date = (pts, date)\\n            self.session.catching_up = False', 'def reset(self):\\n        \"\"\"\\n        Resets the state.\\n        \"\"\"\\n        # Session IDs can be random on every connection\\n        self.id = struct.unpack(\\'q\\', os.urandom(8))[0]\\n        self._sequence = 0\\n        self._last_msg_id = 0', 'def _calc_key(auth_key, msg_key, client):\\n        \"\"\"\\n        Calculate the key based on Telegram guidelines for MTProto 2,\\n        specifying whether it\\'s the client or not. See\\n        https://core.telegram.org/mtproto/description#defining-aes-key-and-initialization-vector\\n        \"\"\"\\n        x = 0 if client else 8\\n        sha256a = sha256(msg_key + auth_key[x: x + 36]).digest()\\n        sha256b = sha256(auth_key[x + 40:x + 76] + msg_key).digest()\\n\\n        aes_key = sha256a[:8] + sha256b[8:24] + sha256a[24:32]\\n        aes_iv = sha256b[:8] + sha256a[8:24] + sha256b[24:32]\\n\\n        return aes_key, aes_iv', 'def write_data_as_message(self, buffer, data, content_related,\\n                              *, after_id=None):\\n        \"\"\"\\n        Writes a message containing the given data into buffer.\\n\\n        Returns the message id.\\n        \"\"\"\\n        msg_id = self._get_new_msg_id()\\n        seq_no = self._get_seq_no(content_related)\\n        if after_id is None:\\n            body = GzipPacked.gzip_if_smaller(content_related, data)\\n        else:\\n            body = GzipPacked.gzip_if_smaller(content_related,\\n                bytes(InvokeAfterMsgRequest(after_id, data)))\\n\\n        buffer.write(struct.pack(\\'<qii\\', msg_id, seq_no, len(body)))\\n        buffer.write(body)\\n        return msg_id', 'def encrypt_message_data(self, data):\\n        \"\"\"\\n        Encrypts the given message data using the current authorization key\\n        following MTProto 2.0 guidelines core.telegram.org/mtproto/description.\\n        \"\"\"\\n        data = struct.pack(\\'<qq\\', self.salt, self.id) + data\\n        padding = os.urandom(-(len(data) + 12) % 16 + 12)\\n\\n        # Being substr(what, offset, length); x = 0 for client\\n        # \"msg_key_large = SHA256(substr(auth_key, 88+x, 32) + pt + padding)\"\\n        msg_key_large = sha256(\\n            self.auth_key.key[88:88 + 32] + data + padding).digest()\\n\\n        # \"msg_key = substr (msg_key_large, 8, 16)\"\\n        msg_key = msg_key_large[8:24]\\n        aes_key, aes_iv = self._calc_key(self.auth_key.key, msg_key, True)\\n\\n        key_id = struct.pack(\\'<Q\\', self.auth_key.key_id)\\n        return (key_id + msg_key +\\n                AES.encrypt_ige(data + padding, aes_key, aes_iv))', 'def decrypt_message_data(self, body):\\n        \"\"\"\\n        Inverse of `encrypt_message_data` for incoming server messages.\\n        \"\"\"\\n        if len(body) < 8:\\n            raise InvalidBufferError(body)\\n\\n        # TODO Check salt, session_id and sequence_number\\n        key_id = struct.unpack(\\'<Q\\', body[:8])[0]\\n        if key_id != self.auth_key.key_id:\\n            raise SecurityError(\\'Server replied with an invalid auth key\\')\\n\\n        msg_key = body[8:24]\\n        aes_key, aes_iv = self._calc_key(self.auth_key.key, msg_key, False)\\n        body = AES.decrypt_ige(body[24:], aes_key, aes_iv)\\n\\n        # https://core.telegram.org/mtproto/security_guidelines\\n        # Sections \"checking sha256 hash\" and \"message length\"\\n        our_key = sha256(self.auth_key.key[96:96 + 32] + body)\\n        if msg_key != our_key.digest()[8:24]:\\n            raise SecurityError(\\n                \"Received msg_key doesn\\'t match with expected one\")\\n\\n        reader = BinaryReader(body)\\n        reader.read_long()  # remote_salt\\n        if reader.read_long() != self.id:\\n            raise SecurityError(\\'Server replied with a wrong session ID\\')\\n\\n        remote_msg_id = reader.read_long()\\n        remote_sequence = reader.read_int()\\n        reader.read_int()  # msg_len for the inner object, padding ignored\\n\\n        # We could read msg_len bytes and use those in a new reader to read\\n        # the next TLObject without including the padding, but since the\\n        # reader isn\\'t used for anything else after this, it\\'s unnecessary.\\n        obj = reader.tgread_object()\\n\\n        return TLMessage(remote_msg_id, remote_sequence, obj)', 'def _get_new_msg_id(self):\\n        \"\"\"\\n        Generates a new unique message ID based on the current\\n        time (in ms) since epoch, applying a known time offset.\\n        \"\"\"\\n        now = time.time() + self.time_offset\\n        nanoseconds = int((now - int(now)) * 1e+9)\\n        new_msg_id = (int(now) << 32) | (nanoseconds << 2)\\n\\n        if self._last_msg_id >= new_msg_id:\\n            new_msg_id = self._last_msg_id + 4\\n\\n        self._last_msg_id = new_msg_id\\n        return new_msg_id', 'def update_time_offset(self, correct_msg_id):\\n        \"\"\"\\n        Updates the time offset to the correct\\n        one given a known valid message ID.\\n        \"\"\"\\n        bad = self._get_new_msg_id()\\n        old = self.time_offset\\n\\n        now = int(time.time())\\n        correct = correct_msg_id >> 32\\n        self.time_offset = correct - now\\n\\n        if self.time_offset != old:\\n            self._last_msg_id = 0\\n            self._log.debug(\\n                \\'Updated time offset (old offset %d, bad %d, good %d, new %d)\\',\\n                old, bad, correct_msg_id, self.time_offset\\n            )\\n\\n        return self.time_offset', 'def _get_seq_no(self, content_related):\\n        \"\"\"\\n        Generates the next sequence number depending on whether\\n        it should be for a content-related query or not.\\n        \"\"\"\\n        if content_related:\\n            result = self._sequence * 2 + 1\\n            self._sequence += 1\\n            return result\\n        else:\\n            return self._sequence * 2', 'def parse_tl(file_path, layer, methods=None, ignored_ids=CORE_TYPES):\\n    \"\"\"\\n    This method yields TLObjects from a given .tl file.\\n\\n    Note that the file is parsed completely before the function yields\\n    because references to other objects may appear later in the file.\\n    \"\"\"\\n    method_info = {m.name: m for m in (methods or [])}\\n    obj_all = []\\n    obj_by_name = {}\\n    obj_by_type = collections.defaultdict(list)\\n    with file_path.open() as file:\\n        is_function = False\\n        for line in file:\\n            comment_index = line.find(\\'//\\')\\n            if comment_index != -1:\\n                line = line[:comment_index]\\n\\n            line = line.strip()\\n            if not line:\\n                continue\\n\\n            match = re.match(\\'---(\\\\w+)---\\', line)\\n            if match:\\n                following_types = match.group(1)\\n                is_function = following_types == \\'functions\\'\\n                continue\\n\\n            try:\\n                result = _from_line(\\n                    line, is_function, method_info, layer=layer)\\n\\n                if result.id in ignored_ids:\\n                    continue\\n\\n                obj_all.append(result)\\n                if not result.is_function:\\n                    obj_by_name[result.fullname] = result\\n                    obj_by_type[result.result].append(result)\\n            except ValueError as e:\\n                if \\'vector#1cb5c415\\' not in str(e):\\n                    raise\\n\\n    # Once all objects have been parsed, replace the\\n    # string type from the arguments with references\\n    for obj in obj_all:\\n        if obj.id in AUTH_KEY_TYPES:\\n            for arg in obj.args:\\n                if arg.type == \\'string\\':\\n                    arg.type = \\'bytes\\'\\n\\n        for arg in obj.args:\\n            arg.cls = obj_by_type.get(arg.type) or (\\n                [obj_by_name[arg.type]] if arg.type in obj_by_name else []\\n            )\\n\\n    yield from obj_all', 'def find_layer(file_path):\\n    \"\"\"Finds the layer used on the specified scheme.tl file.\"\"\"\\n    layer_regex = re.compile(r\\'^//\\\\s*LAYER\\\\s*(\\\\d+)$\\')\\n    with file_path.open(\\'r\\') as file:\\n        for line in file:\\n            match = layer_regex.match(line)\\n            if match:\\n                return int(match.group(1))', 'def url(self):\\n        \"\"\"\\n        The URL present in this inline results. If you want to \"click\"\\n        this URL to open it in your browser, you should use Python\\'s\\n        `webbrowser.open(url)` for such task.\\n        \"\"\"\\n        if isinstance(self.result, types.BotInlineResult):\\n            return self.result.url', 'def photo(self):\\n        \"\"\"\\n        Returns either the :tl:`WebDocument` thumbnail for\\n        normal results or the :tl:`Photo` for media results.\\n        \"\"\"\\n        if isinstance(self.result, types.BotInlineResult):\\n            return self.result.thumb\\n        elif isinstance(self.result, types.BotInlineMediaResult):\\n            return self.result.photo', 'def document(self):\\n        \"\"\"\\n        Returns either the :tl:`WebDocument` content for\\n        normal results or the :tl:`Document` for media results.\\n        \"\"\"\\n        if isinstance(self.result, types.BotInlineResult):\\n            return self.result.content\\n        elif isinstance(self.result, types.BotInlineMediaResult):\\n            return self.result.document', 'async def click(self, entity, reply_to=None,\\n                    silent=False, clear_draft=False, hide_via=False):\\n        \"\"\"\\n        Clicks this result and sends the associated `message`.\\n\\n        Args:\\n            entity (`entity`):\\n                The entity to which the message of this result should be sent.\\n\\n            reply_to (`int` | `Message <telethon.tl.custom.message.Message>`, optional):\\n                If present, the sent message will reply to this ID or message.\\n\\n            silent (`bool`, optional):\\n                If ``True``, the sent message will not notify the user(s).\\n\\n            clear_draft (`bool`, optional):\\n                Whether the draft should be removed after sending the\\n                message from this result or not. Defaults to ``False``.\\n            \\n            hide_via (`bool`, optional):\\n                Whether the \"via @bot\" should be hidden or not.\\n                Only works with certain bots (like @bing or @gif).\\n        \"\"\"\\n        entity = await self._client.get_input_entity(entity)\\n        reply_id = None if reply_to is None else utils.get_message_id(reply_to)\\n        req = functions.messages.SendInlineBotResultRequest(\\n            peer=entity,\\n            query_id=self._query_id,\\n            id=self.result.id,\\n            silent=silent,\\n            clear_draft=clear_draft,\\n            hide_via=hide_via,\\n            reply_to_msg_id=reply_id\\n        )\\n        return self._client._get_response_message(\\n            req, await self._client(req), entity)', 'async def download_media(self, *args, **kwargs):\\n        \"\"\"\\n        Downloads the media in this result (if there is a document, the\\n        document will be downloaded; otherwise, the photo will if present).\\n\\n        This is a wrapper around `client.download_media\\n        <telethon.client.downloads.DownloadMethods.download_media>`.\\n        \"\"\"\\n        if self.document or self.photo:\\n            return await self._client.download_media(\\n                self.document or self.photo, *args, **kwargs)', 'def update(\\n            self,\\n            update,\\n            *,\\n            channel_id=None,\\n            has_pts=(\\n                types.UpdateNewMessage,\\n                types.UpdateDeleteMessages,\\n                types.UpdateReadHistoryInbox,\\n                types.UpdateReadHistoryOutbox,\\n                types.UpdateWebPage,\\n                types.UpdateReadMessagesContents,\\n                types.UpdateEditMessage,\\n                types.updates.State,\\n                types.updates.DifferenceTooLong,\\n                types.UpdateShortMessage,\\n                types.UpdateShortChatMessage,\\n                types.UpdateShortSentMessage\\n            ),\\n            has_date=(\\n                types.UpdateUserPhoto,\\n                types.UpdateEncryption,\\n                types.UpdateEncryptedMessagesRead,\\n                types.UpdateChatParticipantAdd,\\n                types.updates.DifferenceEmpty,\\n                types.UpdateShortMessage,\\n                types.UpdateShortChatMessage,\\n                types.UpdateShort,\\n                types.UpdatesCombined,\\n                types.Updates,\\n                types.UpdateShortSentMessage,\\n            ),\\n            has_channel_pts=(\\n                types.UpdateChannelTooLong,\\n                types.UpdateNewChannelMessage,\\n                types.UpdateDeleteChannelMessages,\\n                types.UpdateEditChannelMessage,\\n                types.UpdateChannelWebPage,\\n                types.updates.ChannelDifferenceEmpty,\\n                types.updates.ChannelDifferenceTooLong,\\n                types.updates.ChannelDifference\\n            )\\n    ):\\n        \"\"\"\\n        Update the state with the given update.\\n        \"\"\"\\n        has_pts = isinstance(update, has_pts)\\n        has_date = isinstance(update, has_date)\\n        has_channel_pts = isinstance(update, has_channel_pts)\\n        if has_pts and has_date:\\n            self._pts_date = update.pts, update.date\\n        elif has_pts:\\n            self._pts_date = update.pts, self._pts_date[1]\\n        elif has_date:\\n            self._pts_date = self._pts_date[0], update.date\\n\\n        if has_channel_pts:\\n            if channel_id is None:\\n                channel_id = self.get_channel_id(update)\\n\\n            if channel_id is None:\\n                self._logger.info(\\n                    \\'Failed to retrieve channel_id from %s\\', update)\\n            else:\\n                self.__dict__[channel_id] = update.pts', 'def iter_dialogs(\\n            self, limit=None, *, offset_date=None, offset_id=0,\\n            offset_peer=types.InputPeerEmpty(), ignore_migrated=False\\n    ):\\n        \"\"\"\\n        Returns an iterator over the dialogs, yielding \\'limit\\' at most.\\n        Dialogs are the open \"chats\" or conversations with other people,\\n        groups you have joined, or channels you are subscribed to.\\n\\n        Args:\\n            limit (`int` | `None`):\\n                How many dialogs to be retrieved as maximum. Can be set to\\n                ``None`` to retrieve all dialogs. Note that this may take\\n                whole minutes if you have hundreds of dialogs, as Telegram\\n                will tell the library to slow down through a\\n                ``FloodWaitError``.\\n\\n            offset_date (`datetime`, optional):\\n                The offset date to be used.\\n\\n            offset_id (`int`, optional):\\n                The message ID to be used as an offset.\\n\\n            offset_peer (:tl:`InputPeer`, optional):\\n                The peer to be used as an offset.\\n\\n            ignore_migrated (`bool`, optional):\\n                Whether :tl:`Chat` that have ``migrated_to`` a :tl:`Channel`\\n                should be included or not. By default all the chats in your\\n                dialogs are returned, but setting this to ``True`` will hide\\n                them in the same way official applications do.\\n\\n        Yields:\\n            Instances of `telethon.tl.custom.dialog.Dialog`.\\n        \"\"\"\\n        return _DialogsIter(\\n            self,\\n            limit,\\n            offset_date=offset_date,\\n            offset_id=offset_id,\\n            offset_peer=offset_peer,\\n            ignore_migrated=ignore_migrated\\n        )', 'def conversation(\\n            self, entity,\\n            *, timeout=60, total_timeout=None, max_messages=100,\\n            exclusive=True, replies_are_responses=True):\\n        \"\"\"\\n        Creates a `Conversation <telethon.tl.custom.conversation.Conversation>`\\n        with the given entity so you can easily send messages and await for\\n        responses or other reactions. Refer to its documentation for more.\\n\\n        Args:\\n            entity (`entity`):\\n                The entity with which a new conversation should be opened.\\n\\n            timeout (`int` | `float`, optional):\\n                The default timeout (in seconds) *per action* to be used. You\\n                may also override this timeout on a per-method basis. By\\n                default each action can take up to 60 seconds (the value of\\n                this timeout).\\n\\n            total_timeout (`int` | `float`, optional):\\n                The total timeout (in seconds) to use for the whole\\n                conversation. This takes priority over per-action\\n                timeouts. After these many seconds pass, subsequent\\n                actions will result in ``asyncio.TimeoutError``.\\n\\n            max_messages (`int`, optional):\\n                The maximum amount of messages this conversation will\\n                remember. After these many messages arrive in the\\n                specified chat, subsequent actions will result in\\n                ``ValueError``.\\n\\n            exclusive (`bool`, optional):\\n                By default, conversations are exclusive within a single\\n                chat. That means that while a conversation is open in a\\n                chat, you can\\'t open another one in the same chat, unless\\n                you disable this flag.\\n\\n                If you try opening an exclusive conversation for\\n                a chat where it\\'s already open, it will raise\\n                ``AlreadyInConversationError``.\\n\\n            replies_are_responses (`bool`, optional):\\n                Whether replies should be treated as responses or not.\\n\\n                If the setting is enabled, calls to `conv.get_response\\n                <telethon.tl.custom.conversation.Conversation.get_response>`\\n                and a subsequent call to `conv.get_reply\\n                <telethon.tl.custom.conversation.Conversation.get_reply>`\\n                will return different messages, otherwise they may return\\n                the same message.\\n\\n                Consider the following scenario with one outgoing message,\\n                1, and two incoming messages, the second one replying::\\n\\n                                        Hello! <1\\n                    2> (reply to 1) Hi!\\n                    3> (reply to 1) How are you?\\n\\n                And the following code:\\n\\n                .. code-block:: python\\n\\n                    async with client.conversation(chat) as conv:\\n                        msg1 = await conv.send_message(\\'Hello!\\')\\n                        msg2 = await conv.get_response()\\n                        msg3 = await conv.get_reply()\\n\\n                With the setting enabled, ``msg2`` will be ``\\'Hi!\\'`` and\\n                ``msg3`` be ``\\'How are you?\\'`` since replies are also\\n                responses, and a response was already returned.\\n\\n                With the setting disabled, both ``msg2`` and ``msg3`` will\\n                be ``\\'Hi!\\'`` since one is a response and also a reply.\\n\\n        Returns:\\n            A `Conversation <telethon.tl.custom.conversation.Conversation>`.\\n        \"\"\"\\n        return custom.Conversation(\\n            self,\\n            entity,\\n            timeout=timeout,\\n            total_timeout=total_timeout,\\n            max_messages=max_messages,\\n            exclusive=exclusive,\\n            replies_are_responses=replies_are_responses\\n\\n        )', 'async def connect(self, connection):\\n        \"\"\"\\n        Connects to the specified given connection using the given auth key.\\n        \"\"\"\\n        if self._user_connected:\\n            self._log.info(\\'User is already connected!\\')\\n            return\\n\\n        self._connection = connection\\n        await self._connect()\\n        self._user_connected = True', 'def send(self, request, ordered=False):\\n        \"\"\"\\n        This method enqueues the given request to be sent. Its send\\n        state will be saved until a response arrives, and a ``Future``\\n        that will be resolved when the response arrives will be returned:\\n\\n        .. code-block:: python\\n\\n            async def method():\\n                # Sending (enqueued for the send loop)\\n                future = sender.send(request)\\n                # Receiving (waits for the receive loop to read the result)\\n                result = await future\\n\\n        Designed like this because Telegram may send the response at\\n        any point, and it can send other items while one waits for it.\\n        Once the response for this future arrives, it is set with the\\n        received result, quite similar to how a ``receive()`` call\\n        would otherwise work.\\n\\n        Since the receiving part is \"built in\" the future, it\\'s\\n        impossible to await receive a result that was never sent.\\n        \"\"\"\\n        if not self._user_connected:\\n            raise ConnectionError(\\'Cannot send requests while disconnected\\')\\n\\n        if not utils.is_list_like(request):\\n            state = RequestState(request, self._loop)\\n            self._send_queue.append(state)\\n            return state.future\\n        else:\\n            states = []\\n            futures = []\\n            state = None\\n            for req in request:\\n                state = RequestState(req, self._loop, after=ordered and state)\\n                states.append(state)\\n                futures.append(state.future)\\n\\n            self._send_queue.extend(states)\\n            return futures', 'async def _connect(self):\\n        \"\"\"\\n        Performs the actual connection, retrying, generating the\\n        authorization key if necessary, and starting the send and\\n        receive loops.\\n        \"\"\"\\n        self._log.info(\\'Connecting to %s...\\', self._connection)\\n        for attempt in retry_range(self._retries):\\n            try:\\n                self._log.debug(\\'Connection attempt {}...\\'.format(attempt))\\n                await self._connection.connect(timeout=self._connect_timeout)\\n            except (IOError, asyncio.TimeoutError) as e:\\n                self._log.warning(\\'Attempt {} at connecting failed: {}: {}\\'\\n                                  .format(attempt, type(e).__name__, e))\\n                await asyncio.sleep(self._delay)\\n            else:\\n                break\\n        else:\\n            raise ConnectionError(\\'Connection to Telegram failed {} time(s)\\'\\n                                  .format(attempt))\\n\\n        self._log.debug(\\'Connection success!\\')\\n        if not self.auth_key:\\n            plain = MTProtoPlainSender(self._connection, loggers=self._loggers)\\n            for attempt in retry_range(self._retries):\\n                try:\\n                    self._log.debug(\\'New auth_key attempt {}...\\'\\n                                    .format(attempt))\\n                    self.auth_key.key, self._state.time_offset =\\\\\\n                        await authenticator.do_authentication(plain)\\n\\n                    # This is *EXTREMELY* important since we don\\'t control\\n                    # external references to the authorization key, we must\\n                    # notify whenever we change it. This is crucial when we\\n                    # switch to different data centers.\\n                    if self._auth_key_callback:\\n                        self._auth_key_callback(self.auth_key)\\n\\n                    break\\n                except (SecurityError, AssertionError) as e:\\n                    self._log.warning(\\'Attempt {} at new auth_key failed: {}\\'\\n                                      .format(attempt, e))\\n                    await asyncio.sleep(self._delay)\\n            else:\\n                e = ConnectionError(\\'auth_key generation failed {} time(s)\\'\\n                                    .format(attempt))\\n                await self._disconnect(error=e)\\n                raise e\\n\\n        self._log.debug(\\'Starting send loop\\')\\n        self._send_loop_handle = self._loop.create_task(self._send_loop())\\n\\n        self._log.debug(\\'Starting receive loop\\')\\n        self._recv_loop_handle = self._loop.create_task(self._recv_loop())\\n\\n        # _disconnected only completes after manual disconnection\\n        # or errors after which the sender cannot continue such\\n        # as failing to reconnect or any unexpected error.\\n        if self._disconnected.done():\\n            self._disconnected = self._loop.create_future()\\n\\n        self._log.info(\\'Connection to %s complete!\\', self._connection)', 'async def _reconnect(self, last_error):\\n        \"\"\"\\n        Cleanly disconnects and then reconnects.\\n        \"\"\"\\n        self._log.debug(\\'Closing current connection...\\')\\n        await self._connection.disconnect()\\n\\n        await helpers._cancel(\\n            self._log,\\n            send_loop_handle=self._send_loop_handle,\\n            recv_loop_handle=self._recv_loop_handle\\n        )\\n\\n        # TODO See comment in `_start_reconnect`\\n        # Perhaps this should be the last thing to do?\\n        # But _connect() creates tasks which may run and,\\n        # if they see that reconnecting is True, they will end.\\n        # Perhaps that task creation should not belong in connect?\\n        self._reconnecting = False\\n\\n        # Start with a clean state (and thus session ID) to avoid old msgs\\n        self._state.reset()\\n\\n        retries = self._retries if self._auto_reconnect else 0\\n        for attempt in retry_range(retries):\\n            try:\\n                await self._connect()\\n            except (IOError, asyncio.TimeoutError) as e:\\n                last_error = e\\n                self._log.info(\\'Failed reconnection attempt %d with %s\\',\\n                               attempt, e.__class__.__name__)\\n\\n                await asyncio.sleep(self._delay)\\n            except Exception as e:\\n                last_error = e\\n                self._log.exception(\\'Unexpected exception reconnecting on \\'\\n                                    \\'attempt %d\\', attempt)\\n\\n                await asyncio.sleep(self._delay)\\n            else:\\n                self._send_queue.extend(self._pending_state.values())\\n                self._pending_state.clear()\\n\\n                if self._auto_reconnect_callback:\\n                    self._loop.create_task(self._auto_reconnect_callback())\\n\\n                break\\n        else:\\n            self._log.error(\\'Automatic reconnection failed {} time(s)\\'\\n                            .format(attempt))\\n            await self._disconnect(error=last_error.with_traceback(None))', 'def _start_reconnect(self, error):\\n        \"\"\"Starts a reconnection in the background.\"\"\"\\n        if self._user_connected and not self._reconnecting:\\n            # We set reconnecting to True here and not inside the new task\\n            # because it may happen that send/recv loop calls this again\\n            # while the new task hasn\\'t had a chance to run yet. This race\\n            # condition puts `self.connection` in a bad state with two calls\\n            # to its `connect` without disconnecting, so it creates a second\\n            # receive loop. There can\\'t be two tasks receiving data from\\n            # the reader, since that causes an error, and the library just\\n            # gets stuck.\\n            # TODO It still gets stuck? Investigate where and why.\\n            self._reconnecting = True\\n            self._loop.create_task(self._reconnect(error))', 'async def _send_loop(self):\\n        \"\"\"\\n        This loop is responsible for popping items off the send\\n        queue, encrypting them, and sending them over the network.\\n\\n        Besides `connect`, only this method ever sends data.\\n        \"\"\"\\n        while self._user_connected and not self._reconnecting:\\n            if self._pending_ack:\\n                ack = RequestState(MsgsAck(list(self._pending_ack)), self._loop)\\n                self._send_queue.append(ack)\\n                self._last_acks.append(ack)\\n                self._pending_ack.clear()\\n\\n            self._log.debug(\\'Waiting for messages to send...\\')\\n            # TODO Wait for the connection send queue to be empty?\\n            # This means that while it\\'s not empty we can wait for\\n            # more messages to be added to the send queue.\\n            batch, data = await self._send_queue.get()\\n\\n            if not data:\\n                continue\\n\\n            self._log.debug(\\'Encrypting %d message(s) in %d bytes for sending\\',\\n                          len(batch), len(data))\\n\\n            data = self._state.encrypt_message_data(data)\\n            try:\\n                await self._connection.send(data)\\n            except IOError as e:\\n                self._log.info(\\'Connection closed while sending data\\')\\n                self._start_reconnect(e)\\n                return\\n\\n            for state in batch:\\n                if not isinstance(state, list):\\n                    if isinstance(state.request, TLRequest):\\n                        self._pending_state[state.msg_id] = state\\n                else:\\n                    for s in state:\\n                        if isinstance(s.request, TLRequest):\\n                            self._pending_state[s.msg_id] = s\\n\\n            self._log.debug(\\'Encrypted messages put in a queue to be sent\\')', 'async def _recv_loop(self):\\n        \"\"\"\\n        This loop is responsible for reading all incoming responses\\n        from the network, decrypting and handling or dispatching them.\\n\\n        Besides `connect`, only this method ever receives data.\\n        \"\"\"\\n        while self._user_connected and not self._reconnecting:\\n            self._log.debug(\\'Receiving items from the network...\\')\\n            try:\\n                body = await self._connection.recv()\\n            except IOError as e:\\n                self._log.info(\\'Connection closed while receiving data\\')\\n                self._start_reconnect(e)\\n                return\\n\\n            try:\\n                message = self._state.decrypt_message_data(body)\\n            except TypeNotFoundError as e:\\n                # Received object which we don\\'t know how to deserialize\\n                self._log.info(\\'Type %08x not found, remaining data %r\\',\\n                             e.invalid_constructor_id, e.remaining)\\n                continue\\n            except SecurityError as e:\\n                # A step while decoding had the incorrect data. This message\\n                # should not be considered safe and it should be ignored.\\n                self._log.warning(\\'Security error while unpacking a \\'\\n                                \\'received message: %s\\', e)\\n                continue\\n            except BufferError as e:\\n                if isinstance(e, InvalidBufferError) and e.code == 404:\\n                    self._log.info(\\'Broken authorization key; resetting\\')\\n                else:\\n                    self._log.warning(\\'Invalid buffer %s\\', e)\\n\\n                self.auth_key.key = None\\n                if self._auth_key_callback:\\n                    self._auth_key_callback(None)\\n\\n                self._start_reconnect(e)\\n                return\\n            except Exception as e:\\n                self._log.exception(\\'Unhandled error while receiving data\\')\\n                self._start_reconnect(e)\\n                return\\n\\n            try:\\n                await self._process_message(message)\\n            except Exception:\\n                self._log.exception(\\'Unhandled error while processing msgs\\')', 'async def _process_message(self, message):\\n        \"\"\"\\n        Adds the given message to the list of messages that must be\\n        acknowledged and dispatches control to different ``_handle_*``\\n        method based on its type.\\n        \"\"\"\\n        self._pending_ack.add(message.msg_id)\\n        handler = self._handlers.get(message.obj.CONSTRUCTOR_ID,\\n                                     self._handle_update)\\n        await handler(message)', 'def _pop_states(self, msg_id):\\n        \"\"\"\\n        Pops the states known to match the given ID from pending messages.\\n\\n        This method should be used when the response isn\\'t specific.\\n        \"\"\"\\n        state = self._pending_state.pop(msg_id, None)\\n        if state:\\n            return [state]\\n\\n        to_pop = []\\n        for state in self._pending_state.values():\\n            if state.container_id == msg_id:\\n                to_pop.append(state.msg_id)\\n\\n        if to_pop:\\n            return [self._pending_state.pop(x) for x in to_pop]\\n\\n        for ack in self._last_acks:\\n            if ack.msg_id == msg_id:\\n                return [ack]\\n\\n        return []', 'async def _handle_rpc_result(self, message):\\n        \"\"\"\\n        Handles the result for Remote Procedure Calls:\\n\\n            rpc_result#f35c6d01 req_msg_id:long result:bytes = RpcResult;\\n\\n        This is where the future results for sent requests are set.\\n        \"\"\"\\n        rpc_result = message.obj\\n        state = self._pending_state.pop(rpc_result.req_msg_id, None)\\n        self._log.debug(\\'Handling RPC result for message %d\\',\\n                      rpc_result.req_msg_id)\\n\\n        if not state:\\n            # TODO We should not get responses to things we never sent\\n            # However receiving a File() with empty bytes is \"common\".\\n            # See #658, #759 and #958. They seem to happen in a container\\n            # which contain the real response right after.\\n            try:\\n                with BinaryReader(rpc_result.body) as reader:\\n                    if not isinstance(reader.tgread_object(), upload.File):\\n                        raise ValueError(\\'Not an upload.File\\')\\n            except (TypeNotFoundError, ValueError):\\n                self._log.info(\\'Received response without parent request: {}\\'\\n                               .format(rpc_result.body))\\n            return\\n\\n        if rpc_result.error:\\n            error = rpc_message_to_error(rpc_result.error, state.request)\\n            self._send_queue.append(\\n                RequestState(MsgsAck([state.msg_id]), loop=self._loop))\\n\\n            if not state.future.cancelled():\\n                state.future.set_exception(error)\\n        else:\\n            with BinaryReader(rpc_result.body) as reader:\\n                result = state.request.read_result(reader)\\n\\n            if not state.future.cancelled():\\n                state.future.set_result(result)', 'async def _handle_container(self, message):\\n        \"\"\"\\n        Processes the inner messages of a container with many of them:\\n\\n            msg_container#73f1f8dc messages:vector<%Message> = MessageContainer;\\n        \"\"\"\\n        self._log.debug(\\'Handling container\\')\\n        for inner_message in message.obj.messages:\\n            await self._process_message(inner_message)', 'async def _handle_gzip_packed(self, message):\\n        \"\"\"\\n        Unpacks the data from a gzipped object and processes it:\\n\\n            gzip_packed#3072cfa1 packed_data:bytes = Object;\\n        \"\"\"\\n        self._log.debug(\\'Handling gzipped data\\')\\n        with BinaryReader(message.obj.data) as reader:\\n            message.obj = reader.tgread_object()\\n            await self._process_message(message)', 'async def _handle_pong(self, message):\\n        \"\"\"\\n        Handles pong results, which don\\'t come inside a ``rpc_result``\\n        but are still sent through a request:\\n\\n            pong#347773c5 msg_id:long ping_id:long = Pong;\\n        \"\"\"\\n        pong = message.obj\\n        self._log.debug(\\'Handling pong for message %d\\', pong.msg_id)\\n        state = self._pending_state.pop(pong.msg_id, None)\\n        if state:\\n            state.future.set_result(pong)', 'async def _handle_bad_server_salt(self, message):\\n        \"\"\"\\n        Corrects the currently used server salt to use the right value\\n        before enqueuing the rejected message to be re-sent:\\n\\n            bad_server_salt#edab447b bad_msg_id:long bad_msg_seqno:int\\n            error_code:int new_server_salt:long = BadMsgNotification;\\n        \"\"\"\\n        bad_salt = message.obj\\n        self._log.debug(\\'Handling bad salt for message %d\\', bad_salt.bad_msg_id)\\n        self._state.salt = bad_salt.new_server_salt\\n        states = self._pop_states(bad_salt.bad_msg_id)\\n        self._send_queue.extend(states)\\n\\n        self._log.debug(\\'%d message(s) will be resent\\', len(states))', 'async def _handle_bad_notification(self, message):\\n        \"\"\"\\n        Adjusts the current state to be correct based on the\\n        received bad message notification whenever possible:\\n\\n            bad_msg_notification#a7eff811 bad_msg_id:long bad_msg_seqno:int\\n            error_code:int = BadMsgNotification;\\n        \"\"\"\\n        bad_msg = message.obj\\n        states = self._pop_states(bad_msg.bad_msg_id)\\n\\n        self._log.debug(\\'Handling bad msg %s\\', bad_msg)\\n        if bad_msg.error_code in (16, 17):\\n            # Sent msg_id too low or too high (respectively).\\n            # Use the current msg_id to determine the right time offset.\\n            to = self._state.update_time_offset(\\n                correct_msg_id=message.msg_id)\\n            self._log.info(\\'System clock is wrong, set time offset to %ds\\', to)\\n        elif bad_msg.error_code == 32:\\n            # msg_seqno too low, so just pump it up by some \"large\" amount\\n            # TODO A better fix would be to start with a new fresh session ID\\n            self._state._sequence += 64\\n        elif bad_msg.error_code == 33:\\n            # msg_seqno too high never seems to happen but just in case\\n            self._state._sequence -= 16\\n        else:\\n            for state in states:\\n                state.future.set_exception(\\n                    BadMessageError(state.request, bad_msg.error_code))\\n            return\\n\\n        # Messages are to be re-sent once we\\'ve corrected the issue\\n        self._send_queue.extend(states)\\n        self._log.debug(\\'%d messages will be resent due to bad msg\\',\\n                        len(states))', 'async def _handle_detailed_info(self, message):\\n        \"\"\"\\n        Updates the current status with the received detailed information:\\n\\n            msg_detailed_info#276d3ec6 msg_id:long answer_msg_id:long\\n            bytes:int status:int = MsgDetailedInfo;\\n        \"\"\"\\n        # TODO https://goo.gl/VvpCC6\\n        msg_id = message.obj.answer_msg_id\\n        self._log.debug(\\'Handling detailed info for message %d\\', msg_id)\\n        self._pending_ack.add(msg_id)', 'async def _handle_new_session_created(self, message):\\n        \"\"\"\\n        Updates the current status with the received session information:\\n\\n            new_session_created#9ec20908 first_msg_id:long unique_id:long\\n            server_salt:long = NewSession;\\n        \"\"\"\\n        # TODO https://goo.gl/LMyN7A\\n        self._log.debug(\\'Handling new session created\\')\\n        self._state.salt = message.obj.server_salt', 'async def _handle_ack(self, message):\\n        \"\"\"\\n        Handles a server acknowledge about our messages. Normally\\n        these can be ignored except in the case of ``auth.logOut``:\\n\\n            auth.logOut#5717da40 = Bool;\\n\\n        Telegram doesn\\'t seem to send its result so we need to confirm\\n        it manually. No other request is known to have this behaviour.\\n\\n        Since the ID of sent messages consisting of a container is\\n        never returned (unless on a bad notification), this method\\n        also removes containers messages when any of their inner\\n        messages are acknowledged.\\n        \"\"\"\\n        ack = message.obj\\n        self._log.debug(\\'Handling acknowledge for %s\\', str(ack.msg_ids))\\n        for msg_id in ack.msg_ids:\\n            state = self._pending_state.get(msg_id)\\n            if state and isinstance(state.request, LogOutRequest):\\n                del self._pending_state[msg_id]\\n                state.future.set_result(True)', 'async def _handle_future_salts(self, message):\\n        \"\"\"\\n        Handles future salt results, which don\\'t come inside a\\n        ``rpc_result`` but are still sent through a request:\\n\\n            future_salts#ae500895 req_msg_id:long now:int\\n            salts:vector<future_salt> = FutureSalts;\\n        \"\"\"\\n        # TODO save these salts and automatically adjust to the\\n        # correct one whenever the salt in use expires.\\n        self._log.debug(\\'Handling future salts for message %d\\', message.msg_id)\\n        state = self._pending_state.pop(message.msg_id, None)\\n        if state:\\n            state.future.set_result(message.obj)', 'async def _handle_state_forgotten(self, message):\\n        \"\"\"\\n        Handles both :tl:`MsgsStateReq` and :tl:`MsgResendReq` by\\n        enqueuing a :tl:`MsgsStateInfo` to be sent at a later point.\\n        \"\"\"\\n        self._send_queue.append(RequestState(MsgsStateInfo(\\n            req_msg_id=message.msg_id, info=chr(1) * len(message.obj.msg_ids)),\\n            loop=self._loop))', 'async def article(\\n            self, title, description=None,\\n            *, url=None, thumb=None, content=None,\\n            id=None, text=None, parse_mode=(), link_preview=True,\\n            geo=None, period=60, contact=None, game=False, buttons=None\\n    ):\\n        \"\"\"\\n        Creates new inline result of article type.\\n\\n        Args:\\n            title (`str`):\\n                The title to be shown for this result.\\n\\n            description (`str`, optional):\\n                Further explanation of what this result means.\\n\\n            url (`str`, optional):\\n                The URL to be shown for this result.\\n\\n            thumb (:tl:`InputWebDocument`, optional):\\n                The thumbnail to be shown for this result.\\n                For now it has to be a :tl:`InputWebDocument` if present.\\n\\n            content (:tl:`InputWebDocument`, optional):\\n                The content to be shown for this result.\\n                For now it has to be a :tl:`InputWebDocument` if present.\\n        \"\"\"\\n        # TODO Does \\'article\\' work always?\\n        # article, photo, gif, mpeg4_gif, video, audio,\\n        # voice, document, location, venue, contact, game\\n        result = types.InputBotInlineResult(\\n            id=id or \\'\\',\\n            type=\\'article\\',\\n            send_message=await self._message(\\n                text=text, parse_mode=parse_mode, link_preview=link_preview,\\n                geo=geo, period=period,\\n                contact=contact,\\n                game=game,\\n                buttons=buttons\\n            ),\\n            title=title,\\n            description=description,\\n            url=url,\\n            thumb=thumb,\\n            content=content\\n        )\\n        if id is None:\\n            result.id = hashlib.sha256(bytes(result)).hexdigest()\\n\\n        return result', 'async def photo(\\n            self, file, *, id=None,\\n            text=None, parse_mode=(), link_preview=True,\\n            geo=None, period=60, contact=None, game=False, buttons=None\\n    ):\\n        \"\"\"\\n        Creates a new inline result of photo type.\\n\\n        Args:\\n            file (`obj`, optional):\\n                Same as ``file`` for `client.send_file\\n                <telethon.client.uploads.UploadMethods.send_file>`.\\n        \"\"\"\\n        try:\\n            fh = utils.get_input_photo(file)\\n        except TypeError:\\n            fh = await self._client.upload_file(file, use_cache=types.InputPhoto)\\n\\n        if not isinstance(fh, types.InputPhoto):\\n            r = await self._client(functions.messages.UploadMediaRequest(\\n                types.InputPeerSelf(), media=types.InputMediaUploadedPhoto(fh)\\n            ))\\n            fh = utils.get_input_photo(r.photo)\\n\\n        result = types.InputBotInlineResultPhoto(\\n            id=id or \\'\\',\\n            type=\\'photo\\',\\n            photo=fh,\\n            send_message=await self._message(\\n                text=text or \\'\\',\\n                parse_mode=parse_mode,\\n                link_preview=link_preview,\\n                geo=geo,\\n                period=period,\\n                contact=contact,\\n                game=game,\\n                buttons=buttons\\n            )\\n        )\\n        if id is None:\\n            result.id = hashlib.sha256(bytes(result)).hexdigest()\\n\\n        return result', 'async def document(\\n            self, file, title=None, *, description=None, type=None,\\n            mime_type=None, attributes=None, force_document=False,\\n            voice_note=False, video_note=False, use_cache=True, id=None,\\n            text=None, parse_mode=(), link_preview=True,\\n            geo=None, period=60, contact=None, game=False, buttons=None\\n    ):\\n        \"\"\"\\n        Creates a new inline result of document type.\\n\\n        `use_cache`, `mime_type`, `attributes`, `force_document`,\\n        `voice_note` and `video_note` are described in `client.send_file\\n        <telethon.client.uploads.UploadMethods.send_file>`.\\n\\n        Args:\\n            file (`obj`):\\n                Same as ``file`` for `client.send_file\\n                <telethon.client.uploads.UploadMethods.send_file>`.\\n\\n            title (`str`, optional):\\n                The title to be shown for this result.\\n\\n            description (`str`, optional):\\n                Further explanation of what this result means.\\n\\n            type (`str`, optional):\\n                The type of the document. May be one of: photo, gif,\\n                mpeg4_gif, video, audio, voice, document, sticker.\\n\\n                See \"Type of the result\" in https://core.telegram.org/bots/api.\\n        \"\"\"\\n        if type is None:\\n            if voice_note:\\n                type = \\'voice\\'\\n            else:\\n                type = \\'document\\'\\n\\n        try:\\n            fh = utils.get_input_document(file)\\n        except TypeError:\\n            use_cache = types.InputDocument if use_cache else None\\n            fh = await self._client.upload_file(file, use_cache=use_cache)\\n\\n        if not isinstance(fh, types.InputDocument):\\n            attributes, mime_type = utils.get_attributes(\\n                file,\\n                mime_type=mime_type,\\n                attributes=attributes,\\n                force_document=force_document,\\n                voice_note=voice_note,\\n                video_note=video_note\\n            )\\n            r = await self._client(functions.messages.UploadMediaRequest(\\n                types.InputPeerSelf(), media=types.InputMediaUploadedDocument(\\n                    fh,\\n                    mime_type=mime_type,\\n                    attributes=attributes,\\n                    nosound_video=None,\\n                    thumb=None\\n            )))\\n            fh = utils.get_input_document(r.document)\\n\\n        result = types.InputBotInlineResultDocument(\\n            id=id or \\'\\',\\n            type=type,\\n            document=fh,\\n            send_message=await self._message(\\n                # Empty string for text if there\\'s media but text is None.\\n                # We may want to display a document but send text; however\\n                # default to sending the media (without text, i.e. stickers).\\n                text=text or \\'\\',\\n                parse_mode=parse_mode,\\n                link_preview=link_preview,\\n                geo=geo,\\n                period=period,\\n                contact=contact,\\n                game=game,\\n                buttons=buttons\\n            ),\\n            title=title,\\n            description=description\\n        )\\n        if id is None:\\n            result.id = hashlib.sha256(bytes(result)).hexdigest()\\n\\n        return result', 'async def game(\\n            self, short_name, *, id=None,\\n            text=None, parse_mode=(), link_preview=True,\\n            geo=None, period=60, contact=None, game=False, buttons=None\\n    ):\\n        \"\"\"\\n        Creates a new inline result of game type.\\n\\n        Args:\\n            short_name (`str`):\\n                The short name of the game to use.\\n        \"\"\"\\n        result = types.InputBotInlineResultGame(\\n            id=id or \\'\\',\\n            short_name=short_name,\\n            send_message=await self._message(\\n                text=text, parse_mode=parse_mode, link_preview=link_preview,\\n                geo=geo, period=period,\\n                contact=contact,\\n                game=game,\\n                buttons=buttons\\n            )\\n        )\\n        if id is None:\\n            result.id = hashlib.sha256(bytes(result)).hexdigest()\\n\\n        return result', 'async def send_message(self, *args, **kwargs):\\n        \"\"\"\\n        Sends a message to this dialog. This is just a wrapper around\\n        ``client.send_message(dialog.input_entity, *args, **kwargs)``.\\n        \"\"\"\\n        return await self._client.send_message(\\n            self.input_entity, *args, **kwargs)', 'async def delete(self):\\n        \"\"\"\\n        Deletes the dialog from your dialog list. If you own the\\n        channel this won\\'t destroy it, only delete it from the list.\\n        \"\"\"\\n        if self.is_channel:\\n            await self._client(functions.channels.LeaveChannelRequest(\\n                self.input_entity))\\n        else:\\n            if self.is_group:\\n                await self._client(functions.messages.DeleteChatUserRequest(\\n                    self.entity.id, types.InputPeerSelf()))\\n            await self._client(functions.messages.DeleteHistoryRequest(\\n                self.input_entity, 0))', 'def read_int(self, signed=True):\\n        \"\"\"Reads an integer (4 bytes) value.\"\"\"\\n        return int.from_bytes(self.read(4), byteorder=\\'little\\', signed=signed)', 'def read_long(self, signed=True):\\n        \"\"\"Reads a long integer (8 bytes) value.\"\"\"\\n        return int.from_bytes(self.read(8), byteorder=\\'little\\', signed=signed)', 'def read_large_int(self, bits, signed=True):\\n        \"\"\"Reads a n-bits long integer value.\"\"\"\\n        return int.from_bytes(\\n            self.read(bits // 8), byteorder=\\'little\\', signed=signed)', 'def read(self, length=None):\\n        \"\"\"Read the given amount of bytes.\"\"\"\\n        if length is None:\\n            return self.reader.read()\\n\\n        result = self.reader.read(length)\\n        if len(result) != length:\\n            raise BufferError(\\n                \\'No more data left to read (need {}, got {}: {}); last read {}\\'\\n                .format(length, len(result), repr(result), repr(self._last))\\n            )\\n\\n        self._last = result\\n        return result', 'def tgread_bytes(self):\\n        \"\"\"\\n        Reads a Telegram-encoded byte array, without the need of\\n        specifying its length.\\n        \"\"\"\\n        first_byte = self.read_byte()\\n        if first_byte == 254:\\n            length = self.read_byte() | (self.read_byte() << 8) | (\\n                self.read_byte() << 16)\\n            padding = length % 4\\n        else:\\n            length = first_byte\\n            padding = (length + 1) % 4\\n\\n        data = self.read(length)\\n        if padding > 0:\\n            padding = 4 - padding\\n            self.read(padding)\\n\\n        return data', 'def tgread_bool(self):\\n        \"\"\"Reads a Telegram boolean value.\"\"\"\\n        value = self.read_int(signed=False)\\n        if value == 0x997275b5:  # boolTrue\\n            return True\\n        elif value == 0xbc799737:  # boolFalse\\n            return False\\n        else:\\n            raise RuntimeError(\\'Invalid boolean code {}\\'.format(hex(value)))', 'def tgread_date(self):\\n        \"\"\"Reads and converts Unix time (used by Telegram)\\n           into a Python datetime object.\\n        \"\"\"\\n        value = self.read_int()\\n        if value == 0:\\n            return None\\n        else:\\n            return datetime.fromtimestamp(value, tz=timezone.utc)', 'def tgread_object(self):\\n        \"\"\"Reads a Telegram object.\"\"\"\\n        constructor_id = self.read_int(signed=False)\\n        clazz = tlobjects.get(constructor_id, None)\\n        if clazz is None:\\n            # The class was None, but there\\'s still a\\n            # chance of it being a manually parsed value like bool!\\n            value = constructor_id\\n            if value == 0x997275b5:  # boolTrue\\n                return True\\n            elif value == 0xbc799737:  # boolFalse\\n                return False\\n            elif value == 0x1cb5c415:  # Vector\\n                return [self.tgread_object() for _ in range(self.read_int())]\\n\\n            clazz = core_objects.get(constructor_id, None)\\n            if clazz is None:\\n                # If there was still no luck, give up\\n                self.seek(-4)  # Go back\\n                pos = self.tell_position()\\n                error = TypeNotFoundError(constructor_id, self.read())\\n                self.set_position(pos)\\n                raise error\\n\\n        return clazz.from_reader(self)', 'def tgread_vector(self):\\n        \"\"\"Reads a vector (a list) of Telegram objects.\"\"\"\\n        if 0x1cb5c415 != self.read_int(signed=False):\\n            raise RuntimeError(\\'Invalid constructor code, vector was expected\\')\\n\\n        count = self.read_int()\\n        return [self.tgread_object() for _ in range(count)]', 'def factorize(cls, pq):\\n        \"\"\"\\n        Factorizes the given large integer.\\n\\n        :param pq: the prime pair pq.\\n        :return: a tuple containing the two factors p and q.\\n        \"\"\"\\n        if pq % 2 == 0:\\n            return 2, pq // 2\\n\\n        y, c, m = randint(1, pq - 1), randint(1, pq - 1), randint(1, pq - 1)\\n        g = r = q = 1\\n        x = ys = 0\\n\\n        while g == 1:\\n            x = y\\n            for i in range(r):\\n                y = (pow(y, 2, pq) + c) % pq\\n\\n            k = 0\\n            while k < r and g == 1:\\n                ys = y\\n                for i in range(min(m, r - k)):\\n                    y = (pow(y, 2, pq) + c) % pq\\n                    q = q * (abs(x - y)) % pq\\n\\n                g = cls.gcd(q, pq)\\n                k += m\\n\\n            r *= 2\\n\\n        if g == pq:\\n            while True:\\n                ys = (pow(ys, 2, pq) + c) % pq\\n                g = cls.gcd(abs(x - ys), pq)\\n                if g > 1:\\n                    break\\n\\n        p, q = g, pq // g\\n        return (p, q) if p < q else (q, p)', 'def gcd(a, b):\\n        \"\"\"\\n        Calculates the Greatest Common Divisor.\\n\\n        :param a: the first number.\\n        :param b: the second number.\\n        :return: GCD(a, b)\\n        \"\"\"\\n        while b:\\n            a, b = b, a % b\\n\\n        return a', 'def takeout(\\n            self, finalize=True, *, contacts=None, users=None, chats=None,\\n            megagroups=None, channels=None, files=None, max_file_size=None):\\n        \"\"\"\\n        Creates a proxy object over the current :ref:`TelegramClient` through\\n        which making requests will use :tl:`InvokeWithTakeoutRequest` to wrap\\n        them. In other words, returns the current client modified so that\\n        requests are done as a takeout:\\n\\n        >>> from telethon.sync import TelegramClient\\n        >>>\\n        >>> with TelegramClient(...) as client:\\n        >>>     with client.takeout() as takeout:\\n        >>>         client.get_messages(\\'me\\')  # normal call\\n        >>>         takeout.get_messages(\\'me\\')  # wrapped through takeout\\n\\n        Some of the calls made through the takeout session will have lower\\n        flood limits. This is useful if you want to export the data from\\n        conversations or mass-download media, since the rate limits will\\n        be lower. Only some requests will be affected, and you will need\\n        to adjust the `wait_time` of methods like `client.iter_messages\\n        <telethon.client.messages.MessageMethods.iter_messages>`.\\n\\n        By default, all parameters are ``None``, and you need to enable those\\n        you plan to use by setting them to either ``True`` or ``False``.\\n\\n        You should ``except errors.TakeoutInitDelayError as e``, since this\\n        exception will raise depending on the condition of the session. You\\n        can then access ``e.seconds`` to know how long you should wait for\\n        before calling the method again.\\n\\n        There\\'s also a `success` property available in the takeout proxy\\n        object, so from the `with` body you can set the boolean result that\\n        will be sent back to Telegram. But if it\\'s left ``None`` as by\\n        default, then the action is based on the `finalize` parameter. If\\n        it\\'s ``True`` then the takeout will be finished, and if no exception\\n        occurred during it, then ``True`` will be considered as a result.\\n        Otherwise, the takeout will not be finished and its ID will be\\n        preserved for future usage as `client.session.takeout_id\\n        <telethon.sessions.abstract.Session.takeout_id>`.\\n\\n        Args:\\n            contacts (`bool`):\\n                Set to ``True`` if you plan on downloading contacts.\\n\\n            users (`bool`):\\n                Set to ``True`` if you plan on downloading information\\n                from users and their private conversations with you.\\n\\n            chats (`bool`):\\n                Set to ``True`` if you plan on downloading information\\n                from small group chats, such as messages and media.\\n\\n            megagroups (`bool`):\\n                Set to ``True`` if you plan on downloading information\\n                from megagroups (channels), such as messages and media.\\n\\n            channels (`bool`):\\n                Set to ``True`` if you plan on downloading information\\n                from broadcast channels, such as messages and media.\\n\\n            files (`bool`):\\n                Set to ``True`` if you plan on downloading media and\\n                you don\\'t only wish to export messages.\\n\\n            max_file_size (`int`):\\n                The maximum file size, in bytes, that you plan\\n                to download for each message with media.\\n        \"\"\"\\n        request_kwargs = dict(\\n            contacts=contacts,\\n            message_users=users,\\n            message_chats=chats,\\n            message_megagroups=megagroups,\\n            message_channels=channels,\\n            files=files,\\n            file_max_size=max_file_size\\n        )\\n        arg_specified = (arg is not None for arg in request_kwargs.values())\\n\\n        if self.session.takeout_id is None or any(arg_specified):\\n            request = functions.account.InitTakeoutSessionRequest(\\n                **request_kwargs)\\n        else:\\n            request = None\\n\\n        return _TakeoutClient(finalize, self, request)', 'async def end_takeout(self, success):\\n        \"\"\"\\n        Finishes a takeout, with specified result sent back to Telegram.\\n\\n        Returns:\\n            ``True`` if the operation was successful, ``False`` otherwise.\\n        \"\"\"\\n        try:\\n            async with _TakeoutClient(True, self, None) as takeout:\\n                takeout.success = success\\n        except ValueError:\\n            return False\\n        return True', 'def syncify(*types):\\n    \"\"\"\\n    Converts all the methods in the given types (class definitions)\\n    into synchronous, which return either the coroutine or the result\\n    based on whether ``asyncio\\'s`` event loop is running.\\n    \"\"\"\\n    # Our asynchronous generators all are `RequestIter`, which already\\n    # provide a synchronous iterator variant, so we don\\'t need to worry\\n    # about asyncgenfunction\\'s here.\\n    for t in types:\\n        for name in dir(t):\\n            if not name.startswith(\\'_\\') or name == \\'__call__\\':\\n                if inspect.iscoroutinefunction(getattr(t, name)):\\n                    _syncify_wrap(t, name)', 'async def get(self):\\n        \"\"\"\\n        Returns (batch, data) if one or more items could be retrieved.\\n\\n        If the cancellation occurs or only invalid items were in the\\n        queue, (None, None) will be returned instead.\\n        \"\"\"\\n        if not self._deque:\\n            self._ready.clear()\\n            await self._ready.wait()\\n\\n        buffer = io.BytesIO()\\n        batch = []\\n        size = 0\\n\\n        # Fill a new batch to return while the size is small enough,\\n        # as long as we don\\'t exceed the maximum length of messages.\\n        while self._deque and len(batch) <= MessageContainer.MAXIMUM_LENGTH:\\n            state = self._deque.popleft()\\n            size += len(state.data) + TLMessage.SIZE_OVERHEAD\\n\\n            if size <= MessageContainer.MAXIMUM_SIZE:\\n                state.msg_id = self._state.write_data_as_message(\\n                    buffer, state.data, isinstance(state.request, TLRequest),\\n                    after_id=state.after.msg_id if state.after else None\\n                )\\n                batch.append(state)\\n                self._log.debug(\\'Assigned msg_id = %d to %s (%x)\\',\\n                                state.msg_id, state.request.__class__.__name__,\\n                                id(state.request))\\n                continue\\n\\n            if batch:\\n                # Put the item back since it can\\'t be sent in this batch\\n                self._deque.appendleft(state)\\n                break\\n\\n            # If a single message exceeds the maximum size, then the\\n            # message payload cannot be sent. Telegram would forcibly\\n            # close the connection; message would never be confirmed.\\n            #\\n            # We don\\'t put the item back because it can never be sent.\\n            # If we did, we would loop again and reach this same path.\\n            # Setting the exception twice results in `InvalidStateError`\\n            # and this method should never return with error, which we\\n            # really want to avoid.\\n            self._log.warning(\\n                \\'Message payload for %s is too long (%d) and cannot be sent\\',\\n                state.request.__class__.__name__, len(state.data)\\n            )\\n            state.future.set_exception(\\n                ValueError(\\'Request payload is too big\\'))\\n\\n            size = 0\\n            continue\\n\\n        if not batch:\\n            return None, None\\n\\n        if len(batch) > 1:\\n            # Inlined code to pack several messages into a container\\n            data = struct.pack(\\n                \\'<Ii\\', MessageContainer.CONSTRUCTOR_ID, len(batch)\\n            ) + buffer.getvalue()\\n            buffer = io.BytesIO()\\n            container_id = self._state.write_data_as_message(\\n                buffer, data, content_related=False\\n            )\\n            for s in batch:\\n                s.container_id = container_id\\n\\n        data = buffer.getvalue()\\n        return batch, data', 'async def connect(self, timeout=None, ssl=None):\\n        \"\"\"\\n        Establishes a connection with the server.\\n        \"\"\"\\n        await self._connect(timeout=timeout, ssl=ssl)\\n        self._connected = True\\n\\n        self._send_task = self._loop.create_task(self._send_loop())\\n        self._recv_task = self._loop.create_task(self._recv_loop())', 'async def disconnect(self):\\n        \"\"\"\\n        Disconnects from the server, and clears\\n        pending outgoing and incoming messages.\\n        \"\"\"\\n        self._connected = False\\n\\n        await helpers._cancel(\\n            self._log,\\n            send_task=self._send_task,\\n            recv_task=self._recv_task\\n        )\\n\\n        if self._writer:\\n            self._writer.close()\\n            if sys.version_info >= (3, 7):\\n                await self._writer.wait_closed()', 'def send(self, data):\\n        \"\"\"\\n        Sends a packet of data through this connection mode.\\n\\n        This method returns a coroutine.\\n        \"\"\"\\n        if not self._connected:\\n            raise ConnectionError(\\'Not connected\\')\\n\\n        return self._send_queue.put(data)', 'async def recv(self):\\n        \"\"\"\\n        Receives a packet of data through this connection mode.\\n\\n        This method returns a coroutine.\\n        \"\"\"\\n        while self._connected:\\n            result = await self._recv_queue.get()\\n            if result:  # None = sentinel value = keep trying\\n                return result\\n\\n        raise ConnectionError(\\'Not connected\\')', 'async def _send_loop(self):\\n        \"\"\"\\n        This loop is constantly popping items off the queue to send them.\\n        \"\"\"\\n        try:\\n            while self._connected:\\n                self._send(await self._send_queue.get())\\n                await self._writer.drain()\\n        except asyncio.CancelledError:\\n            pass\\n        except Exception as e:\\n            if isinstance(e, IOError):\\n                self._log.info(\\'The server closed the connection while sending\\')\\n            else:\\n                self._log.exception(\\'Unexpected exception in the send loop\\')\\n\\n            await self.disconnect()', 'async def _recv_loop(self):\\n        \"\"\"\\n        This loop is constantly putting items on the queue as they\\'re read.\\n        \"\"\"\\n        while self._connected:\\n            try:\\n                data = await self._recv()\\n            except asyncio.CancelledError:\\n                break\\n            except Exception as e:\\n                if isinstance(e, (IOError, asyncio.IncompleteReadError)):\\n                    msg = \\'The server closed the connection\\'\\n                    self._log.info(msg)\\n                elif isinstance(e, InvalidChecksumError):\\n                    msg = \\'The server response had an invalid checksum\\'\\n                    self._log.info(msg)\\n                else:\\n                    msg = \\'Unexpected exception in the receive loop\\'\\n                    self._log.exception(msg)\\n\\n                await self.disconnect()\\n\\n                # Add a sentinel value to unstuck recv\\n                if self._recv_queue.empty():\\n                    self._recv_queue.put_nowait(None)\\n\\n                break\\n\\n            try:\\n                await self._recv_queue.put(data)\\n            except asyncio.CancelledError:\\n                break', 'def _init_conn(self):\\n        \"\"\"\\n        This method will be called after `connect` is called.\\n        After this method finishes, the writer will be drained.\\n\\n        Subclasses should make use of this if they need to send\\n        data to Telegram to indicate which connection mode will\\n        be used.\\n        \"\"\"\\n        if self._codec.tag:\\n            self._writer.write(self._codec.tag)', 'def gzip_if_smaller(content_related, data):\\n        \"\"\"Calls bytes(request), and based on a certain threshold,\\n           optionally gzips the resulting data. If the gzipped data is\\n           smaller than the original byte array, this is returned instead.\\n\\n           Note that this only applies to content related requests.\\n        \"\"\"\\n        if content_related and len(data) > 512:\\n            gzipped = bytes(GzipPacked(data))\\n            return gzipped if len(gzipped) < len(data) else data\\n        else:\\n            return data', 'def sorted_args(self):\\n        \"\"\"Returns the arguments properly sorted and ready to plug-in\\n           into a Python\\'s method header (i.e., flags and those which\\n           can be inferred will go last so they can default =None)\\n        \"\"\"\\n        return sorted(self.args,\\n                      key=lambda x: x.is_flag or x.can_be_inferred)', 'def _cursor(self):\\n        \"\"\"Asserts that the connection is open and returns a cursor\"\"\"\\n        if self._conn is None:\\n            self._conn = sqlite3.connect(self.filename,\\n                                         check_same_thread=False)\\n        return self._conn.cursor()', 'def _execute(self, stmt, *values):\\n        \"\"\"\\n        Gets a cursor, executes `stmt` and closes the cursor,\\n        fetching one row afterwards and returning its result.\\n        \"\"\"\\n        c = self._cursor()\\n        try:\\n            return c.execute(stmt, values).fetchone()\\n        finally:\\n            c.close()', 'def close(self):\\n        \"\"\"Closes the connection unless we\\'re working in-memory\"\"\"\\n        if self.filename != \\':memory:\\':\\n            if self._conn is not None:\\n                self._conn.commit()\\n                self._conn.close()\\n                self._conn = None', 'def delete(self):\\n        \"\"\"Deletes the current session file\"\"\"\\n        if self.filename == \\':memory:\\':\\n            return True\\n        try:\\n            os.remove(self.filename)\\n            return True\\n        except OSError:\\n            return False', 'def list_sessions(cls):\\n        \"\"\"Lists all the sessions of the users who have ever connected\\n           using this client and never logged out\\n        \"\"\"\\n        return [os.path.splitext(os.path.basename(f))[0]\\n                for f in os.listdir(\\'.\\') if f.endswith(EXTENSION)]', 'def process_entities(self, tlo):\\n        \"\"\"Processes all the found entities on the given TLObject,\\n           unless .enabled is False.\\n\\n           Returns True if new input entities were added.\\n        \"\"\"\\n        if not self.save_entities:\\n            return\\n\\n        rows = self._entities_to_rows(tlo)\\n        if not rows:\\n            return\\n\\n        c = self._cursor()\\n        try:\\n            c.executemany(\\n                \\'insert or replace into entities values (?,?,?,?,?)\\', rows)\\n        finally:\\n            c.close()', 'def parse_methods(csv_file, errors_dict):\\n    \"\"\"\\n    Parses the input CSV file with columns (method, usability, errors)\\n    and yields `MethodInfo` instances as a result.\\n    \"\"\"\\n    with csv_file.open(newline=\\'\\') as f:\\n        f = csv.reader(f)\\n        next(f, None)  # header\\n        for line, (method, usability, errors) in enumerate(f, start=2):\\n            try:\\n                errors = [errors_dict[x] for x in errors.split()]\\n            except KeyError:\\n                raise ValueError(\\'Method {} references unknown errors {}\\'\\n                                 .format(method, errors)) from None\\n\\n            yield MethodInfo(method, usability, errors)', 'async def connect(self):\\n        \"\"\"\\n        Connects to Telegram.\\n        \"\"\"\\n        await self._sender.connect(self._connection(\\n            self.session.server_address,\\n            self.session.port,\\n            self.session.dc_id,\\n            loop=self._loop,\\n            loggers=self._log,\\n            proxy=self._proxy\\n        ))\\n        self.session.auth_key = self._sender.auth_key\\n        self.session.save()\\n\\n        await self._sender.send(self._init_with(\\n            functions.help.GetConfigRequest()))\\n\\n        self._updates_handle = self._loop.create_task(self._update_loop())', 'def disconnect(self):\\n        \"\"\"\\n        Disconnects from Telegram.\\n\\n        If the event loop is already running, this method returns a\\n        coroutine that you should await on your own code; otherwise\\n        the loop is ran until said coroutine completes.\\n        \"\"\"\\n        if self._loop.is_running():\\n            return self._disconnect_coro()\\n        else:\\n            self._loop.run_until_complete(self._disconnect_coro())', 'async def _disconnect(self):\\n        \"\"\"\\n        Disconnect only, without closing the session. Used in reconnections\\n        to different data centers, where we don\\'t want to close the session\\n        file; user disconnects however should close it since it means that\\n        their job with the client is complete and we should clean it up all.\\n        \"\"\"\\n        await self._sender.disconnect()\\n        await helpers._cancel(self._log[__name__],\\n                              updates_handle=self._updates_handle)', 'async def _switch_dc(self, new_dc):\\n        \"\"\"\\n        Permanently switches the current connection to the new data center.\\n        \"\"\"\\n        self._log[__name__].info(\\'Reconnecting to new data center %s\\', new_dc)\\n        dc = await self._get_dc(new_dc)\\n\\n        self.session.set_dc(dc.id, dc.ip_address, dc.port)\\n        # auth_key\\'s are associated with a server, which has now changed\\n        # so it\\'s not valid anymore. Set to None to force recreating it.\\n        self._sender.auth_key.key = None\\n        self.session.auth_key = None\\n        self.session.save()\\n        await self._disconnect()\\n        return await self.connect()', 'def _auth_key_callback(self, auth_key):\\n        \"\"\"\\n        Callback from the sender whenever it needed to generate a\\n        new authorization key. This means we are not authorized.\\n        \"\"\"\\n        self.session.auth_key = auth_key\\n        self.session.save()', 'async def _get_dc(self, dc_id, cdn=False):\\n        \"\"\"Gets the Data Center (DC) associated to \\'dc_id\\'\"\"\"\\n        cls = self.__class__\\n        if not cls._config:\\n            cls._config = await self(functions.help.GetConfigRequest())\\n\\n        if cdn and not self._cdn_config:\\n            cls._cdn_config = await self(functions.help.GetCdnConfigRequest())\\n            for pk in cls._cdn_config.public_keys:\\n                rsa.add_key(pk.public_key)\\n\\n        return next(\\n            dc for dc in cls._config.dc_options\\n            if dc.id == dc_id\\n            and bool(dc.ipv6) == self._use_ipv6 and bool(dc.cdn) == cdn\\n        )', 'async def _create_exported_sender(self, dc_id):\\n        \"\"\"\\n        Creates a new exported `MTProtoSender` for the given `dc_id` and\\n        returns it. This method should be used by `_borrow_exported_sender`.\\n        \"\"\"\\n        # Thanks badoualy/kotlogram on /telegram/api/DefaultTelegramClient.kt\\n        # for clearly showing how to export the authorization\\n        dc = await self._get_dc(dc_id)\\n        # Can\\'t reuse self._sender._connection as it has its own seqno.\\n        #\\n        # If one were to do that, Telegram would reset the connection\\n        # with no further clues.\\n        sender = MTProtoSender(None, self._loop, loggers=self._log)\\n        await sender.connect(self._connection(\\n            dc.ip_address,\\n            dc.port,\\n            dc.id,\\n            loop=self._loop,\\n            loggers=self._log,\\n            proxy=self._proxy\\n        ))\\n        self._log[__name__].info(\\'Exporting authorization for data center %s\\',\\n                                 dc)\\n        auth = await self(functions.auth.ExportAuthorizationRequest(dc_id))\\n        req = self._init_with(functions.auth.ImportAuthorizationRequest(\\n            id=auth.id, bytes=auth.bytes\\n        ))\\n        await sender.send(req)\\n        return sender', 'async def _borrow_exported_sender(self, dc_id):\\n        \"\"\"\\n        Borrows a connected `MTProtoSender` for the given `dc_id`.\\n        If it\\'s not cached, creates a new one if it doesn\\'t exist yet,\\n        and imports a freshly exported authorization key for it to be usable.\\n\\n        Once its job is over it should be `_return_exported_sender`.\\n        \"\"\"\\n        async with self._borrow_sender_lock:\\n            n, sender = self._borrowed_senders.get(dc_id, (0, None))\\n            if not sender:\\n                sender = await self._create_exported_sender(dc_id)\\n                sender.dc_id = dc_id\\n            elif not n:\\n                dc = await self._get_dc(dc_id)\\n                await sender.connect(self._connection(\\n                    dc.ip_address,\\n                    dc.port,\\n                    dc.id,\\n                    loop=self._loop,\\n                    loggers=self._log,\\n                    proxy=self._proxy\\n                ))\\n\\n            self._borrowed_senders[dc_id] = (n + 1, sender)\\n\\n        return sender', 'async def _return_exported_sender(self, sender):\\n        \"\"\"\\n        Returns a borrowed exported sender. If all borrows have\\n        been returned, the sender is cleanly disconnected.\\n        \"\"\"\\n        async with self._borrow_sender_lock:\\n            dc_id = sender.dc_id\\n            n, _ = self._borrowed_senders[dc_id]\\n            n -= 1\\n            self._borrowed_senders[dc_id] = (n, sender)\\n            if not n:\\n                self._log[__name__].info(\\n                    \\'Disconnecting borrowed sender for DC %d\\', dc_id)\\n                await sender.disconnect()', 'async def _get_cdn_client(self, cdn_redirect):\\n        \"\"\"Similar to ._borrow_exported_client, but for CDNs\"\"\"\\n        # TODO Implement\\n        raise NotImplementedError\\n        session = self._exported_sessions.get(cdn_redirect.dc_id)\\n        if not session:\\n            dc = await self._get_dc(cdn_redirect.dc_id, cdn=True)\\n            session = self.session.clone()\\n            await session.set_dc(dc.id, dc.ip_address, dc.port)\\n            self._exported_sessions[cdn_redirect.dc_id] = session\\n\\n        self._log[__name__].info(\\'Creating new CDN client\\')\\n        client = TelegramBareClient(\\n            session, self.api_id, self.api_hash,\\n            proxy=self._sender.connection.conn.proxy,\\n            timeout=self._sender.connection.get_timeout()\\n        )\\n\\n        # This will make use of the new RSA keys for this specific CDN.\\n        #\\n        # We won\\'t be calling GetConfigRequest because it\\'s only called\\n        # when needed by ._get_dc, and also it\\'s static so it\\'s likely\\n        # set already. Avoid invoking non-CDN methods by not syncing updates.\\n        client.connect(_sync_updates=False)\\n        return client', 'def chunks(iterable, size=100):\\n    \"\"\"\\n    Turns the given iterable into chunks of the specified size,\\n    which is 100 by default since that\\'s what Telegram uses the most.\\n    \"\"\"\\n    it = iter(iterable)\\n    size -= 1\\n    for head in it:\\n        yield itertools.chain([head], itertools.islice(it, size))', 'def get_display_name(entity):\\n    \"\"\"\\n    Gets the display name for the given entity, if it\\'s an :tl:`User`,\\n    :tl:`Chat` or :tl:`Channel`. Returns an empty string otherwise.\\n    \"\"\"\\n    if isinstance(entity, types.User):\\n        if entity.last_name and entity.first_name:\\n            return \\'{} {}\\'.format(entity.first_name, entity.last_name)\\n        elif entity.first_name:\\n            return entity.first_name\\n        elif entity.last_name:\\n            return entity.last_name\\n        else:\\n            return \\'\\'\\n\\n    elif isinstance(entity, (types.Chat, types.Channel)):\\n        return entity.title\\n\\n    return \\'\\'', 'def get_extension(media):\\n    \"\"\"Gets the corresponding extension for any Telegram media.\"\"\"\\n\\n    # Photos are always compressed as .jpg by Telegram\\n    if isinstance(media, (types.UserProfilePhoto,\\n                          types.ChatPhoto, types.MessageMediaPhoto)):\\n        return \\'.jpg\\'\\n\\n    # Documents will come with a mime type\\n    if isinstance(media, types.MessageMediaDocument):\\n        media = media.document\\n    if isinstance(media, (\\n            types.Document, types.WebDocument, types.WebDocumentNoProxy)):\\n        if media.mime_type == \\'application/octet-stream\\':\\n            # Octet stream are just bytes, which have no default extension\\n            return \\'\\'\\n        else:\\n            return guess_extension(media.mime_type) or \\'\\'\\n\\n    return \\'\\'', 'def get_input_peer(entity, allow_self=True, check_hash=True):\\n    \"\"\"\\n    Gets the input peer for the given \"entity\" (user, chat or channel).\\n\\n    A ``TypeError`` is raised if the given entity isn\\'t a supported type\\n    or if ``check_hash is True`` but the entity\\'s ``access_hash is None``.\\n\\n    Note that ``check_hash`` **is ignored** if an input peer is already\\n    passed since in that case we assume the user knows what they\\'re doing.\\n    This is key to getting entities by explicitly passing ``hash = 0``.\\n    \"\"\"\\n    try:\\n        if entity.SUBCLASS_OF_ID == 0xc91c90b6:  # crc32(b\\'InputPeer\\')\\n            return entity\\n    except AttributeError:\\n        # e.g. custom.Dialog (can\\'t cyclic import).\\n        if allow_self and hasattr(entity, \\'input_entity\\'):\\n            return entity.input_entity\\n        elif hasattr(entity, \\'entity\\'):\\n            return get_input_peer(entity.entity)\\n        else:\\n            _raise_cast_fail(entity, \\'InputPeer\\')\\n\\n    if isinstance(entity, types.User):\\n        if entity.is_self and allow_self:\\n            return types.InputPeerSelf()\\n        elif entity.access_hash is not None or not check_hash:\\n            return types.InputPeerUser(entity.id, entity.access_hash)\\n        else:\\n            raise TypeError(\\'User without access_hash cannot be input\\')\\n\\n    if isinstance(entity, (types.Chat, types.ChatEmpty, types.ChatForbidden)):\\n        return types.InputPeerChat(entity.id)\\n\\n    if isinstance(entity, (types.Channel, types.ChannelForbidden)):\\n        if entity.access_hash is not None or not check_hash:\\n            return types.InputPeerChannel(entity.id, entity.access_hash)\\n        else:\\n            raise TypeError(\\'Channel without access_hash cannot be input\\')\\n\\n    if isinstance(entity, types.InputUser):\\n        return types.InputPeerUser(entity.user_id, entity.access_hash)\\n\\n    if isinstance(entity, types.InputChannel):\\n        return types.InputPeerChannel(entity.channel_id, entity.access_hash)\\n\\n    if isinstance(entity, types.InputUserSelf):\\n        return types.InputPeerSelf()\\n\\n    if isinstance(entity, types.UserEmpty):\\n        return types.InputPeerEmpty()\\n\\n    if isinstance(entity, types.UserFull):\\n        return get_input_peer(entity.user)\\n\\n    if isinstance(entity, types.ChatFull):\\n        return types.InputPeerChat(entity.id)\\n\\n    if isinstance(entity, types.PeerChat):\\n        return types.InputPeerChat(entity.chat_id)\\n\\n    _raise_cast_fail(entity, \\'InputPeer\\')', 'def get_input_channel(entity):\\n    \"\"\"Similar to :meth:`get_input_peer`, but for :tl:`InputChannel`\\'s alone.\"\"\"\\n    try:\\n        if entity.SUBCLASS_OF_ID == 0x40f202fd:  # crc32(b\\'InputChannel\\')\\n            return entity\\n    except AttributeError:\\n        _raise_cast_fail(entity, \\'InputChannel\\')\\n\\n    if isinstance(entity, (types.Channel, types.ChannelForbidden)):\\n        return types.InputChannel(entity.id, entity.access_hash or 0)\\n\\n    if isinstance(entity, types.InputPeerChannel):\\n        return types.InputChannel(entity.channel_id, entity.access_hash)\\n\\n    _raise_cast_fail(entity, \\'InputChannel\\')', 'def get_input_user(entity):\\n    \"\"\"Similar to :meth:`get_input_peer`, but for :tl:`InputUser`\\'s alone.\"\"\"\\n    try:\\n        if entity.SUBCLASS_OF_ID == 0xe669bf46:  # crc32(b\\'InputUser\\'):\\n            return entity\\n    except AttributeError:\\n        _raise_cast_fail(entity, \\'InputUser\\')\\n\\n    if isinstance(entity, types.User):\\n        if entity.is_self:\\n            return types.InputUserSelf()\\n        else:\\n            return types.InputUser(entity.id, entity.access_hash or 0)\\n\\n    if isinstance(entity, types.InputPeerSelf):\\n        return types.InputUserSelf()\\n\\n    if isinstance(entity, (types.UserEmpty, types.InputPeerEmpty)):\\n        return types.InputUserEmpty()\\n\\n    if isinstance(entity, types.UserFull):\\n        return get_input_user(entity.user)\\n\\n    if isinstance(entity, types.InputPeerUser):\\n        return types.InputUser(entity.user_id, entity.access_hash)\\n\\n    _raise_cast_fail(entity, \\'InputUser\\')', 'def get_input_dialog(dialog):\\n    \"\"\"Similar to :meth:`get_input_peer`, but for dialogs\"\"\"\\n    try:\\n        if dialog.SUBCLASS_OF_ID == 0xa21c9795:  # crc32(b\\'InputDialogPeer\\')\\n            return dialog\\n        if dialog.SUBCLASS_OF_ID == 0xc91c90b6:  # crc32(b\\'InputPeer\\')\\n            return types.InputDialogPeer(dialog)\\n    except AttributeError:\\n        _raise_cast_fail(dialog, \\'InputDialogPeer\\')\\n\\n    try:\\n        return types.InputDialogPeer(get_input_peer(dialog))\\n    except TypeError:\\n        pass\\n\\n    _raise_cast_fail(dialog, \\'InputDialogPeer\\')', 'def get_input_document(document):\\n    \"\"\"Similar to :meth:`get_input_peer`, but for documents\"\"\"\\n    try:\\n        if document.SUBCLASS_OF_ID == 0xf33fdb68:  # crc32(b\\'InputDocument\\'):\\n            return document\\n    except AttributeError:\\n        _raise_cast_fail(document, \\'InputDocument\\')\\n\\n    if isinstance(document, types.Document):\\n        return types.InputDocument(\\n            id=document.id, access_hash=document.access_hash,\\n            file_reference=document.file_reference)\\n\\n    if isinstance(document, types.DocumentEmpty):\\n        return types.InputDocumentEmpty()\\n\\n    if isinstance(document, types.MessageMediaDocument):\\n        return get_input_document(document.document)\\n\\n    if isinstance(document, types.Message):\\n        return get_input_document(document.media)\\n\\n    _raise_cast_fail(document, \\'InputDocument\\')', 'def get_input_photo(photo):\\n    \"\"\"Similar to :meth:`get_input_peer`, but for photos\"\"\"\\n    try:\\n        if photo.SUBCLASS_OF_ID == 0x846363e0:  # crc32(b\\'InputPhoto\\'):\\n            return photo\\n    except AttributeError:\\n        _raise_cast_fail(photo, \\'InputPhoto\\')\\n\\n    if isinstance(photo, types.photos.Photo):\\n        photo = photo.photo\\n\\n    if isinstance(photo, types.Photo):\\n        return types.InputPhoto(id=photo.id, access_hash=photo.access_hash,\\n                                file_reference=photo.file_reference)\\n\\n    if isinstance(photo, types.PhotoEmpty):\\n        return types.InputPhotoEmpty()\\n\\n    if isinstance(photo, types.messages.ChatFull):\\n        photo = photo.full_chat\\n    if isinstance(photo, types.ChannelFull):\\n        return get_input_photo(photo.chat_photo)\\n    elif isinstance(photo, types.UserFull):\\n        return get_input_photo(photo.profile_photo)\\n    elif isinstance(photo, (types.Channel, types.Chat, types.User)):\\n        return get_input_photo(photo.photo)\\n\\n    if isinstance(photo, (types.UserEmpty, types.ChatEmpty,\\n                          types.ChatForbidden, types.ChannelForbidden)):\\n        return types.InputPhotoEmpty()\\n\\n    _raise_cast_fail(photo, \\'InputPhoto\\')', 'def get_input_chat_photo(photo):\\n    \"\"\"Similar to :meth:`get_input_peer`, but for chat photos\"\"\"\\n    try:\\n        if photo.SUBCLASS_OF_ID == 0xd4eb2d74:  # crc32(b\\'InputChatPhoto\\')\\n            return photo\\n        elif photo.SUBCLASS_OF_ID == 0xe7655f1f:  # crc32(b\\'InputFile\\'):\\n            return types.InputChatUploadedPhoto(photo)\\n    except AttributeError:\\n        _raise_cast_fail(photo, \\'InputChatPhoto\\')\\n\\n    photo = get_input_photo(photo)\\n    if isinstance(photo, types.InputPhoto):\\n        return types.InputChatPhoto(photo)\\n    elif isinstance(photo, types.InputPhotoEmpty):\\n        return types.InputChatPhotoEmpty()\\n\\n    _raise_cast_fail(photo, \\'InputChatPhoto\\')', 'def get_input_geo(geo):\\n    \"\"\"Similar to :meth:`get_input_peer`, but for geo points\"\"\"\\n    try:\\n        if geo.SUBCLASS_OF_ID == 0x430d225:  # crc32(b\\'InputGeoPoint\\'):\\n            return geo\\n    except AttributeError:\\n        _raise_cast_fail(geo, \\'InputGeoPoint\\')\\n\\n    if isinstance(geo, types.GeoPoint):\\n        return types.InputGeoPoint(lat=geo.lat, long=geo.long)\\n\\n    if isinstance(geo, types.GeoPointEmpty):\\n        return types.InputGeoPointEmpty()\\n\\n    if isinstance(geo, types.MessageMediaGeo):\\n        return get_input_geo(geo.geo)\\n\\n    if isinstance(geo, types.Message):\\n        return get_input_geo(geo.media)\\n\\n    _raise_cast_fail(geo, \\'InputGeoPoint\\')', 'def get_input_media(\\n        media, *,\\n        is_photo=False, attributes=None, force_document=False,\\n        voice_note=False, video_note=False, supports_streaming=False\\n):\\n    \"\"\"\\n    Similar to :meth:`get_input_peer`, but for media.\\n\\n    If the media is :tl:`InputFile` and ``is_photo`` is known to be ``True``,\\n    it will be treated as an :tl:`InputMediaUploadedPhoto`. Else, the rest\\n    of parameters will indicate how to treat it.\\n    \"\"\"\\n    try:\\n        if media.SUBCLASS_OF_ID == 0xfaf846f4:  # crc32(b\\'InputMedia\\')\\n            return media\\n        elif media.SUBCLASS_OF_ID == 0x846363e0:  # crc32(b\\'InputPhoto\\')\\n            return types.InputMediaPhoto(media)\\n        elif media.SUBCLASS_OF_ID == 0xf33fdb68:  # crc32(b\\'InputDocument\\')\\n            return types.InputMediaDocument(media)\\n    except AttributeError:\\n        _raise_cast_fail(media, \\'InputMedia\\')\\n\\n    if isinstance(media, types.MessageMediaPhoto):\\n        return types.InputMediaPhoto(\\n            id=get_input_photo(media.photo),\\n            ttl_seconds=media.ttl_seconds\\n        )\\n\\n    if isinstance(media, (types.Photo, types.photos.Photo, types.PhotoEmpty)):\\n        return types.InputMediaPhoto(\\n            id=get_input_photo(media)\\n        )\\n\\n    if isinstance(media, types.MessageMediaDocument):\\n        return types.InputMediaDocument(\\n            id=get_input_document(media.document),\\n            ttl_seconds=media.ttl_seconds\\n        )\\n\\n    if isinstance(media, (types.Document, types.DocumentEmpty)):\\n        return types.InputMediaDocument(\\n            id=get_input_document(media)\\n        )\\n\\n    if isinstance(media, (types.InputFile, types.InputFileBig)):\\n        if is_photo:\\n            return types.InputMediaUploadedPhoto(file=media)\\n        else:\\n            attrs, mime = get_attributes(\\n                media,\\n                attributes=attributes,\\n                force_document=force_document,\\n                voice_note=voice_note,\\n                video_note=video_note,\\n                supports_streaming=supports_streaming\\n            )\\n            return types.InputMediaUploadedDocument(\\n                file=media, mime_type=mime, attributes=attrs)\\n\\n    if isinstance(media, types.MessageMediaGame):\\n        return types.InputMediaGame(id=media.game.id)\\n\\n    if isinstance(media, types.MessageMediaContact):\\n        return types.InputMediaContact(\\n            phone_number=media.phone_number,\\n            first_name=media.first_name,\\n            last_name=media.last_name,\\n            vcard=\\'\\'\\n        )\\n\\n    if isinstance(media, types.MessageMediaGeo):\\n        return types.InputMediaGeoPoint(geo_point=get_input_geo(media.geo))\\n\\n    if isinstance(media, types.MessageMediaVenue):\\n        return types.InputMediaVenue(\\n            geo_point=get_input_geo(media.geo),\\n            title=media.title,\\n            address=media.address,\\n            provider=media.provider,\\n            venue_id=media.venue_id,\\n            venue_type=\\'\\'\\n        )\\n\\n    if isinstance(media, (\\n            types.MessageMediaEmpty, types.MessageMediaUnsupported,\\n            types.ChatPhotoEmpty, types.UserProfilePhotoEmpty,\\n            types.ChatPhoto, types.UserProfilePhoto,\\n            types.FileLocationToBeDeprecated)):\\n        return types.InputMediaEmpty()\\n\\n    if isinstance(media, types.Message):\\n        return get_input_media(media.media, is_photo=is_photo)\\n\\n    _raise_cast_fail(media, \\'InputMedia\\')', 'def get_input_message(message):\\n    \"\"\"Similar to :meth:`get_input_peer`, but for input messages.\"\"\"\\n    try:\\n        if isinstance(message, int):  # This case is really common too\\n            return types.InputMessageID(message)\\n        elif message.SUBCLASS_OF_ID == 0x54b6bcc5:  # crc32(b\\'InputMessage\\'):\\n            return message\\n        elif message.SUBCLASS_OF_ID == 0x790009e3:  # crc32(b\\'Message\\'):\\n            return types.InputMessageID(message.id)\\n    except AttributeError:\\n        pass\\n\\n    _raise_cast_fail(message, \\'InputMedia\\')', 'def get_message_id(message):\\n    \"\"\"Similar to :meth:`get_input_peer`, but for message IDs.\"\"\"\\n    if message is None:\\n        return None\\n\\n    if isinstance(message, int):\\n        return message\\n\\n    try:\\n        if message.SUBCLASS_OF_ID == 0x790009e3:\\n            # hex(crc32(b\\'Message\\')) = 0x790009e3\\n            return message.id\\n    except AttributeError:\\n        pass\\n\\n    raise TypeError(\\'Invalid message type: {}\\'.format(type(message)))', 'def get_attributes(file, *, attributes=None, mime_type=None,\\n                   force_document=False, voice_note=False, video_note=False,\\n                   supports_streaming=False):\\n    \"\"\"\\n    Get a list of attributes for the given file and\\n    the mime type as a tuple ([attribute], mime_type).\\n    \"\"\"\\n    # Note: ``file.name`` works for :tl:`InputFile` and some `IOBase` streams\\n    name = file if isinstance(file, str) else getattr(file, \\'name\\', \\'unnamed\\')\\n    if mime_type is None:\\n        mime_type = mimetypes.guess_type(name)[0]\\n\\n    attr_dict = {types.DocumentAttributeFilename:\\n        types.DocumentAttributeFilename(os.path.basename(name))}\\n\\n    if is_audio(file):\\n        m = _get_metadata(file)\\n        if m:\\n            attr_dict[types.DocumentAttributeAudio] = \\\\\\n                types.DocumentAttributeAudio(\\n                    voice=voice_note,\\n                    title=m.get(\\'title\\') if m.has(\\'title\\') else None,\\n                    performer=m.get(\\'author\\') if m.has(\\'author\\') else None,\\n                    duration=int(m.get(\\'duration\\').seconds\\n                                 if m.has(\\'duration\\') else 0)\\n                )\\n\\n    if not force_document and is_video(file):\\n        m = _get_metadata(file)\\n        if m:\\n            doc = types.DocumentAttributeVideo(\\n                round_message=video_note,\\n                w=m.get(\\'width\\') if m.has(\\'width\\') else 0,\\n                h=m.get(\\'height\\') if m.has(\\'height\\') else 0,\\n                duration=int(m.get(\\'duration\\').seconds\\n                             if m.has(\\'duration\\') else 0),\\n                supports_streaming=supports_streaming\\n            )\\n        else:\\n            doc = types.DocumentAttributeVideo(\\n                0, 1, 1, round_message=video_note,\\n                supports_streaming=supports_streaming)\\n\\n        attr_dict[types.DocumentAttributeVideo] = doc\\n\\n    if voice_note:\\n        if types.DocumentAttributeAudio in attr_dict:\\n            attr_dict[types.DocumentAttributeAudio].voice = True\\n        else:\\n            attr_dict[types.DocumentAttributeAudio] = \\\\\\n                types.DocumentAttributeAudio(0, voice=True)\\n\\n    # Now override the attributes if any. As we have a dict of\\n    # {cls: instance}, we can override any class with the list\\n    # of attributes provided by the user easily.\\n    if attributes:\\n        for a in attributes:\\n            attr_dict[type(a)] = a\\n\\n    # Ensure we have a mime type, any; but it cannot be None\\n    # \\'The \"octet-stream\" subtype is used to indicate that a body\\n    # contains arbitrary binary data.\\'\\n    if not mime_type:\\n        mime_type = \\'application/octet-stream\\'\\n\\n    return list(attr_dict.values()), mime_type', 'def sanitize_parse_mode(mode):\\n    \"\"\"\\n    Converts the given parse mode into an object with\\n    ``parse`` and ``unparse`` callable properties.\\n    \"\"\"\\n    if not mode:\\n        return None\\n\\n    if callable(mode):\\n        class CustomMode:\\n            @staticmethod\\n            def unparse(text, entities):\\n                raise NotImplementedError\\n\\n        CustomMode.parse = mode\\n        return CustomMode\\n    elif (all(hasattr(mode, x) for x in (\\'parse\\', \\'unparse\\'))\\n          and all(callable(x) for x in (mode.parse, mode.unparse))):\\n        return mode\\n    elif isinstance(mode, str):\\n        try:\\n            return {\\n                \\'md\\': markdown,\\n                \\'markdown\\': markdown,\\n                \\'htm\\': html,\\n                \\'html\\': html\\n            }[mode.lower()]\\n        except KeyError:\\n            raise ValueError(\\'Unknown parse mode {}\\'.format(mode))\\n    else:\\n        raise TypeError(\\'Invalid parse mode type {}\\'.format(mode))', 'def get_input_location(location):\\n    \"\"\"\\n    Similar to :meth:`get_input_peer`, but for input messages.\\n\\n    Note that this returns a tuple ``(dc_id, location)``, the\\n    ``dc_id`` being present if known.\\n    \"\"\"\\n    try:\\n        if location.SUBCLASS_OF_ID == 0x1523d462:\\n            return None, location  # crc32(b\\'InputFileLocation\\'):\\n    except AttributeError:\\n        _raise_cast_fail(location, \\'InputFileLocation\\')\\n\\n    if isinstance(location, types.Message):\\n        location = location.media\\n\\n    if isinstance(location, types.MessageMediaDocument):\\n        location = location.document\\n    elif isinstance(location, types.MessageMediaPhoto):\\n        location = location.photo\\n\\n    if isinstance(location, types.Document):\\n        return (location.dc_id, types.InputDocumentFileLocation(\\n            id=location.id,\\n            access_hash=location.access_hash,\\n            file_reference=location.file_reference,\\n            thumb_size=\\'\\'  # Presumably to download one of its thumbnails\\n        ))\\n    elif isinstance(location, types.Photo):\\n        return (location.dc_id, types.InputPhotoFileLocation(\\n            id=location.id,\\n            access_hash=location.access_hash,\\n            file_reference=location.file_reference,\\n            thumb_size=location.sizes[-1].type\\n        ))\\n\\n    if isinstance(location, types.FileLocationToBeDeprecated):\\n        raise TypeError(\\'Unavailable location cannot be used as input\\')\\n\\n    _raise_cast_fail(location, \\'InputFileLocation\\')', 'def _get_extension(file):\\n    \"\"\"\\n    Gets the extension for the given file, which can be either a\\n    str or an ``open()``\\'ed file (which has a ``.name`` attribute).\\n    \"\"\"\\n    if isinstance(file, str):\\n        return os.path.splitext(file)[-1]\\n    elif isinstance(file, bytes):\\n        kind = imghdr.what(io.BytesIO(file))\\n        return (\\'.\\' + kind) if kind else \\'\\'\\n    elif isinstance(file, io.IOBase) and file.seekable():\\n        kind = imghdr.what(file)\\n        return (\\'.\\' + kind) if kind is not None else \\'\\'\\n    elif getattr(file, \\'name\\', None):\\n        # Note: ``file.name`` works for :tl:`InputFile` and some `IOBase`\\n        return _get_extension(file.name)\\n    else:\\n        return \\'\\'', 'def is_image(file):\\n    \"\"\"\\n    Returns ``True`` if the file extension looks like an image file to Telegram.\\n    \"\"\"\\n    match = re.match(r\\'\\\\.(png|jpe?g)\\', _get_extension(file), re.IGNORECASE)\\n    if match:\\n        return True\\n    else:\\n        return isinstance(resolve_bot_file_id(file), types.Photo)', 'def parse_phone(phone):\\n    \"\"\"Parses the given phone, or returns ``None`` if it\\'s invalid.\"\"\"\\n    if isinstance(phone, int):\\n        return str(phone)\\n    else:\\n        phone = re.sub(r\\'[+()\\\\s-]\\', \\'\\', str(phone))\\n        if phone.isdigit():\\n            return phone', 'def parse_username(username):\\n    \"\"\"\\n    Parses the given username or channel access hash, given\\n    a string, username or URL. Returns a tuple consisting of\\n    both the stripped, lowercase username and whether it is\\n    a joinchat/ hash (in which case is not lowercase\\'d).\\n\\n    Returns ``(None, False)`` if the ``username`` or link is not valid.\\n    \"\"\"\\n    username = username.strip()\\n    m = USERNAME_RE.match(username) or TG_JOIN_RE.match(username)\\n    if m:\\n        username = username[m.end():]\\n        is_invite = bool(m.group(1))\\n        if is_invite:\\n            return username, True\\n        else:\\n            username = username.rstrip(\\'/\\')\\n\\n    if VALID_USERNAME_RE.match(username):\\n        return username.lower(), False\\n    else:\\n        return None, False', 'def get_inner_text(text, entities):\\n    \"\"\"\\n    Gets the inner text that\\'s surrounded by the given entities.\\n    For instance: text = \\'hey!\\', entity = MessageEntityBold(2, 2) -> \\'y!\\'.\\n\\n    :param text:     the original text.\\n    :param entities: the entity or entities that must be matched.\\n    :return: a single result or a list of the text surrounded by the entities.\\n    \"\"\"\\n    text = add_surrogate(text)\\n    result = []\\n    for e in entities:\\n        start = e.offset\\n        end = e.offset + e.length\\n        result.append(del_surrogate(text[start:end]))\\n\\n    return result', 'def get_peer_id(peer, add_mark=True):\\n    \"\"\"\\n    Finds the ID of the given peer, and converts it to the \"bot api\" format\\n    so it the peer can be identified back. User ID is left unmodified,\\n    chat ID is negated, and channel ID is prefixed with -100.\\n\\n    The original ID and the peer type class can be returned with\\n    a call to :meth:`resolve_id(marked_id)`.\\n    \"\"\"\\n    # First we assert it\\'s a Peer TLObject, or early return for integers\\n    if isinstance(peer, int):\\n        return peer if add_mark else resolve_id(peer)[0]\\n\\n    # Tell the user to use their client to resolve InputPeerSelf if we got one\\n    if isinstance(peer, types.InputPeerSelf):\\n        _raise_cast_fail(peer, \\'int (you might want to use client.get_peer_id)\\')\\n\\n    try:\\n        peer = get_peer(peer)\\n    except TypeError:\\n        _raise_cast_fail(peer, \\'int\\')\\n\\n    if isinstance(peer, types.PeerUser):\\n        return peer.user_id\\n    elif isinstance(peer, types.PeerChat):\\n        # Check in case the user mixed things up to avoid blowing up\\n        if not (0 < peer.chat_id <= 0x7fffffff):\\n            peer.chat_id = resolve_id(peer.chat_id)[0]\\n\\n        return -peer.chat_id if add_mark else peer.chat_id\\n    else:  # if isinstance(peer, types.PeerChannel):\\n        # Check in case the user mixed things up to avoid blowing up\\n        if not (0 < peer.channel_id <= 0x7fffffff):\\n            peer.channel_id = resolve_id(peer.channel_id)[0]\\n\\n        if not add_mark:\\n            return peer.channel_id\\n\\n        # Concat -100 through math tricks, .to_supergroup() on\\n        # Madeline IDs will be strictly positive -> log works.\\n        return -(peer.channel_id + pow(\\n            10, math.floor(math.log10(peer.channel_id) + 3)))', 'def resolve_id(marked_id):\\n    \"\"\"Given a marked ID, returns the original ID and its :tl:`Peer` type.\"\"\"\\n    if marked_id >= 0:\\n        return marked_id, types.PeerUser\\n\\n    # There have been report of chat IDs being 10000xyz, which means their\\n    # marked version is -10000xyz, which in turn looks like a channel but\\n    # it becomes 00xyz (= xyz). Hence, we must assert that there are only\\n    # two zeroes.\\n    m = re.match(r\\'-100([^0]\\\\d*)\\', str(marked_id))\\n    if m:\\n        return int(m.group(1)), types.PeerChannel\\n\\n    return -marked_id, types.PeerChat', 'def _rle_decode(data):\\n    \"\"\"\\n    Decodes run-length-encoded `data`.\\n    \"\"\"\\n    if not data:\\n        return data\\n\\n    new = b\\'\\'\\n    last = b\\'\\'\\n    for cur in data:\\n        if last == b\\'\\\\0\\':\\n            new += last * cur\\n            last = b\\'\\'\\n        else:\\n            new += last\\n            last = bytes([cur])\\n\\n    return new + last', 'def _decode_telegram_base64(string):\\n    \"\"\"\\n    Decodes an url-safe base64-encoded string into its bytes\\n    by first adding the stripped necessary padding characters.\\n\\n    This is the way Telegram shares binary data as strings,\\n    such as Bot API-style file IDs or invite links.\\n\\n    Returns ``None`` if the input string was not valid.\\n    \"\"\"\\n    try:\\n        return base64.urlsafe_b64decode(string + \\'=\\' * (len(string) % 4))\\n    except (binascii.Error, ValueError, TypeError):\\n        return None', 'def _encode_telegram_base64(string):\\n    \"\"\"\\n    Inverse for `_decode_telegram_base64`.\\n    \"\"\"\\n    try:\\n        return base64.urlsafe_b64encode(string).rstrip(b\\'=\\').decode(\\'ascii\\')\\n    except (binascii.Error, ValueError, TypeError):\\n        return None', 'def resolve_bot_file_id(file_id):\\n    \"\"\"\\n    Given a Bot API-style `file_id`, returns the media it represents.\\n    If the `file_id` is not valid, ``None`` is returned instead.\\n\\n    Note that the `file_id` does not have information such as image\\n    dimensions or file size, so these will be zero if present.\\n\\n    For thumbnails, the photo ID and hash will always be zero.\\n    \"\"\"\\n    data = _rle_decode(_decode_telegram_base64(file_id))\\n    if not data or data[-1] == b\\'\\\\x02\\':\\n        return None\\n\\n    data = data[:-1]\\n    if len(data) == 24:\\n        file_type, dc_id, media_id, access_hash = struct.unpack(\\'<iiqq\\', data)\\n\\n        if not (1 <= dc_id <= 5):\\n            # Valid `file_id`\\'s must have valid DC IDs. Since this method is\\n            # called when sending a file and the user may have entered a path\\n            # they believe is correct but the file doesn\\'t exist, this method\\n            # may detect a path as \"valid\" bot `file_id` even when it\\'s not.\\n            # By checking the `dc_id`, we greatly reduce the chances of this\\n            # happening.\\n            return None\\n\\n        attributes = []\\n        if file_type == 3 or file_type == 9:\\n            attributes.append(types.DocumentAttributeAudio(\\n                duration=0,\\n                voice=file_type == 3\\n            ))\\n        elif file_type == 4 or file_type == 13:\\n            attributes.append(types.DocumentAttributeVideo(\\n                duration=0,\\n                w=0,\\n                h=0,\\n                round_message=file_type == 13\\n            ))\\n        # elif file_type == 5:  # other, cannot know which\\n        elif file_type == 8:\\n            attributes.append(types.DocumentAttributeSticker(\\n                alt=\\'\\',\\n                stickerset=types.InputStickerSetEmpty()\\n            ))\\n        elif file_type == 10:\\n            attributes.append(types.DocumentAttributeAnimated())\\n\\n        return types.Document(\\n            id=media_id,\\n            access_hash=access_hash,\\n            date=None,\\n            mime_type=\\'\\',\\n            size=0,\\n            thumbs=None,\\n            dc_id=dc_id,\\n            attributes=attributes,\\n            file_reference=b\\'\\'\\n        )\\n    elif len(data) == 44:\\n        (file_type, dc_id, media_id, access_hash,\\n            volume_id, secret, local_id) = struct.unpack(\\'<iiqqqqi\\', data)\\n\\n        if not (1 <= dc_id <= 5):\\n            return None\\n\\n        # Thumbnails (small) always have ID 0; otherwise size \\'x\\'\\n        photo_size = \\'s\\' if media_id or access_hash else \\'x\\'\\n        return types.Photo(\\n            id=media_id,\\n            access_hash=access_hash,\\n            file_reference=b\\'\\',\\n            date=None,\\n            sizes=[types.PhotoSize(\\n                type=photo_size,\\n                location=types.FileLocationToBeDeprecated(\\n                    volume_id=volume_id,\\n                    local_id=local_id\\n                ),\\n                w=0,\\n                h=0,\\n                size=0\\n            )],\\n            dc_id=dc_id,\\n            has_stickers=None\\n        )', 'def pack_bot_file_id(file):\\n    \"\"\"\\n    Inverse operation for `resolve_bot_file_id`.\\n\\n    The only parameters this method will accept are :tl:`Document` and\\n    :tl:`Photo`, and it will return a variable-length ``file_id`` string.\\n\\n    If an invalid parameter is given, it will ``return None``.\\n    \"\"\"\\n    if isinstance(file, types.MessageMediaDocument):\\n        file = file.document\\n    elif isinstance(file, types.MessageMediaPhoto):\\n        file = file.photo\\n\\n    if isinstance(file, types.Document):\\n        file_type = 5\\n        for attribute in file.attributes:\\n            if isinstance(attribute, types.DocumentAttributeAudio):\\n                file_type = 3 if attribute.voice else 9\\n            elif isinstance(attribute, types.DocumentAttributeVideo):\\n                file_type = 13 if attribute.round_message else 4\\n            elif isinstance(attribute, types.DocumentAttributeSticker):\\n                file_type = 8\\n            elif isinstance(attribute, types.DocumentAttributeAnimated):\\n                file_type = 10\\n            else:\\n                continue\\n            break\\n\\n        return _encode_telegram_base64(_rle_encode(struct.pack(\\n            \\'<iiqqb\\', file_type, file.dc_id, file.id, file.access_hash, 2)))\\n\\n    elif isinstance(file, types.Photo):\\n        size = next((x for x in reversed(file.sizes) if isinstance(\\n            x, (types.PhotoSize, types.PhotoCachedSize))), None)\\n\\n        if not size:\\n            return None\\n\\n        size = size.location\\n        return _encode_telegram_base64(_rle_encode(struct.pack(\\n            \\'<iiqqqqib\\', 2, file.dc_id, file.id, file.access_hash,\\n            size.volume_id, 0, size.local_id, 2  # 0 = old `secret`\\n        )))\\n    else:\\n        return None', 'def resolve_invite_link(link):\\n    \"\"\"\\n    Resolves the given invite link. Returns a tuple of\\n    ``(link creator user id, global chat id, random int)``.\\n\\n    Note that for broadcast channels, the link creator\\n    user ID will be zero to protect their identity.\\n    Normal chats and megagroup channels will have such ID.\\n\\n    Note that the chat ID may not be accurate for chats\\n    with a link that were upgraded to megagroup, since\\n    the link can remain the same, but the chat ID will\\n    be correct once a new link is generated.\\n    \"\"\"\\n    link_hash, is_link = parse_username(link)\\n    if not is_link:\\n        # Perhaps the user passed the link hash directly\\n        link_hash = link\\n\\n    try:\\n        return struct.unpack(\\'>LLQ\\', _decode_telegram_base64(link_hash))\\n    except (struct.error, TypeError):\\n        return None, None, None', 'def resolve_inline_message_id(inline_msg_id):\\n    \"\"\"\\n    Resolves an inline message ID. Returns a tuple of\\n    ``(message id, peer, dc id, access hash)``\\n\\n    The ``peer`` may either be a :tl:`PeerUser` referencing\\n    the user who sent the message via the bot in a private\\n    conversation or small group chat, or a :tl:`PeerChannel`\\n    if the message was sent in a channel.\\n\\n    The ``access_hash`` does not have any use yet.\\n    \"\"\"\\n    try:\\n        dc_id, message_id, pid, access_hash = \\\\\\n            struct.unpack(\\'<iiiq\\', _decode_telegram_base64(inline_msg_id))\\n        peer = types.PeerChannel(-pid) if pid < 0 else types.PeerUser(pid)\\n        return message_id, peer, dc_id, access_hash\\n    except (struct.error, TypeError):\\n        return None, None, None, None', 'def stripped_photo_to_jpg(stripped):\\n    \"\"\"\\n    Adds the JPG header and footer to a stripped image.\\n\\n    Ported from https://github.com/telegramdesktop/tdesktop/blob/bec39d89e19670eb436dc794a8f20b657cb87c71/Telegram/SourceFiles/ui/image/image.cpp#L225\\n    \"\"\"\\n    if len(stripped) < 3 or stripped[0] != 1:\\n        return stripped\\n\\n    header = bytearray(b\\'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x00\\\\x00\\\\x01\\\\x00\\\\x01\\\\x00\\\\x00\\\\xff\\\\xdb\\\\x00C\\\\x00(\\\\x1c\\\\x1e#\\\\x1e\\\\x19(#!#-+(0<dA<77<{X]Id\\\\x91\\\\x80\\\\x99\\\\x96\\\\x8f\\\\x80\\\\x8c\\\\x8a\\\\xa0\\\\xb4\\\\xe6\\\\xc3\\\\xa0\\\\xaa\\\\xda\\\\xad\\\\x8a\\\\x8c\\\\xc8\\\\xff\\\\xcb\\\\xda\\\\xee\\\\xf5\\\\xff\\\\xff\\\\xff\\\\x9b\\\\xc1\\\\xff\\\\xff\\\\xff\\\\xfa\\\\xff\\\\xe6\\\\xfd\\\\xff\\\\xf8\\\\xff\\\\xdb\\\\x00C\\\\x01+--<5<vAAv\\\\xf8\\\\xa5\\\\x8c\\\\xa5\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xf8\\\\xff\\\\xc0\\\\x00\\\\x11\\\\x08\\\\x00\\\\x00\\\\x00\\\\x00\\\\x03\\\\x01\"\\\\x00\\\\x02\\\\x11\\\\x01\\\\x03\\\\x11\\\\x01\\\\xff\\\\xc4\\\\x00\\\\x1f\\\\x00\\\\x00\\\\x01\\\\x05\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x01\\\\x02\\\\x03\\\\x04\\\\x05\\\\x06\\\\x07\\\\x08\\\\t\\\\n\\\\x0b\\\\xff\\\\xc4\\\\x00\\\\xb5\\\\x10\\\\x00\\\\x02\\\\x01\\\\x03\\\\x03\\\\x02\\\\x04\\\\x03\\\\x05\\\\x05\\\\x04\\\\x04\\\\x00\\\\x00\\\\x01}\\\\x01\\\\x02\\\\x03\\\\x00\\\\x04\\\\x11\\\\x05\\\\x12!1A\\\\x06\\\\x13Qa\\\\x07\"q\\\\x142\\\\x81\\\\x91\\\\xa1\\\\x08#B\\\\xb1\\\\xc1\\\\x15R\\\\xd1\\\\xf0$3br\\\\x82\\\\t\\\\n\\\\x16\\\\x17\\\\x18\\\\x19\\\\x1a%&\\\\\\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\\\x83\\\\x84\\\\x85\\\\x86\\\\x87\\\\x88\\\\x89\\\\x8a\\\\x92\\\\x93\\\\x94\\\\x95\\\\x96\\\\x97\\\\x98\\\\x99\\\\x9a\\\\xa2\\\\xa3\\\\xa4\\\\xa5\\\\xa6\\\\xa7\\\\xa8\\\\xa9\\\\xaa\\\\xb2\\\\xb3\\\\xb4\\\\xb5\\\\xb6\\\\xb7\\\\xb8\\\\xb9\\\\xba\\\\xc2\\\\xc3\\\\xc4\\\\xc5\\\\xc6\\\\xc7\\\\xc8\\\\xc9\\\\xca\\\\xd2\\\\xd3\\\\xd4\\\\xd5\\\\xd6\\\\xd7\\\\xd8\\\\xd9\\\\xda\\\\xe1\\\\xe2\\\\xe3\\\\xe4\\\\xe5\\\\xe6\\\\xe7\\\\xe8\\\\xe9\\\\xea\\\\xf1\\\\xf2\\\\xf3\\\\xf4\\\\xf5\\\\xf6\\\\xf7\\\\xf8\\\\xf9\\\\xfa\\\\xff\\\\xc4\\\\x00\\\\x1f\\\\x01\\\\x00\\\\x03\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x01\\\\x02\\\\x03\\\\x04\\\\x05\\\\x06\\\\x07\\\\x08\\\\t\\\\n\\\\x0b\\\\xff\\\\xc4\\\\x00\\\\xb5\\\\x11\\\\x00\\\\x02\\\\x01\\\\x02\\\\x04\\\\x04\\\\x03\\\\x04\\\\x07\\\\x05\\\\x04\\\\x04\\\\x00\\\\x01\\\\x02w\\\\x00\\\\x01\\\\x02\\\\x03\\\\x11\\\\x04\\\\x05!1\\\\x06\\\\x12AQ\\\\x07aq\\\\x13\"2\\\\x81\\\\x08\\\\x14B\\\\x91\\\\xa1\\\\xb1\\\\xc1\\\\t#3R\\\\xf0\\\\x15br\\\\xd1\\\\n\\\\x16$4\\\\xe1%\\\\xf1\\\\x17\\\\x18\\\\x19\\\\x1a&\\\\\\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\\\x82\\\\x83\\\\x84\\\\x85\\\\x86\\\\x87\\\\x88\\\\x89\\\\x8a\\\\x92\\\\x93\\\\x94\\\\x95\\\\x96\\\\x97\\\\x98\\\\x99\\\\x9a\\\\xa2\\\\xa3\\\\xa4\\\\xa5\\\\xa6\\\\xa7\\\\xa8\\\\xa9\\\\xaa\\\\xb2\\\\xb3\\\\xb4\\\\xb5\\\\xb6\\\\xb7\\\\xb8\\\\xb9\\\\xba\\\\xc2\\\\xc3\\\\xc4\\\\xc5\\\\xc6\\\\xc7\\\\xc8\\\\xc9\\\\xca\\\\xd2\\\\xd3\\\\xd4\\\\xd5\\\\xd6\\\\xd7\\\\xd8\\\\xd9\\\\xda\\\\xe2\\\\xe3\\\\xe4\\\\xe5\\\\xe6\\\\xe7\\\\xe8\\\\xe9\\\\xea\\\\xf2\\\\xf3\\\\xf4\\\\xf5\\\\xf6\\\\xf7\\\\xf8\\\\xf9\\\\xfa\\\\xff\\\\xda\\\\x00\\\\x0c\\\\x03\\\\x01\\\\x00\\\\x02\\\\x11\\\\x03\\\\x11\\\\x00?\\\\x00\\')\\n    footer = b\"\\\\xff\\\\xd9\"\\n    header[164] = stripped[1]\\n    header[166] = stripped[2]\\n    return bytes(header) + stripped[3:] + footer', 'async def send(self, request):\\n        \"\"\"\\n        Sends and receives the result for the given request.\\n        \"\"\"\\n        body = bytes(request)\\n        msg_id = self._state._get_new_msg_id()\\n        await self._connection.send(\\n            struct.pack(\\'<qqi\\', 0, msg_id, len(body)) + body\\n        )\\n\\n        body = await self._connection.recv()\\n        if len(body) < 8:\\n            raise InvalidBufferError(body)\\n\\n        with BinaryReader(body) as reader:\\n            auth_key_id = reader.read_long()\\n            assert auth_key_id == 0, \\'Bad auth_key_id\\'\\n\\n            msg_id = reader.read_long()\\n            assert msg_id != 0,  \\'Bad msg_id\\'\\n            # ^ We should make sure that the read ``msg_id`` is greater\\n            # than our own ``msg_id``. However, under some circumstances\\n            # (bad system clock/working behind proxies) this seems to not\\n            # be the case, which would cause endless assertion errors.\\n\\n            length = reader.read_int()\\n            assert length > 0,  \\'Bad length\\'\\n            # We could read length bytes and use those in a new reader to read\\n            # the next TLObject without including the padding, but since the\\n            # reader isn\\'t used for anything else after this, it\\'s unnecessary.\\n            return reader.tgread_object()', 'def _rel(self, path):\\n        \"\"\"\\n        Get the relative path for the given path from the current\\n        file by working around https://bugs.python.org/issue20012.\\n        \"\"\"\\n        return os.path.relpath(\\n            str(path), self._parent).replace(os.path.sep, \\'/\\')', 'def write_head(self, title, css_path, default_css):\\n        \"\"\"Writes the head part for the generated document,\\n           with the given title and CSS\\n        \"\"\"\\n        self.title = title\\n        self.write(\\n            \\'\\'\\'<!DOCTYPE html>\\n<html>\\n<head>\\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\\n    <title>{title}</title>\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n    <link id=\"style\" href=\"{rel_css}/docs.dark.css\" rel=\"stylesheet\">\\n    <script>\\n    document.getElementById(\"style\").href = \"{rel_css}/docs.\"\\n        + (localStorage.getItem(\"theme\") || \"{def_css}\")\\n        + \".css\";\\n    </script>\\n    <link href=\"https://fonts.googleapis.com/css?family=Nunito|Source+Code+Pro\"\\n          rel=\"stylesheet\">\\n</head>\\n<body>\\n<div id=\"main_div\">\\'\\'\\',\\n            title=title,\\n            rel_css=self._rel(css_path),\\n            def_css=default_css\\n        )', 'def set_menu_separator(self, img):\\n        \"\"\"Sets the menu separator.\\n           Must be called before adding entries to the menu\\n        \"\"\"\\n        if img:\\n            self.menu_separator_tag = \\'<img src=\"{}\" alt=\"/\" />\\'.format(\\n                self._rel(img))\\n        else:\\n            self.menu_separator_tag = None', 'def add_menu(self, name, link=None):\\n        \"\"\"Adds a menu entry, will create it if it doesn\\'t exist yet\"\"\"\\n        if self.menu_began:\\n            if self.menu_separator_tag:\\n                self.write(self.menu_separator_tag)\\n        else:\\n            # First time, create the menu tag\\n            self.write(\\'<ul class=\"horizontal\">\\')\\n            self.menu_began = True\\n\\n        self.write(\\'<li>\\')\\n        if link:\\n            self.write(\\'<a href=\"{}\">\\', self._rel(link))\\n\\n        # Write the real menu entry text\\n        self.write(name)\\n\\n        if link:\\n            self.write(\\'</a>\\')\\n        self.write(\\'</li>\\')', 'def write_title(self, title, level=1, id=None):\\n        \"\"\"Writes a title header in the document body,\\n           with an optional depth level\\n        \"\"\"\\n        if id:\\n            self.write(\\'<h{lv} id=\"{id}\">{title}</h{lv}>\\',\\n                       title=title, lv=level, id=id)\\n        else:\\n            self.write(\\'<h{lv}>{title}</h{lv}>\\',\\n                       title=title, lv=level)', 'def write_code(self, tlobject):\\n        \"\"\"Writes the code for the given \\'tlobject\\' properly\\n           formatted with hyperlinks\\n        \"\"\"\\n        self.write(\\'<pre>---{}---\\\\n\\',\\n                   \\'functions\\' if tlobject.is_function else \\'types\\')\\n\\n        # Write the function or type and its ID\\n        if tlobject.namespace:\\n            self.write(tlobject.namespace)\\n            self.write(\\'.\\')\\n\\n        self.write(\\'{}#{:08x}\\', tlobject.name, tlobject.id)\\n\\n        # Write all the arguments (or do nothing if there\\'s none)\\n        for arg in tlobject.args:\\n            self.write(\\' \\')\\n            add_link = not arg.generic_definition and not arg.is_generic\\n\\n            # \"Opening\" modifiers\\n            if arg.generic_definition:\\n                self.write(\\'{\\')\\n\\n            # Argument name\\n            self.write(arg.name)\\n            self.write(\\':\\')\\n\\n            # \"Opening\" modifiers\\n            if arg.is_flag:\\n                self.write(\\'flags.{}?\\', arg.flag_index)\\n\\n            if arg.is_generic:\\n                self.write(\\'!\\')\\n\\n            if arg.is_vector:\\n                self.write(\\'<a href=\"{}\">Vector</a>&lt;\\',\\n                           self.type_to_path(\\'vector\\'))\\n\\n            # Argument type\\n            if arg.type:\\n                if add_link:\\n                    self.write(\\'<a href=\"{}\">\\', self.type_to_path(arg.type))\\n                self.write(arg.type)\\n                if add_link:\\n                    self.write(\\'</a>\\')\\n            else:\\n                self.write(\\'#\\')\\n\\n            # \"Closing\" modifiers\\n            if arg.is_vector:\\n                self.write(\\'&gt;\\')\\n\\n            if arg.generic_definition:\\n                self.write(\\'}\\')\\n\\n        # Now write the resulting type (result from a function/type)\\n        self.write(\\' = \\')\\n        generic_name = next((arg.name for arg in tlobject.args\\n                             if arg.generic_definition), None)\\n\\n        if tlobject.result == generic_name:\\n            # Generic results cannot have any link\\n            self.write(tlobject.result)\\n        else:\\n            if re.search(\\'^vector<\\', tlobject.result, re.IGNORECASE):\\n                # Notice that we don\\'t simply make up the \"Vector\" part,\\n                # because some requests (as of now, only FutureSalts),\\n                # use a lower type name for it (see #81)\\n                vector, inner = tlobject.result.split(\\'<\\')\\n                inner = inner.strip(\\'>\\')\\n                self.write(\\'<a href=\"{}\">{}</a>&lt;\\',\\n                           self.type_to_path(vector), vector)\\n\\n                self.write(\\'<a href=\"{}\">{}</a>&gt;\\',\\n                           self.type_to_path(inner), inner)\\n            else:\\n                self.write(\\'<a href=\"{}\">{}</a>\\',\\n                           self.type_to_path(tlobject.result), tlobject.result)\\n\\n        self.write(\\'</pre>\\')', 'def begin_table(self, column_count):\\n        \"\"\"Begins a table with the given \\'column_count\\', required to automatically\\n           create the right amount of columns when adding items to the rows\"\"\"\\n        self.table_columns = column_count\\n        self.table_columns_left = 0\\n        self.write(\\'<table>\\')', 'def add_row(self, text, link=None, bold=False, align=None):\\n        \"\"\"This will create a new row, or add text to the next column\\n           of the previously created, incomplete row, closing it if complete\"\"\"\\n        if not self.table_columns_left:\\n            # Starting a new row\\n            self.write(\\'<tr>\\')\\n            self.table_columns_left = self.table_columns\\n\\n        self.write(\\'<td\\')\\n        if align:\\n            self.write(\\' style=\"text-align:{}\"\\', align)\\n        self.write(\\'>\\')\\n\\n        if bold:\\n            self.write(\\'<b>\\')\\n        if link:\\n            self.write(\\'<a href=\"{}\">\\', self._rel(link))\\n\\n        # Finally write the real table data, the given text\\n        self.write(text)\\n\\n        if link:\\n            self.write(\\'</a>\\')\\n        if bold:\\n            self.write(\\'</b>\\')\\n\\n        self.write(\\'</td>\\')\\n\\n        self.table_columns_left -= 1\\n        if not self.table_columns_left:\\n            self.write(\\'</tr>\\')', 'def write_copy_button(self, text, text_to_copy):\\n        \"\"\"Writes a button with \\'text\\' which can be used\\n           to copy \\'text_to_copy\\' to clipboard when it\\'s clicked.\"\"\"\\n        self.write_copy_script = True\\n        self.write(\\'<button onclick=\"cp(\\\\\\'{}\\\\\\');\">{}</button>\\'\\n                   .format(text_to_copy, text))', 'def end_body(self):\\n        \"\"\"Ends the whole document. This should be called the last\"\"\"\\n        if self.write_copy_script:\\n            self.write(\\n                \\'<textarea id=\"c\" class=\"invisible\"></textarea>\\'\\n                \\'<script>\\'\\n                \\'function cp(t){\\'\\n                \\'var c=document.getElementById(\"c\");\\'\\n                \\'c.value=t;\\'\\n                \\'c.select();\\'\\n                \\'try{document.execCommand(\"copy\")}\\'\\n                \\'catch(e){}}\\'\\n                \\'</script>\\'\\n            )\\n\\n        self.write(\\'</div>{}</body></html>\\', self._script)', 'def write(self, s, *args, **kwargs):\\n        \"\"\"Wrapper around handle.write\"\"\"\\n        if args or kwargs:\\n            self.handle.write(s.format(*args, **kwargs))\\n        else:\\n            self.handle.write(s)', 'def iter_participants(\\n            self, entity, limit=None, *, search=\\'\\',\\n            filter=None, aggressive=False\\n    ):\\n        \"\"\"\\n        Iterator over the participants belonging to the specified chat.\\n\\n        Args:\\n            entity (`entity`):\\n                The entity from which to retrieve the participants list.\\n\\n            limit (`int`):\\n                Limits amount of participants fetched.\\n\\n            search (`str`, optional):\\n                Look for participants with this string in name/username.\\n\\n                If ``aggressive is True``, the symbols from this string will\\n                be used.\\n\\n            filter (:tl:`ChannelParticipantsFilter`, optional):\\n                The filter to be used, if you want e.g. only admins\\n                Note that you might not have permissions for some filter.\\n                This has no effect for normal chats or users.\\n\\n                .. note::\\n\\n                    The filter :tl:`ChannelParticipantsBanned` will return\\n                    *restricted* users. If you want *banned* users you should\\n                    use :tl:`ChannelParticipantsKicked` instead.\\n\\n            aggressive (`bool`, optional):\\n                Aggressively looks for all participants in the chat.\\n\\n                This is useful for channels since 20 July 2018,\\n                Telegram added a server-side limit where only the\\n                first 200 members can be retrieved. With this flag\\n                set, more than 200 will be often be retrieved.\\n\\n                This has no effect if a ``filter`` is given.\\n\\n        Yields:\\n            The :tl:`User` objects returned by :tl:`GetParticipantsRequest`\\n            with an additional ``.participant`` attribute which is the\\n            matched :tl:`ChannelParticipant` type for channels/megagroups\\n            or :tl:`ChatParticipants` for normal chats.\\n        \"\"\"\\n        return _ParticipantsIter(\\n            self,\\n            limit,\\n            entity=entity,\\n            filter=filter,\\n            search=search,\\n            aggressive=aggressive\\n        )', 'def iter_admin_log(\\n            self, entity, limit=None, *, max_id=0, min_id=0, search=None,\\n            admins=None, join=None, leave=None, invite=None, restrict=None,\\n            unrestrict=None, ban=None, unban=None, promote=None, demote=None,\\n            info=None, settings=None, pinned=None, edit=None, delete=None):\\n        \"\"\"\\n        Iterator over the admin log for the specified channel.\\n\\n        Note that you must be an administrator of it to use this method.\\n\\n        If none of the filters are present (i.e. they all are ``None``),\\n        *all* event types will be returned. If at least one of them is\\n        ``True``, only those that are true will be returned.\\n\\n        Args:\\n            entity (`entity`):\\n                The channel entity from which to get its admin log.\\n\\n            limit (`int` | `None`, optional):\\n                Number of events to be retrieved.\\n\\n                The limit may also be ``None``, which would eventually return\\n                the whole history.\\n\\n            max_id (`int`):\\n                All the events with a higher (newer) ID or equal to this will\\n                be excluded.\\n\\n            min_id (`int`):\\n                All the events with a lower (older) ID or equal to this will\\n                be excluded.\\n\\n            search (`str`):\\n                The string to be used as a search query.\\n\\n            admins (`entity` | `list`):\\n                If present, the events will be filtered by these admins\\n                (or single admin) and only those caused by them will be\\n                returned.\\n\\n            join (`bool`):\\n                If ``True``, events for when a user joined will be returned.\\n\\n            leave (`bool`):\\n                If ``True``, events for when a user leaves will be returned.\\n\\n            invite (`bool`):\\n                If ``True``, events for when a user joins through an invite\\n                link will be returned.\\n\\n            restrict (`bool`):\\n                If ``True``, events with partial restrictions will be\\n                returned. This is what the API calls \"ban\".\\n\\n            unrestrict (`bool`):\\n                If ``True``, events removing restrictions will be returned.\\n                This is what the API calls \"unban\".\\n\\n            ban (`bool`):\\n                If ``True``, events applying or removing all restrictions will\\n                be returned. This is what the API calls \"kick\" (restricting\\n                all permissions removed is a ban, which kicks the user).\\n\\n            unban (`bool`):\\n                If ``True``, events removing all restrictions will be\\n                returned. This is what the API calls \"unkick\".\\n\\n            promote (`bool`):\\n                If ``True``, events with admin promotions will be returned.\\n\\n            demote (`bool`):\\n                If ``True``, events with admin demotions will be returned.\\n\\n            info (`bool`):\\n                If ``True``, events changing the group info will be returned.\\n\\n            settings (`bool`):\\n                If ``True``, events changing the group settings will be\\n                returned.\\n\\n            pinned (`bool`):\\n                If ``True``, events of new pinned messages will be returned.\\n\\n            edit (`bool`):\\n                If ``True``, events of message edits will be returned.\\n\\n            delete (`bool`):\\n                If ``True``, events of message deletions will be returned.\\n\\n        Yields:\\n            Instances of `telethon.tl.custom.adminlogevent.AdminLogEvent`.\\n        \"\"\"\\n        return _AdminLogIter(\\n            self,\\n            limit,\\n            entity=entity,\\n            admins=admins,\\n            search=search,\\n            min_id=min_id,\\n            max_id=max_id,\\n            join=join,\\n            leave=leave,\\n            invite=invite,\\n            restrict=restrict,\\n            unrestrict=unrestrict,\\n            ban=ban,\\n            unban=unban,\\n            promote=promote,\\n            demote=demote,\\n            info=info,\\n            settings=settings,\\n            pinned=pinned,\\n            edit=edit,\\n            delete=delete\\n        )', 'def action(self, entity, action, *, delay=4, auto_cancel=True):\\n        \"\"\"\\n        Returns a context-manager object to represent a \"chat action\".\\n\\n        Chat actions indicate things like \"user is typing\", \"user is\\n        uploading a photo\", etc. Normal usage is as follows:\\n\\n        .. code-block:: python\\n\\n            async with client.action(chat, \\'typing\\'):\\n                await asyncio.sleep(2)  # type for 2 seconds\\n                await client.send_message(chat, \\'Hello world! I type slow ^^\\')\\n\\n        If the action is ``\\'cancel\\'``, you should just ``await`` the result,\\n        since it makes no sense to use a context-manager for it:\\n\\n        .. code-block:: python\\n\\n            await client.action(chat, \\'cancel\\')\\n\\n        Args:\\n            entity (`entity`):\\n                The entity where the action should be showed in.\\n\\n            action (`str` | :tl:`SendMessageAction`):\\n                The action to show. You can either pass a instance of\\n                :tl:`SendMessageAction` or better, a string used while:\\n\\n                * ``\\'typing\\'``: typing a text message.\\n                * ``\\'contact\\'``: choosing a contact.\\n                * ``\\'game\\'``: playing a game.\\n                * ``\\'location\\'``: choosing a geo location.\\n                * ``\\'record-audio\\'``: recording a voice note.\\n                  You may use ``\\'record-voice\\'`` as alias.\\n                * ``\\'record-round\\'``: recording a round video.\\n                * ``\\'record-video\\'``: recording a normal video.\\n                * ``\\'audio\\'``: sending an audio file (voice note or song).\\n                  You may use ``\\'voice\\'`` and ``\\'song\\'`` as aliases.\\n                * ``\\'round\\'``: uploading a round video.\\n                * ``\\'video\\'``: uploading a video file.\\n                * ``\\'photo\\'``: uploading a photo.\\n                * ``\\'document\\'``: uploading a document file.\\n                  You may use ``\\'file\\'`` as alias.\\n                * ``\\'cancel\\'``: cancel any pending action in this chat.\\n\\n                Invalid strings will raise a ``ValueError``.\\n\\n            delay (`int` | `float`):\\n                The delay, in seconds, to wait between sending actions.\\n                For example, if the delay is 5 and it takes 7 seconds to\\n                do something, three requests will be made at 0s, 5s, and\\n                7s to cancel the action.\\n\\n            auto_cancel (`bool`):\\n                Whether the action should be cancelled once the context\\n                manager exists or not. The default is ``True``, since\\n                you don\\'t want progress to be shown when it has already\\n                completed.\\n\\n        If you are uploading a file, you may do\\n        ``progress_callback=chat.progress`` to update the progress of\\n        the action. Some clients don\\'t care about this progress, though,\\n        so it\\'s mostly not needed, but still available.\\n        \"\"\"\\n        if isinstance(action, str):\\n            try:\\n                action = _ChatAction._str_mapping[action.lower()]\\n            except KeyError:\\n                raise ValueError(\\'No such action \"{}\"\\'.format(action)) from None\\n        elif not isinstance(action, types.TLObject) or action.SUBCLASS_OF_ID != 0x20b2cc21:\\n            # 0x20b2cc21 = crc32(b\\'SendMessageAction\\')\\n            if isinstance(action, type):\\n                raise ValueError(\\'You must pass an instance, not the class\\')\\n            else:\\n                raise ValueError(\\'Cannot use {} as action\\'.format(action))\\n\\n        if isinstance(action, types.SendMessageCancelAction):\\n            # ``SetTypingRequest.resolve`` will get input peer of ``entity``.\\n            return self(functions.messages.SetTypingRequest(\\n                entity, types.SendMessageCancelAction()))\\n\\n        return _ChatAction(\\n            self, entity, action, delay=delay, auto_cancel=auto_cancel)', 'async def prepare_decrypter(client, cdn_client, cdn_redirect):\\n        \"\"\"\\n        Prepares a new CDN decrypter.\\n\\n        :param client: a TelegramClient connected to the main servers.\\n        :param cdn_client: a new client connected to the CDN.\\n        :param cdn_redirect: the redirect file object that caused this call.\\n        :return: (CdnDecrypter, first chunk file data)\\n        \"\"\"\\n        cdn_aes = AESModeCTR(\\n            key=cdn_redirect.encryption_key,\\n            # 12 first bytes of the IV..4 bytes of the offset (0, big endian)\\n            iv=cdn_redirect.encryption_iv[:12] + bytes(4)\\n        )\\n\\n        # We assume that cdn_redirect.cdn_file_hashes are ordered by offset,\\n        # and that there will be enough of these to retrieve the whole file.\\n        decrypter = CdnDecrypter(\\n            cdn_client, cdn_redirect.file_token,\\n            cdn_aes, cdn_redirect.cdn_file_hashes\\n        )\\n\\n        cdn_file = await cdn_client(GetCdnFileRequest(\\n            file_token=cdn_redirect.file_token,\\n            offset=cdn_redirect.cdn_file_hashes[0].offset,\\n            limit=cdn_redirect.cdn_file_hashes[0].limit\\n        ))\\n        if isinstance(cdn_file, CdnFileReuploadNeeded):\\n            # We need to use the original client here\\n            await client(ReuploadCdnFileRequest(\\n                file_token=cdn_redirect.file_token,\\n                request_token=cdn_file.request_token\\n            ))\\n\\n            # We want to always return a valid upload.CdnFile\\n            cdn_file = decrypter.get_file()\\n        else:\\n            cdn_file.bytes = decrypter.cdn_aes.encrypt(cdn_file.bytes)\\n            cdn_hash = decrypter.cdn_file_hashes.pop(0)\\n            decrypter.check(cdn_file.bytes, cdn_hash)\\n\\n        return decrypter, cdn_file', 'def get_file(self):\\n        \"\"\"\\n        Calls GetCdnFileRequest and decrypts its bytes.\\n        Also ensures that the file hasn\\'t been tampered.\\n\\n        :return: the CdnFile result.\\n        \"\"\"\\n        if self.cdn_file_hashes:\\n            cdn_hash = self.cdn_file_hashes.pop(0)\\n            cdn_file = self.client(GetCdnFileRequest(\\n                self.file_token, cdn_hash.offset, cdn_hash.limit\\n            ))\\n            cdn_file.bytes = self.cdn_aes.encrypt(cdn_file.bytes)\\n            self.check(cdn_file.bytes, cdn_hash)\\n        else:\\n            cdn_file = CdnFile(bytes(0))\\n\\n        return cdn_file', 'def parse(html):\\n    \"\"\"\\n    Parses the given HTML message and returns its stripped representation\\n    plus a list of the MessageEntity\\'s that were found.\\n\\n    :param message: the message with HTML to be parsed.\\n    :return: a tuple consisting of (clean message, [message entities]).\\n    \"\"\"\\n    if not html:\\n        return html, []\\n\\n    parser = HTMLToTelegramParser()\\n    parser.feed(_add_surrogate(html))\\n    text = helpers.strip_text(parser.text, parser.entities)\\n    return _del_surrogate(text), parser.entities', 'def unparse(text, entities):\\n    \"\"\"\\n    Performs the reverse operation to .parse(), effectively returning HTML\\n    given a normal text and its MessageEntity\\'s.\\n\\n    :param text: the text to be reconverted into HTML.\\n    :param entities: the MessageEntity\\'s applied to the text.\\n    :return: a HTML representation of the combination of both inputs.\\n    \"\"\"\\n    if not text or not entities:\\n        return text\\n\\n    text = _add_surrogate(text)\\n    html = []\\n    last_offset = 0\\n    for entity in entities:\\n        if entity.offset > last_offset:\\n            html.append(escape(text[last_offset:entity.offset]))\\n        elif entity.offset < last_offset:\\n            continue\\n\\n        skip_entity = False\\n        entity_text = escape(text[entity.offset:entity.offset + entity.length])\\n        entity_type = type(entity)\\n\\n        if entity_type == MessageEntityBold:\\n            html.append(\\'<strong>{}</strong>\\'.format(entity_text))\\n        elif entity_type == MessageEntityItalic:\\n            html.append(\\'<em>{}</em>\\'.format(entity_text))\\n        elif entity_type == MessageEntityCode:\\n            html.append(\\'<code>{}</code>\\'.format(entity_text))\\n        elif entity_type == MessageEntityPre:\\n            if entity.language:\\n                html.append(\\n                    \"<pre>\\\\n\"\\n                    \"    <code class=\\'language-{}\\'>\\\\n\"\\n                    \"        {}\\\\n\"\\n                    \"    </code>\\\\n\"\\n                    \"</pre>\".format(entity.language, entity_text))\\n            else:\\n                html.append(\\'<pre><code>{}</code></pre>\\'\\n                            .format(entity_text))\\n        elif entity_type == MessageEntityEmail:\\n            html.append(\\'<a href=\"mailto:{0}\">{0}</a>\\'.format(entity_text))\\n        elif entity_type == MessageEntityUrl:\\n            html.append(\\'<a href=\"{0}\">{0}</a>\\'.format(entity_text))\\n        elif entity_type == MessageEntityTextUrl:\\n            html.append(\\'<a href=\"{}\">{}</a>\\'\\n                        .format(escape(entity.url), entity_text))\\n        elif entity_type == MessageEntityMentionName:\\n            html.append(\\'<a href=\"tg://user?id={}\">{}</a>\\'\\n                        .format(entity.user_id, entity_text))\\n        else:\\n            skip_entity = True\\n        last_offset = entity.offset + (0 if skip_entity else entity.length)\\n    html.append(text[last_offset:])\\n    return _del_surrogate(\\'\\'.join(html))', 'def data(self):\\n        \"\"\"The ``bytes`` data for :tl:`KeyboardButtonCallback` objects.\"\"\"\\n        if isinstance(self.button, types.KeyboardButtonCallback):\\n            return self.button.data', 'def inline_query(self):\\n        \"\"\"The query ``str`` for :tl:`KeyboardButtonSwitchInline` objects.\"\"\"\\n        if isinstance(self.button, types.KeyboardButtonSwitchInline):\\n            return self.button.query', 'def url(self):\\n        \"\"\"The url ``str`` for :tl:`KeyboardButtonUrl` objects.\"\"\"\\n        if isinstance(self.button, types.KeyboardButtonUrl):\\n            return self.button.url', 'async def click(self):\\n        \"\"\"\\n        Emulates the behaviour of clicking this button.\\n\\n        If it\\'s a normal :tl:`KeyboardButton` with text, a message will be\\n        sent, and the sent `telethon.tl.custom.message.Message` returned.\\n\\n        If it\\'s an inline :tl:`KeyboardButtonCallback` with text and data,\\n        it will be \"clicked\" and the :tl:`BotCallbackAnswer` returned.\\n\\n        If it\\'s an inline :tl:`KeyboardButtonSwitchInline` button, the\\n        :tl:`StartBotRequest` will be invoked and the resulting updates\\n        returned.\\n\\n        If it\\'s a :tl:`KeyboardButtonUrl`, the URL of the button will\\n        be passed to ``webbrowser.open`` and return ``True`` on success.\\n        \"\"\"\\n        if isinstance(self.button, types.KeyboardButton):\\n            return await self._client.send_message(\\n                self._chat, self.button.text, reply_to=self._msg_id)\\n        elif isinstance(self.button, types.KeyboardButtonCallback):\\n            req = functions.messages.GetBotCallbackAnswerRequest(\\n                peer=self._chat, msg_id=self._msg_id, data=self.button.data\\n            )\\n            try:\\n                return await self._client(req)\\n            except BotTimeout:\\n                return None\\n        elif isinstance(self.button, types.KeyboardButtonSwitchInline):\\n            return await self._client(functions.messages.StartBotRequest(\\n                bot=self._bot, peer=self._chat, start_param=self.button.query\\n            ))\\n        elif isinstance(self.button, types.KeyboardButtonUrl):\\n            return webbrowser.open(self.button.url)\\n        elif isinstance(self.button, types.KeyboardButtonGame):\\n            req = functions.messages.GetBotCallbackAnswerRequest(\\n                peer=self._chat, msg_id=self._msg_id, game=True\\n            )\\n            try:\\n                return await self._client(req)\\n            except BotTimeout:\\n                return None', 'async def _into_id_set(client, chats):\\n    \"\"\"Helper util to turn the input chat or chats into a set of IDs.\"\"\"\\n    if chats is None:\\n        return None\\n\\n    if not utils.is_list_like(chats):\\n        chats = (chats,)\\n\\n    result = set()\\n    for chat in chats:\\n        if isinstance(chat, int):\\n            if chat < 0:\\n                result.add(chat)  # Explicitly marked IDs are negative\\n            else:\\n                result.update({  # Support all valid types of peers\\n                    utils.get_peer_id(types.PeerUser(chat)),\\n                    utils.get_peer_id(types.PeerChat(chat)),\\n                    utils.get_peer_id(types.PeerChannel(chat)),\\n                })\\n        elif isinstance(chat, TLObject) and chat.SUBCLASS_OF_ID == 0x2d45687:\\n            # 0x2d45687 == crc32(b\\'Peer\\')\\n            result.add(utils.get_peer_id(chat))\\n        else:\\n            chat = await client.get_input_entity(chat)\\n            if isinstance(chat, types.InputPeerSelf):\\n                chat = await client.get_me(input_peer=True)\\n            result.add(utils.get_peer_id(chat))\\n\\n    return result', 'def name_inner_event(cls):\\n    \"\"\"Decorator to rename cls.Event \\'Event\\' as \\'cls.Event\\'\"\"\"\\n    if hasattr(cls, \\'Event\\'):\\n        cls.Event._event_name = \\'{}.Event\\'.format(cls.__name__)\\n    else:\\n        warnings.warn(\\'Class {} does not have a inner Event\\'.format(cls))\\n    return cls', 'async def resolve(self, client):\\n        \"\"\"Helper method to allow event builders to be resolved before usage\"\"\"\\n        if self.resolved:\\n            return\\n\\n        if not self._resolve_lock:\\n            self._resolve_lock = asyncio.Lock(loop=client.loop)\\n\\n        async with self._resolve_lock:\\n            if not self.resolved:\\n                await self._resolve(client)\\n                self.resolved = True', 'def filter(self, event):\\n        \"\"\"\\n        If the ID of ``event._chat_peer`` isn\\'t in the chats set (or it is\\n        but the set is a blacklist) returns ``None``, otherwise the event.\\n\\n        The events must have been resolved before this can be called.\\n        \"\"\"\\n        if not self.resolved:\\n            return None\\n\\n        if self.chats is not None:\\n            # Note: the `event.chat_id` property checks if it\\'s `None` for us\\n            inside = event.chat_id in self.chats\\n            if inside == self.blacklist_chats:\\n                # If this chat matches but it\\'s a blacklist ignore.\\n                # If it doesn\\'t match but it\\'s a whitelist ignore.\\n                return None\\n\\n        if not self.func or self.func(event):\\n            return event', 'def _get_entity_pair(self, entity_id):\\n        \"\"\"\\n        Returns ``(entity, input_entity)`` for the given entity ID.\\n        \"\"\"\\n        entity = self._entities.get(entity_id)\\n        try:\\n            input_entity = utils.get_input_peer(entity)\\n        except TypeError:\\n            try:\\n                input_entity = self._client._entity_cache[entity_id]\\n            except KeyError:\\n                input_entity = None\\n\\n        return entity, input_entity', 'def _load_entities(self):\\n        \"\"\"\\n        Must load all the entities it needs from cache, and\\n        return ``False`` if it could not find all of them.\\n        \"\"\"\\n        if not self._chat_peer:\\n            return True  # Nothing to load (e.g. MessageDeleted)\\n\\n        self._chat, self._input_chat = self._get_entity_pair(self.chat_id)\\n        return self._input_chat is not None', 'async def _get_difference(self, channel_id, pts_date):\\n        \"\"\"\\n        Get the difference for this `channel_id` if any, then load entities.\\n\\n        Calls :tl:`updates.getDifference`, which fills the entities cache\\n        (always done by `__call__`) and lets us know about the full entities.\\n        \"\"\"\\n        # Fetch since the last known pts/date before this update arrived,\\n        # in order to fetch this update at full, including its entities.\\n        self.client._log[__name__].debug(\\'Getting difference for entities\\')\\n        if channel_id:\\n            try:\\n                where = await self.client.get_input_entity(channel_id)\\n            except ValueError:\\n                return\\n\\n            result = await self.client(functions.updates.GetChannelDifferenceRequest(\\n                channel=where,\\n                filter=types.ChannelMessagesFilterEmpty(),\\n                pts=pts_date,  # just pts\\n                limit=100,\\n                force=True\\n            ))\\n        else:\\n            result = await self.client(functions.updates.GetDifferenceRequest(\\n                pts=pts_date[0],\\n                date=pts_date[1],\\n                qts=0\\n            ))\\n\\n        if isinstance(result, (types.updates.Difference,\\n                               types.updates.DifferenceSlice,\\n                               types.updates.ChannelDifference,\\n                               types.updates.ChannelDifferenceTooLong)):\\n            self.original_update._entities.update({\\n                utils.get_peer_id(x): x for x in\\n                itertools.chain(result.users, result.chats)\\n            })\\n\\n        if not self._load_entities():\\n            self.client._log[__name__].info(\\n                \\'Could not find all entities for update.pts = %s\\',\\n                getattr(self.original_update, \\'pts\\', None)\\n            )', 'async def get_chat(self):\\n        \"\"\"\\n        Returns `chat`, but will make an API call to find the\\n        chat unless it\\'s already cached.\\n        \"\"\"\\n        # See `get_sender` for information about \\'min\\'.\\n        if (self._chat is None or getattr(self._chat, \\'min\\', None))\\\\\\n                and await self.get_input_chat():\\n            try:\\n                self._chat =\\\\\\n                    await self._client.get_entity(self._input_chat)\\n            except ValueError:\\n                await self._refetch_chat()\\n        return self._chat', 'def input_chat(self):\\n        \"\"\"\\n        This :tl:`InputPeer` is the input version of the chat where the\\n        message was sent. Similarly to `input_sender`, this doesn\\'t have\\n        things like username or similar, but still useful in some cases.\\n\\n        Note that this might not be available if the library doesn\\'t\\n        have enough information available.\\n        \"\"\"\\n        if self._input_chat is None and self._chat_peer:\\n            try:\\n                self._input_chat = self._client._entity_cache[self._chat_peer]\\n            except KeyError:\\n                pass\\n\\n        return self._input_chat', 'async def get_input_chat(self):\\n        \"\"\"\\n        Returns `input_chat`, but will make an API call to find the\\n        input chat unless it\\'s already cached.\\n        \"\"\"\\n        if self.input_chat is None and self.chat_id:\\n            try:\\n                # The chat may be recent, look in dialogs\\n                target = self.chat_id\\n                async for d in self._client.iter_dialogs(100):\\n                    if d.id == target:\\n                        self._chat = d.entity\\n                        self._input_chat = d.input_entity\\n                        break\\n            except errors.RPCError:\\n                pass\\n\\n        return self._input_chat', 'def is_group(self):\\n        \"\"\"True if the message was sent on a group or megagroup.\"\"\"\\n        if self._broadcast is None and self.chat:\\n            self._broadcast = getattr(self.chat, \\'broadcast\\', None)\\n\\n        return (\\n            isinstance(self._chat_peer, (types.PeerChat, types.PeerChannel))\\n            and not self._broadcast\\n        )', 'def generate_random_long(signed=True):\\n    \"\"\"Generates a random long integer (8 bytes), which is optionally signed\"\"\"\\n    return int.from_bytes(os.urandom(8), signed=signed, byteorder=\\'little\\')', 'def ensure_parent_dir_exists(file_path):\\n    \"\"\"Ensures that the parent directory exists\"\"\"\\n    parent = os.path.dirname(file_path)\\n    if parent:\\n        os.makedirs(parent, exist_ok=True)', 'def strip_text(text, entities):\\n    \"\"\"\\n    Strips whitespace from the given text modifying the provided entities.\\n\\n    This assumes that there are no overlapping entities, that their length\\n    is greater or equal to one, and that their length is not out of bounds.\\n    \"\"\"\\n    if not entities:\\n        return text.strip()\\n\\n    while text and text[-1].isspace():\\n        e = entities[-1]\\n        if e.offset + e.length == len(text):\\n            if e.length == 1:\\n                del entities[-1]\\n                if not entities:\\n                    return text.strip()\\n            else:\\n                e.length -= 1\\n        text = text[:-1]\\n\\n    while text and text[0].isspace():\\n        for i in reversed(range(len(entities))):\\n            e = entities[i]\\n            if e.offset != 0:\\n                e.offset -= 1\\n                continue\\n\\n            if e.length == 1:\\n                del entities[0]\\n                if not entities:\\n                    return text.lstrip()\\n            else:\\n                e.length -= 1\\n\\n        text = text[1:]\\n\\n    return text', 'async def _cancel(log, **tasks):\\n    \"\"\"\\n    Helper to cancel one or more tasks gracefully, logging exceptions.\\n    \"\"\"\\n    for name, task in tasks.items():\\n        if not task:\\n            continue\\n\\n        task.cancel()\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            pass\\n        except Exception:\\n            log.exception(\\'Unhandled exception from %s after cancel\\', name)', 'def _sync_enter(self):\\n    \"\"\"\\n    Helps to cut boilerplate on async context\\n    managers that offer synchronous variants.\\n    \"\"\"\\n    if hasattr(self, \\'loop\\'):\\n        loop = self.loop\\n    else:\\n        loop = self._client.loop\\n\\n    if loop.is_running():\\n        raise RuntimeError(\\n            \\'You must use \"async with\" if the event loop \\'\\n            \\'is running (i.e. you are inside an \"async def\")\\'\\n        )\\n\\n    return loop.run_until_complete(self.__aenter__())', 'def generate_key_data_from_nonce(server_nonce, new_nonce):\\n    \"\"\"Generates the key data corresponding to the given nonce\"\"\"\\n    server_nonce = server_nonce.to_bytes(16, \\'little\\', signed=True)\\n    new_nonce = new_nonce.to_bytes(32, \\'little\\', signed=True)\\n    hash1 = sha1(new_nonce + server_nonce).digest()\\n    hash2 = sha1(server_nonce + new_nonce).digest()\\n    hash3 = sha1(new_nonce + new_nonce).digest()\\n\\n    key = hash1 + hash2[:12]\\n    iv = hash2[12:20] + hash3 + new_nonce[:4]\\n    return key, iv', 'def write(self, string, *args, **kwargs):\\n        \"\"\"Writes a string into the source code,\\n           applying indentation if required\\n        \"\"\"\\n        if self.on_new_line:\\n            self.on_new_line = False  # We\\'re not on a new line anymore\\n            # If the string was not empty, indent; Else probably a new line\\n            if string.strip():\\n                self.indent()\\n\\n        if args or kwargs:\\n            self.out_stream.write(string.format(*args, **kwargs))\\n        else:\\n            self.out_stream.write(string)', 'def writeln(self, string=\\'\\', *args, **kwargs):\\n        \"\"\"Writes a string into the source code _and_ appends a new line,\\n           applying indentation if required\\n        \"\"\"\\n        self.write(string + \\'\\\\n\\', *args, **kwargs)\\n        self.on_new_line = True\\n\\n        # If we\\'re writing a block, increment indent for the next time\\n        if string and string[-1] == \\':\\':\\n            self.current_indent += 1\\n\\n        # Clear state after the user adds a new line\\n        self.auto_added_line = False', 'def end_block(self):\\n        \"\"\"Ends an indentation block, leaving an empty line afterwards\"\"\"\\n        self.current_indent -= 1\\n\\n        # If we did not add a new line automatically yet, now it\\'s the time!\\n        if not self.auto_added_line:\\n            self.writeln()\\n            self.auto_added_line = True', 'def _write_source_code(tlobject, kind, builder, type_constructors):\\n    \"\"\"\\n    Writes the source code corresponding to the given TLObject\\n    by making use of the ``builder`` `SourceBuilder`.\\n\\n    Additional information such as file path depth and\\n    the ``Type: [Constructors]`` must be given for proper\\n    importing and documentation strings.\\n    \"\"\"\\n    _write_class_init(tlobject, kind, type_constructors, builder)\\n    _write_resolve(tlobject, builder)\\n    _write_to_dict(tlobject, builder)\\n    _write_to_bytes(tlobject, builder)\\n    _write_from_reader(tlobject, builder)\\n    _write_read_result(tlobject, builder)', 'def _write_arg_to_bytes(builder, arg, args, name=None):\\n    \"\"\"\\n    Writes the .__bytes__() code for the given argument\\n    :param builder: The source code builder\\n    :param arg: The argument to write\\n    :param args: All the other arguments in TLObject same __bytes__.\\n                 This is required to determine the flags value\\n    :param name: The name of the argument. Defaults to \"self.argname\"\\n                 This argument is an option because it\\'s required when\\n                 writing Vectors<>\\n    \"\"\"\\n    if arg.generic_definition:\\n        return  # Do nothing, this only specifies a later type\\n\\n    if name is None:\\n        name = \\'self.{}\\'.format(arg.name)\\n\\n    # The argument may be a flag, only write if it\\'s not None AND\\n    # if it\\'s not a True type.\\n    # True types are not actually sent, but instead only used to\\n    # determine the flags.\\n    if arg.is_flag:\\n        if arg.type == \\'true\\':\\n            return  # Exit, since True type is never written\\n        elif arg.is_vector:\\n            # Vector flags are special since they consist of 3 values,\\n            # so we need an extra join here. Note that empty vector flags\\n            # should NOT be sent either!\\n            builder.write(\"b\\'\\' if {0} is None or {0} is False \"\\n                          \"else b\\'\\'.join((\", name)\\n        else:\\n            builder.write(\"b\\'\\' if {0} is None or {0} is False \"\\n                          \"else (\", name)\\n\\n    if arg.is_vector:\\n        if arg.use_vector_id:\\n            # vector code, unsigned 0x1cb5c415 as little endian\\n            builder.write(r\"b\\'\\\\x15\\\\xc4\\\\xb5\\\\x1c\\',\")\\n\\n        builder.write(\"struct.pack(\\'<i\\', len({})),\", name)\\n\\n        # Cannot unpack the values for the outer tuple through *[(\\n        # since that\\'s a Python >3.5 feature, so add another join.\\n        builder.write(\"b\\'\\'.join(\")\\n\\n        # Temporary disable .is_vector, not to enter this if again\\n        # Also disable .is_flag since it\\'s not needed per element\\n        old_flag = arg.is_flag\\n        arg.is_vector = arg.is_flag = False\\n        _write_arg_to_bytes(builder, arg, args, name=\\'x\\')\\n        arg.is_vector = True\\n        arg.is_flag = old_flag\\n\\n        builder.write(\\' for x in {})\\', name)\\n\\n    elif arg.flag_indicator:\\n        # Calculate the flags with those items which are not None\\n        if not any(f.is_flag for f in args):\\n            # There\\'s a flag indicator, but no flag arguments so it\\'s 0\\n            builder.write(r\"b\\'\\\\0\\\\0\\\\0\\\\0\\'\")\\n        else:\\n            builder.write(\"struct.pack(\\'<I\\', \")\\n            builder.write(\\n                \\' | \\'.join(\\'(0 if {0} is None or {0} is False else {1})\\'\\n                           .format(\\'self.{}\\'.format(flag.name),\\n                                   1 << flag.flag_index)\\n                           for flag in args if flag.is_flag)\\n            )\\n            builder.write(\\')\\')\\n\\n    elif \\'int\\' == arg.type:\\n        # struct.pack is around 4 times faster than int.to_bytes\\n        builder.write(\"struct.pack(\\'<i\\', {})\", name)\\n\\n    elif \\'long\\' == arg.type:\\n        builder.write(\"struct.pack(\\'<q\\', {})\", name)\\n\\n    elif \\'int128\\' == arg.type:\\n        builder.write(\"{}.to_bytes(16, \\'little\\', signed=True)\", name)\\n\\n    elif \\'int256\\' == arg.type:\\n        builder.write(\"{}.to_bytes(32, \\'little\\', signed=True)\", name)\\n\\n    elif \\'double\\' == arg.type:\\n        builder.write(\"struct.pack(\\'<d\\', {})\", name)\\n\\n    elif \\'string\\' == arg.type:\\n        builder.write(\\'self.serialize_bytes({})\\', name)\\n\\n    elif \\'Bool\\' == arg.type:\\n        # 0x997275b5 if boolean else 0xbc799737\\n        builder.write(r\"b\\'\\\\xb5ur\\\\x99\\' if {} else b\\'7\\\\x97y\\\\xbc\\'\", name)\\n\\n    elif \\'true\\' == arg.type:\\n        pass  # These are actually NOT written! Only used for flags\\n\\n    elif \\'bytes\\' == arg.type:\\n        builder.write(\\'self.serialize_bytes({})\\', name)\\n\\n    elif \\'date\\' == arg.type:  # Custom format\\n        builder.write(\\'self.serialize_datetime({})\\', name)\\n\\n    else:\\n        # Else it may be a custom type\\n        builder.write(\\'bytes({})\\', name)\\n\\n        # If the type is not boxed (i.e. starts with lowercase) we should\\n        # not serialize the constructor ID (so remove its first 4 bytes).\\n        boxed = arg.type[arg.type.find(\\'.\\') + 1].isupper()\\n        if not boxed:\\n            builder.write(\\'[4:]\\')\\n\\n    if arg.is_flag:\\n        builder.write(\\')\\')\\n        if arg.is_vector:\\n            builder.write(\\')\\')  # We were using a tuple\\n\\n    return True', 'def _write_arg_read_code(builder, arg, args, name):\\n    \"\"\"\\n    Writes the read code for the given argument, setting the\\n    arg.name variable to its read value.\\n\\n    :param builder: The source code builder\\n    :param arg: The argument to write\\n    :param args: All the other arguments in TLObject same on_send.\\n                 This is required to determine the flags value\\n    :param name: The name of the argument. Defaults to \"self.argname\"\\n                 This argument is an option because it\\'s required when\\n                 writing Vectors<>\\n    \"\"\"\\n\\n    if arg.generic_definition:\\n        return  # Do nothing, this only specifies a later type\\n\\n    # The argument may be a flag, only write that flag was given!\\n    was_flag = False\\n    if arg.is_flag:\\n        # Treat \\'true\\' flags as a special case, since they\\'re true if\\n        # they\\'re set, and nothing else needs to actually be read.\\n        if \\'true\\' == arg.type:\\n            builder.writeln(\\'{} = bool(flags & {})\\',\\n                            name, 1 << arg.flag_index)\\n            return\\n\\n        was_flag = True\\n        builder.writeln(\\'if flags & {}:\\', 1 << arg.flag_index)\\n        # Temporary disable .is_flag not to enter this if\\n        # again when calling the method recursively\\n        arg.is_flag = False\\n\\n    if arg.is_vector:\\n        if arg.use_vector_id:\\n            # We have to read the vector\\'s constructor ID\\n            builder.writeln(\"reader.read_int()\")\\n\\n        builder.writeln(\\'{} = []\\', name)\\n        builder.writeln(\\'for _ in range(reader.read_int()):\\')\\n        # Temporary disable .is_vector, not to enter this if again\\n        arg.is_vector = False\\n        _write_arg_read_code(builder, arg, args, name=\\'_x\\')\\n        builder.writeln(\\'{}.append(_x)\\', name)\\n        arg.is_vector = True\\n\\n    elif arg.flag_indicator:\\n        # Read the flags, which will indicate what items we should read next\\n        builder.writeln(\\'flags = reader.read_int()\\')\\n        builder.writeln()\\n\\n    elif \\'int\\' == arg.type:\\n        builder.writeln(\\'{} = reader.read_int()\\', name)\\n\\n    elif \\'long\\' == arg.type:\\n        builder.writeln(\\'{} = reader.read_long()\\', name)\\n\\n    elif \\'int128\\' == arg.type:\\n        builder.writeln(\\'{} = reader.read_large_int(bits=128)\\', name)\\n\\n    elif \\'int256\\' == arg.type:\\n        builder.writeln(\\'{} = reader.read_large_int(bits=256)\\', name)\\n\\n    elif \\'double\\' == arg.type:\\n        builder.writeln(\\'{} = reader.read_double()\\', name)\\n\\n    elif \\'string\\' == arg.type:\\n        builder.writeln(\\'{} = reader.tgread_string()\\', name)\\n\\n    elif \\'Bool\\' == arg.type:\\n        builder.writeln(\\'{} = reader.tgread_bool()\\', name)\\n\\n    elif \\'true\\' == arg.type:\\n        # Arbitrary not-None value, don\\'t actually read \"true\" flags\\n        builder.writeln(\\'{} = True\\', name)\\n\\n    elif \\'bytes\\' == arg.type:\\n        builder.writeln(\\'{} = reader.tgread_bytes()\\', name)\\n\\n    elif \\'date\\' == arg.type:  # Custom format\\n        builder.writeln(\\'{} = reader.tgread_date()\\', name)\\n\\n    else:\\n        # Else it may be a custom type\\n        if not arg.skip_constructor_id:\\n            builder.writeln(\\'{} = reader.tgread_object()\\', name)\\n        else:\\n            # Import the correct type inline to avoid cyclic imports.\\n            # There may be better solutions so that we can just access\\n            # all the types before the files have been parsed, but I\\n            # don\\'t know of any.\\n            sep_index = arg.type.find(\\'.\\')\\n            if sep_index == -1:\\n                ns, t = \\'.\\', arg.type\\n            else:\\n                ns, t = \\'.\\' + arg.type[:sep_index], arg.type[sep_index+1:]\\n            class_name = snake_to_camel_case(t)\\n\\n            # There would be no need to import the type if we\\'re in the\\n            # file with the same namespace, but since it does no harm\\n            # and we don\\'t have information about such thing in the\\n            # method we just ignore that case.\\n            builder.writeln(\\'from {} import {}\\', ns, class_name)\\n            builder.writeln(\\'{} = {}.from_reader(reader)\\',\\n                            name, class_name)\\n\\n    # End vector and flag blocks if required (if we opened them before)\\n    if arg.is_vector:\\n        builder.end_block()\\n\\n    if was_flag:\\n        builder.current_indent -= 1\\n        builder.writeln(\\'else:\\')\\n        builder.writeln(\\'{} = None\\', name)\\n        builder.current_indent -= 1\\n        # Restore .is_flag\\n        arg.is_flag = True', 'def rpc_message_to_error(rpc_error, request):\\n    \"\"\"\\n    Converts a Telegram\\'s RPC Error to a Python error.\\n\\n    :param rpc_error: the RpcError instance.\\n    :param request: the request that caused this error.\\n    :return: the RPCError as a Python exception that represents this error.\\n    \"\"\"\\n    # Try to get the error by direct look-up, otherwise regex\\n    cls = rpc_errors_dict.get(rpc_error.error_message, None)\\n    if cls:\\n        return cls(request)\\n\\n    for msg_regex, cls in rpc_errors_re:\\n        m = re.match(msg_regex, rpc_error.error_message)\\n        if m:\\n            capture = int(m.group(1)) if m.groups() else None\\n            return cls(request, capture=capture)\\n\\n    # Some errors are negative:\\n    # * -500 for \"No workers running\",\\n    # * -503 for \"Timeout\"\\n    #\\n    # We treat them as if they were positive, so -500 will be treated\\n    # as a `ServerError`, etc.\\n    cls = base_errors.get(abs(rpc_error.error_code))\\n    if cls:\\n        return cls(request, rpc_error.error_message)\\n\\n    return RPCError(request, rpc_error.error_message, rpc_error.error_code)', 'def _is_inline(button):\\n        \"\"\"\\n        Returns ``True`` if the button belongs to an inline keyboard.\\n        \"\"\"\\n        return isinstance(button, (\\n            types.KeyboardButtonCallback,\\n            types.KeyboardButtonSwitchInline,\\n            types.KeyboardButtonUrl\\n        ))', 'def inline(text, data=None):\\n        \"\"\"\\n        Creates a new inline button.\\n\\n        If `data` is omitted, the given `text` will be used as `data`.\\n        In any case `data` should be either ``bytes`` or ``str``.\\n\\n        Note that the given `data` must be less or equal to 64 bytes.\\n        If more than 64 bytes are passed as data, ``ValueError`` is raised.\\n        \"\"\"\\n        if not data:\\n            data = text.encode(\\'utf-8\\')\\n        elif not isinstance(data, (bytes, bytearray, memoryview)):\\n            data = str(data).encode(\\'utf-8\\')\\n\\n        if len(data) > 64:\\n            raise ValueError(\\'Too many bytes for the data\\')\\n\\n        return types.KeyboardButtonCallback(text, data)', 'def switch_inline(text, query=\\'\\', same_peer=False):\\n        \"\"\"\\n        Creates a new button to switch to inline query.\\n\\n        If `query` is given, it will be the default text to be used\\n        when making the inline query.\\n\\n        If ``same_peer is True`` the inline query will directly be\\n        set under the currently opened chat. Otherwise, the user will\\n        have to select a different dialog to make the query.\\n        \"\"\"\\n        return types.KeyboardButtonSwitchInline(text, query, same_peer)', 'def text(cls, text, *, resize=None, single_use=None, selective=None):\\n        \"\"\"\\n        Creates a new button with the given text.\\n\\n        Args:\\n            resize (`bool`):\\n                If present, the entire keyboard will be reconfigured to\\n                be resized and be smaller if there are not many buttons.\\n\\n            single_use (`bool`):\\n                If present, the entire keyboard will be reconfigured to\\n                be usable only once before it hides itself.\\n\\n            selective (`bool`):\\n                If present, the entire keyboard will be reconfigured to\\n                be \"selective\". The keyboard will be shown only to specific\\n                users. It will target users that are @mentioned in the text\\n                of the message or to the sender of the message you reply to.\\n        \"\"\"\\n        return cls(types.KeyboardButton(text),\\n                   resize=resize, single_use=single_use, selective=selective)', 'def request_location(cls, text, *,\\n                         resize=None, single_use=None, selective=None):\\n        \"\"\"\\n        Creates a new button that will request\\n        the user\\'s location upon being clicked.\\n\\n        ``resize``, ``single_use`` and ``selective`` are documented in `text`.\\n        \"\"\"\\n        return cls(types.KeyboardButtonRequestGeoLocation(text),\\n                   resize=resize, single_use=single_use, selective=selective)', 'def request_phone(cls, text, *,\\n                      resize=None, single_use=None, selective=None):\\n        \"\"\"\\n        Creates a new button that will request\\n        the user\\'s phone number upon being clicked.\\n\\n        ``resize``, ``single_use`` and ``selective`` are documented in `text`.\\n        \"\"\"\\n        return cls(types.KeyboardButtonRequestPhone(text),\\n                   resize=resize, single_use=single_use, selective=selective)', 'def sprint(string, *args, **kwargs):\\n    \"\"\"Safe Print (handle UnicodeEncodeErrors on some terminals)\"\"\"\\n    try:\\n        print(string, *args, **kwargs)\\n    except UnicodeEncodeError:\\n        string = string.encode(\\'utf-8\\', errors=\\'ignore\\')\\\\\\n                       .decode(\\'ascii\\', errors=\\'ignore\\')\\n        print(string, *args, **kwargs)', 'def print_title(title):\\n    \"\"\"Helper function to print titles to the console more nicely\"\"\"\\n    sprint(\\'\\\\n\\')\\n    sprint(\\'=={}==\\'.format(\\'=\\' * len(title)))\\n    sprint(\\'= {} =\\'.format(title))\\n    sprint(\\'=={}==\\'.format(\\'=\\' * len(title)))', 'async def async_input(prompt):\\n    \"\"\"\\n    Python\\'s ``input()`` is blocking, which means the event loop we set\\n    above can\\'t be running while we\\'re blocking there. This method will\\n    let the loop run while we wait for input.\\n    \"\"\"\\n    print(prompt, end=\\'\\', flush=True)\\n    return (await loop.run_in_executor(None, sys.stdin.readline)).rstrip()', 'async def run(self):\\n        \"\"\"Main loop of the TelegramClient, will wait for user action\"\"\"\\n\\n        # Once everything is ready, we can add an event handler.\\n        #\\n        # Events are an abstraction over Telegram\\'s \"Updates\" and\\n        # are much easier to use.\\n        self.add_event_handler(self.message_handler, events.NewMessage)\\n\\n        # Enter a while loop to chat as long as the user wants\\n        while True:\\n            # Retrieve the top dialogs. You can set the limit to None to\\n            # retrieve all of them if you wish, but beware that may take\\n            # a long time if you have hundreds of them.\\n            dialog_count = 15\\n\\n            # Entities represent the user, chat or channel\\n            # corresponding to the dialog on the same index.\\n            dialogs = await self.get_dialogs(limit=dialog_count)\\n\\n            i = None\\n            while i is None:\\n                print_title(\\'Dialogs window\\')\\n\\n                # Display them so the user can choose\\n                for i, dialog in enumerate(dialogs, start=1):\\n                    sprint(\\'{}. {}\\'.format(i, get_display_name(dialog.entity)))\\n\\n                # Let the user decide who they want to talk to\\n                print()\\n                print(\\'> Who do you want to send messages to?\\')\\n                print(\\'> Available commands:\\')\\n                print(\\'  !q: Quits the dialogs window and exits.\\')\\n                print(\\'  !l: Logs out, terminating this session.\\')\\n                print()\\n                i = await async_input(\\'Enter dialog ID or a command: \\')\\n                if i == \\'!q\\':\\n                    return\\n                if i == \\'!l\\':\\n                    # Logging out will cause the user to need to reenter the\\n                    # code next time they want to use the library, and will\\n                    # also delete the *.session file off the filesystem.\\n                    #\\n                    # This is not the same as simply calling .disconnect(),\\n                    # which simply shuts down everything gracefully.\\n                    await self.log_out()\\n                    return\\n\\n                try:\\n                    i = int(i if i else 0) - 1\\n                    # Ensure it is inside the bounds, otherwise retry\\n                    if not 0 <= i < dialog_count:\\n                        i = None\\n                except ValueError:\\n                    i = None\\n\\n            # Retrieve the selected user (or chat, or channel)\\n            entity = dialogs[i].entity\\n\\n            # Show some information\\n            print_title(\\'Chat with \"{}\"\\'.format(get_display_name(entity)))\\n            print(\\'Available commands:\\')\\n            print(\\'  !q:  Quits the current chat.\\')\\n            print(\\'  !Q:  Quits the current chat and exits.\\')\\n            print(\\'  !h:  prints the latest messages (message History).\\')\\n            print(\\'  !up  <path>: Uploads and sends the Photo from path.\\')\\n            print(\\'  !uf  <path>: Uploads and sends the File from path.\\')\\n            print(\\'  !d   <msg-id>: Deletes a message by its id\\')\\n            print(\\'  !dm  <msg-id>: Downloads the given message Media (if any).\\')\\n            print(\\'  !dp: Downloads the current dialog Profile picture.\\')\\n            print(\\'  !i:  Prints information about this chat..\\')\\n            print()\\n\\n            # And start a while loop to chat\\n            while True:\\n                msg = await async_input(\\'Enter a message: \\')\\n                # Quit\\n                if msg == \\'!q\\':\\n                    break\\n                elif msg == \\'!Q\\':\\n                    return\\n\\n                # History\\n                elif msg == \\'!h\\':\\n                    # First retrieve the messages and some information\\n                    messages = await self.get_messages(entity, limit=10)\\n\\n                    # Iterate over all (in reverse order so the latest appear\\n                    # the last in the console) and print them with format:\\n                    # \"[hh:mm] Sender: Message\"\\n                    for msg in reversed(messages):\\n                        # Note how we access .sender here. Since we made an\\n                        # API call using the self client, it will always have\\n                        # information about the sender. This is different to\\n                        # events, where Telegram may not always send the user.\\n                        name = get_display_name(msg.sender)\\n\\n                        # Format the message content\\n                        if getattr(msg, \\'media\\', None):\\n                            self.found_media[msg.id] = msg\\n                            content = \\'<{}> {}\\'.format(\\n                                type(msg.media).__name__, msg.message)\\n\\n                        elif hasattr(msg, \\'message\\'):\\n                            content = msg.message\\n                        elif hasattr(msg, \\'action\\'):\\n                            content = str(msg.action)\\n                        else:\\n                            # Unknown message, simply print its class name\\n                            content = type(msg).__name__\\n\\n                        # And print it to the user\\n                        sprint(\\'[{}:{}] (ID={}) {}: {}\\'.format(\\n                            msg.date.hour, msg.date.minute, msg.id, name, content))\\n\\n                # Send photo\\n                elif msg.startswith(\\'!up \\'):\\n                    # Slice the message to get the path\\n                    path = msg[len(\\'!up \\'):]\\n                    await self.send_photo(path=path, entity=entity)\\n\\n                # Send file (document)\\n                elif msg.startswith(\\'!uf \\'):\\n                    # Slice the message to get the path\\n                    path = msg[len(\\'!uf \\'):]\\n                    await self.send_document(path=path, entity=entity)\\n\\n                # Delete messages\\n                elif msg.startswith(\\'!d \\'):\\n                    # Slice the message to get message ID\\n                    msg = msg[len(\\'!d \\'):]\\n                    deleted_msg = await self.delete_messages(entity, msg)\\n                    print(\\'Deleted {}\\'.format(deleted_msg))\\n\\n                # Download media\\n                elif msg.startswith(\\'!dm \\'):\\n                    # Slice the message to get message ID\\n                    await self.download_media_by_id(msg[len(\\'!dm \\'):])\\n\\n                # Download profile photo\\n                elif msg == \\'!dp\\':\\n                    print(\\'Downloading profile picture to usermedia/...\\')\\n                    os.makedirs(\\'usermedia\\', exist_ok=True)\\n                    output = await self.download_profile_photo(entity,\\n                                                               \\'usermedia\\')\\n                    if output:\\n                        print(\\'Profile picture downloaded to\\', output)\\n                    else:\\n                        print(\\'No profile picture found for this user!\\')\\n\\n                elif msg == \\'!i\\':\\n                    attributes = list(entity.to_dict().items())\\n                    pad = max(len(x) for x, _ in attributes)\\n                    for name, val in attributes:\\n                        print(\"{:<{width}} : {}\".format(name, val, width=pad))\\n\\n                # Send chat message (if any)\\n                elif msg:\\n                    await self.send_message(entity, msg, link_preview=False)', 'async def send_photo(self, path, entity):\\n        \"\"\"Sends the file located at path to the desired entity as a photo\"\"\"\\n        await self.send_file(\\n            entity, path,\\n            progress_callback=self.upload_progress_callback\\n        )\\n        print(\\'Photo sent!\\')', 'async def send_document(self, path, entity):\\n        \"\"\"Sends the file located at path to the desired entity as a document\"\"\"\\n        await self.send_file(\\n            entity, path,\\n            force_document=True,\\n            progress_callback=self.upload_progress_callback\\n        )\\n        print(\\'Document sent!\\')', 'async def download_media_by_id(self, media_id):\\n        \"\"\"Given a message ID, finds the media this message contained and\\n           downloads it.\\n        \"\"\"\\n        try:\\n            msg = self.found_media[int(media_id)]\\n        except (ValueError, KeyError):\\n            # ValueError when parsing, KeyError when accessing dictionary\\n            print(\\'Invalid media ID given or message not found!\\')\\n            return\\n\\n        print(\\'Downloading media to usermedia/...\\')\\n        os.makedirs(\\'usermedia\\', exist_ok=True)\\n        output = await self.download_media(\\n            msg.media,\\n            file=\\'usermedia/\\',\\n            progress_callback=self.download_progress_callback\\n        )\\n        print(\\'Media downloaded to {}!\\'.format(output))', 'async def message_handler(self, event):\\n        \"\"\"Callback method for received events.NewMessage\"\"\"\\n\\n        # Note that message_handler is called when a Telegram update occurs\\n        # and an event is created. Telegram may not always send information\\n        # about the ``.sender`` or the ``.chat``, so if you *really* want it\\n        # you should use ``get_chat()`` and ``get_sender()`` while working\\n        # with events. Since they are methods, you know they may make an API\\n        # call, which can be expensive.\\n        chat = await event.get_chat()\\n        if event.is_group:\\n            if event.out:\\n                sprint(\\'>> sent \"{}\" to chat {}\\'.format(\\n                    event.text, get_display_name(chat)\\n                ))\\n            else:\\n                sprint(\\'<< {} @ {} sent \"{}\"\\'.format(\\n                    get_display_name(await event.get_sender()),\\n                    get_display_name(chat),\\n                    event.text\\n                ))\\n        else:\\n            if event.out:\\n                sprint(\\'>> \"{}\" to user {}\\'.format(\\n                    event.text, get_display_name(chat)\\n                ))\\n            else:\\n                sprint(\\'<< {} sent \"{}\"\\'.format(\\n                    get_display_name(chat), event.text\\n                ))', 'def _get_file_name(tlobject):\\n    \"\"\"``ClassName -> class_name.html``.\"\"\"\\n    name = tlobject.name if isinstance(tlobject, TLObject) else tlobject\\n    # Courtesy of http://stackoverflow.com/a/1176023/4759433\\n    s1 = re.sub(\\'(.)([A-Z][a-z]+)\\', r\\'\\\\1_\\\\2\\', name)\\n    result = re.sub(\\'([a-z0-9])([A-Z])\\', r\\'\\\\1_\\\\2\\', s1).lower()\\n    return \\'{}.html\\'.format(result)', 'def get_import_code(tlobject):\\n    \"\"\"``TLObject -> from ... import ...``.\"\"\"\\n    kind = \\'functions\\' if tlobject.is_function else \\'types\\'\\n    ns = \\'.\\' + tlobject.namespace if tlobject.namespace else \\'\\'\\n    return \\'from telethon.tl.{}{} import {}\\'\\\\\\n        .format(kind, ns, tlobject.class_name)', 'def _get_path_for(root, tlobject):\\n    \"\"\"Creates and returns the path for the given TLObject at root.\"\"\"\\n    out_dir = root / (\\'methods\\' if tlobject.is_function else \\'constructors\\')\\n    if tlobject.namespace:\\n        out_dir /= tlobject.namespace\\n\\n    return out_dir / _get_file_name(tlobject)', 'def _get_path_for_type(type_):\\n    \"\"\"Similar to `_get_path_for` but for only type names.\"\"\"\\n    if type_.lower() in CORE_TYPES:\\n        return Path(\\'index.html#%s\\' % type_.lower())\\n    elif \\'.\\' in type_:\\n        namespace, name = type_.split(\\'.\\')\\n        return Path(\\'types\\', namespace, _get_file_name(name))\\n    else:\\n        return Path(\\'types\\',  _get_file_name(type_))', 'def _find_title(html_file):\\n    \"\"\"Finds the <title> for the given HTML file, or (Unknown).\"\"\"\\n    # TODO Is it necessary to read files like this?\\n    with html_file.open() as f:\\n        for line in f:\\n            if \\'<title>\\' in line:\\n                # + 7 to skip len(\\'<title>\\')\\n                return line[line.index(\\'<title>\\') + 7:line.index(\\'</title>\\')]\\n\\n    return \\'(Unknown)\\'', 'def _build_menu(docs):\\n    \"\"\"\\n    Builds the menu used for the current ``DocumentWriter``.\\n    \"\"\"\\n\\n    paths = []\\n    current = docs.filename\\n    while current != docs.root:\\n        current = current.parent\\n        paths.append(current)\\n\\n    for path in reversed(paths):\\n        docs.add_menu(path.stem.title(), link=path / \\'index.html\\')\\n\\n    if docs.filename.stem != \\'index\\':\\n        docs.add_menu(docs.title, link=docs.filename)\\n\\n    docs.end_menu()', 'def _generate_index(root, folder, paths,\\n                    bots_index=False, bots_index_paths=()):\\n    \"\"\"Generates the index file for the specified folder\"\"\"\\n    # Determine the namespaces listed here (as sub folders)\\n    # and the files (.html files) that we should link to\\n    namespaces = []\\n    files = []\\n    INDEX = \\'index.html\\'\\n    BOT_INDEX = \\'botindex.html\\'\\n\\n    for item in (bots_index_paths or folder.iterdir()):\\n        if item.is_dir():\\n            namespaces.append(item)\\n        elif item.name not in (INDEX, BOT_INDEX):\\n            files.append(item)\\n\\n    # Now that everything is setup, write the index.html file\\n    filename = folder / (BOT_INDEX if bots_index else INDEX)\\n    with DocsWriter(root, filename, _get_path_for_type) as docs:\\n        # Title should be the current folder name\\n        docs.write_head(str(folder).replace(os.path.sep, \\'/\\').title(),\\n                        css_path=paths[\\'css\\'],\\n                        default_css=paths[\\'default_css\\'])\\n\\n        docs.set_menu_separator(paths[\\'arrow\\'])\\n        _build_menu(docs)\\n        docs.write_title(str(filename.parent.relative_to(root))\\n                         .replace(os.path.sep, \\'/\\').title())\\n\\n        if bots_index:\\n            docs.write_text(\\'These are the methods that you may be able to \\'\\n                            \\'use as a bot. Click <a href=\"{}\">here</a> to \\'\\n                            \\'view them all.\\'.format(INDEX))\\n        else:\\n            docs.write_text(\\'Click <a href=\"{}\">here</a> to view the methods \\'\\n                            \\'that you can use as a bot.\\'.format(BOT_INDEX))\\n        if namespaces:\\n            docs.write_title(\\'Namespaces\\', level=3)\\n            docs.begin_table(4)\\n            namespaces.sort()\\n            for namespace in namespaces:\\n                # For every namespace, also write the index of it\\n                namespace_paths = []\\n                if bots_index:\\n                    for item in bots_index_paths:\\n                        if item.parent == namespace:\\n                            namespace_paths.append(item)\\n\\n                _generate_index(root, namespace, paths,\\n                                bots_index, namespace_paths)\\n\\n                docs.add_row(\\n                    namespace.stem.title(),\\n                    link=namespace / (BOT_INDEX if bots_index else INDEX))\\n\\n            docs.end_table()\\n\\n        docs.write_title(\\'Available items\\')\\n        docs.begin_table(2)\\n\\n        files = [(f, _find_title(f)) for f in files]\\n        files.sort(key=lambda t: t[1])\\n\\n        for file, title in files:\\n            docs.add_row(title, link=file)\\n\\n        docs.end_table()\\n        docs.end_body()', 'def _get_description(arg):\\n    \"\"\"Generates a proper description for the given argument.\"\"\"\\n    desc = []\\n    otherwise = False\\n    if arg.can_be_inferred:\\n        desc.append(\\'If left unspecified, it will be inferred automatically.\\')\\n        otherwise = True\\n    elif arg.is_flag:\\n        desc.append(\\'This argument defaults to \\'\\n                    \\'<code>None</code> and can be omitted.\\')\\n        otherwise = True\\n\\n    if arg.type in {\\'InputPeer\\', \\'InputUser\\', \\'InputChannel\\',\\n                    \\'InputNotifyPeer\\', \\'InputDialogPeer\\'}:\\n        desc.append(\\n            \\'Anything entity-like will work if the library can find its \\'\\n            \\'<code>Input</code> version (e.g., usernames, <code>Peer</code>, \\'\\n            \\'<code>User</code> or <code>Channel</code> objects, etc.).\\'\\n        )\\n\\n    if arg.is_vector:\\n        if arg.is_generic:\\n            desc.append(\\'A list of other Requests must be supplied.\\')\\n        else:\\n            desc.append(\\'A list must be supplied.\\')\\n    elif arg.is_generic:\\n        desc.append(\\'A different Request must be supplied for this argument.\\')\\n    else:\\n        otherwise = False  # Always reset to false if no other text is added\\n\\n    if otherwise:\\n        desc.insert(1, \\'Otherwise,\\')\\n        desc[-1] = desc[-1][:1].lower() + desc[-1][1:]\\n\\n    return \\' \\'.join(desc).replace(\\n        \\'list\\',\\n        \\'<span class=\"tooltip\" title=\"Any iterable that supports len() \\'\\n        \\'will work too\">list</span>\\'\\n    )', 'def _copy_replace(src, dst, replacements):\\n    \"\"\"Copies the src file into dst applying the replacements dict\"\"\"\\n    with src.open() as infile, dst.open(\\'w\\') as outfile:\\n        outfile.write(re.sub(\\n            \\'|\\'.join(re.escape(k) for k in replacements),\\n            lambda m: str(replacements[m.group(0)]),\\n            infile.read()\\n        ))', 'def _write_html_pages(root, tlobjects, methods, layer, input_res):\\n    \"\"\"\\n    Generates the documentation HTML files from from ``scheme.tl``\\n    to ``/methods`` and ``/constructors``, etc.\\n    \"\"\"\\n    # Save \\'Type: [Constructors]\\' for use in both:\\n    # * Seeing the return type or constructors belonging to the same type.\\n    # * Generating the types documentation, showing available constructors.\\n    paths = {k: root / v for k, v in (\\n        (\\'css\\', \\'css\\'),\\n        (\\'arrow\\', \\'img/arrow.svg\\'),\\n        (\\'search.js\\', \\'js/search.js\\'),\\n        (\\'404\\', \\'404.html\\'),\\n        (\\'index_all\\', \\'index.html\\'),\\n        (\\'bot_index\\', \\'botindex.html\\'),\\n        (\\'index_types\\', \\'types/index.html\\'),\\n        (\\'index_methods\\', \\'methods/index.html\\'),\\n        (\\'index_constructors\\', \\'constructors/index.html\\')\\n    )}\\n    paths[\\'default_css\\'] = \\'light\\'  # docs.<name>.css, local path\\n    type_to_constructors = defaultdict(list)\\n    type_to_functions = defaultdict(list)\\n    for tlobject in tlobjects:\\n        d = type_to_functions if tlobject.is_function else type_to_constructors\\n        d[tlobject.result].append(tlobject)\\n\\n    for t, cs in type_to_constructors.items():\\n        type_to_constructors[t] = list(sorted(cs, key=lambda c: c.name))\\n\\n    methods = {m.name: m for m in methods}\\n\\n    # Since the output directory is needed everywhere partially apply it now\\n    create_path_for = functools.partial(_get_path_for, root)\\n    path_for_type = lambda t: root / _get_path_for_type(t)\\n    bot_docs_paths = []\\n\\n    for tlobject in tlobjects:\\n        filename = create_path_for(tlobject)\\n        with DocsWriter(root, filename, path_for_type) as docs:\\n            docs.write_head(title=tlobject.class_name,\\n                            css_path=paths[\\'css\\'],\\n                            default_css=paths[\\'default_css\\'])\\n\\n            # Create the menu (path to the current TLObject)\\n            docs.set_menu_separator(paths[\\'arrow\\'])\\n            _build_menu(docs)\\n\\n            # Create the page title\\n            docs.write_title(tlobject.class_name)\\n\\n            if tlobject.is_function:\\n                if tlobject.usability == Usability.USER:\\n                    start = \\'<strong>Only users</strong> can\\'\\n                elif tlobject.usability == Usability.BOT:\\n                    bot_docs_paths.append(filename)\\n                    start = \\'<strong>Only bots</strong> can\\'\\n                elif tlobject.usability == Usability.BOTH:\\n                    bot_docs_paths.append(filename)\\n                    start = \\'<strong>Both users and bots</strong> can\\'\\n                else:\\n                    bot_docs_paths.append(filename)\\n                    start = \\\\\\n                        \\'Both users and bots <strong>may</strong> be able to\\'\\n\\n                docs.write_text(\\'{} use this method. <a href=\"#examples\">\\'\\n                                \\'See code examples.</a>\\'.format(start))\\n\\n            # Write the code definition for this TLObject\\n            docs.write_code(tlobject)\\n            docs.write_copy_button(\\'Copy import to the clipboard\\',\\n                                   get_import_code(tlobject))\\n\\n            # Write the return type (or constructors belonging to the same type)\\n            docs.write_title(\\'Returns\\' if tlobject.is_function\\n                             else \\'Belongs to\\', level=3)\\n\\n            generic_arg = next((arg.name for arg in tlobject.args\\n                                if arg.generic_definition), None)\\n\\n            if tlobject.result == generic_arg:\\n                # We assume it\\'s a function returning a generic type\\n                generic_arg = next((arg.name for arg in tlobject.args\\n                                    if arg.is_generic))\\n                docs.write_text(\\'This function returns the result of whatever \\'\\n                                \\'the result from invoking the request passed \\'\\n                                \\'through <i>{}</i> is.\\'.format(generic_arg))\\n            else:\\n                if re.search(\\'^vector<\\', tlobject.result, re.IGNORECASE):\\n                    docs.write_text(\\'A list of the following type is returned.\\')\\n                    _, inner = tlobject.result.split(\\'<\\')\\n                    inner = inner.strip(\\'>\\')\\n                else:\\n                    inner = tlobject.result\\n\\n                docs.begin_table(column_count=1)\\n                docs.add_row(inner, link=path_for_type(inner))\\n                docs.end_table()\\n\\n                cs = type_to_constructors.get(inner, [])\\n                if not cs:\\n                    docs.write_text(\\'This type has no instances available.\\')\\n                elif len(cs) == 1:\\n                    docs.write_text(\\'This type can only be an instance of:\\')\\n                else:\\n                    docs.write_text(\\'This type can be an instance of either:\\')\\n\\n                docs.begin_table(column_count=2)\\n                for constructor in cs:\\n                    link = create_path_for(constructor)\\n                    docs.add_row(constructor.class_name, link=link)\\n                docs.end_table()\\n\\n            # Return (or similar types) written. Now parameters/members\\n            docs.write_title(\\n                \\'Parameters\\' if tlobject.is_function else \\'Members\\', level=3\\n            )\\n\\n            # Sort the arguments in the same way they\\'re sorted\\n            # on the generated code (flags go last)\\n            args = [\\n                a for a in tlobject.sorted_args()\\n                if not a.flag_indicator and not a.generic_definition\\n            ]\\n\\n            if args:\\n                # Writing parameters\\n                docs.begin_table(column_count=3)\\n\\n                for arg in args:\\n                    # Name row\\n                    docs.add_row(arg.name,\\n                                 bold=True)\\n\\n                    # Type row\\n                    friendly_type = \\'flag\\' if arg.type == \\'true\\' else arg.type\\n                    if arg.is_generic:\\n                        docs.add_row(\\'!\\' + friendly_type, align=\\'center\\')\\n                    else:\\n                        docs.add_row(\\n                            friendly_type, align=\\'center\\',\\n                            link=path_for_type(arg.type)\\n                         )\\n\\n                    # Add a description for this argument\\n                    docs.add_row(_get_description(arg))\\n\\n                docs.end_table()\\n            else:\\n                if tlobject.is_function:\\n                    docs.write_text(\\'This request takes no input parameters.\\')\\n                else:\\n                    docs.write_text(\\'This type has no members.\\')\\n\\n            if tlobject.is_function:\\n                docs.write_title(\\'Known RPC errors\\')\\n                method_info = methods.get(tlobject.fullname)\\n                errors = method_info and method_info.errors\\n                if not errors:\\n                    docs.write_text(\"This request can\\'t cause any RPC error \"\\n                                    \"as far as we know.\")\\n                else:\\n                    docs.write_text(\\n                        \\'This request can cause {} known error{}:\\'.format(\\n                            len(errors), \\'\\' if len(errors) == 1 else \\'s\\'\\n                    ))\\n                    docs.begin_table(column_count=2)\\n                    for error in errors:\\n                        docs.add_row(\\'<code>{}</code>\\'.format(error.name))\\n                        docs.add_row(\\'{}.\\'.format(error.description))\\n                    docs.end_table()\\n                    docs.write_text(\\'You can import these from \\'\\n                                    \\'<code>telethon.errors</code>.\\')\\n\\n                docs.write_title(\\'Example\\', id=\\'examples\\')\\n                docs.write(\\'\\'\\'<pre>\\\\\\n<strong>from</strong> telethon.sync <strong>import</strong> TelegramClient\\n<strong>from</strong> telethon <strong>import</strong> functions, types\\n\\n<strong>with</strong> TelegramClient(name, api_id, api_hash) <strong>as</strong> client:\\n    result = client(\\'\\'\\')\\n                tlobject.as_example(docs, indent=1)\\n                docs.write(\\')\\\\n\\')\\n                if tlobject.result.startswith(\\'Vector\\'):\\n                    docs.write(\\'\\'\\'\\\\\\n    <strong>for</strong> x <strong>in</strong> result:\\n        print(x\\'\\'\\')\\n                else:\\n                    docs.write(\\'    print(result\\')\\n                    if tlobject.result != \\'Bool\\' \\\\\\n                            and not tlobject.result.startswith(\\'Vector\\'):\\n                        docs.write(\\'.stringify()\\')\\n\\n                docs.write(\\')</pre>\\')\\n\\n            depth = \\'../\\' * (2 if tlobject.namespace else 1)\\n            docs.add_script(src=\\'prependPath = \"{}\";\\'.format(depth))\\n            docs.add_script(path=paths[\\'search.js\\'])\\n            docs.end_body()\\n\\n    # Find all the available types (which are not the same as the constructors)\\n    # Each type has a list of constructors associated to it, hence is a map\\n    for t, cs in type_to_constructors.items():\\n        filename = path_for_type(t)\\n        out_dir = filename.parent\\n        if out_dir:\\n            out_dir.mkdir(parents=True, exist_ok=True)\\n\\n        # Since we don\\'t have access to the full TLObject, split the type\\n        if \\'.\\' in t:\\n            namespace, name = t.split(\\'.\\')\\n        else:\\n            namespace, name = None, t\\n\\n        with DocsWriter(root, filename, path_for_type) as docs:\\n            docs.write_head(title=snake_to_camel_case(name),\\n                            css_path=paths[\\'css\\'],\\n                            default_css=paths[\\'default_css\\'])\\n\\n            docs.set_menu_separator(paths[\\'arrow\\'])\\n            _build_menu(docs)\\n\\n            # Main file title\\n            docs.write_title(snake_to_camel_case(name))\\n\\n            # List available constructors for this type\\n            docs.write_title(\\'Available constructors\\', level=3)\\n            if not cs:\\n                docs.write_text(\\'This type has no constructors available.\\')\\n            elif len(cs) == 1:\\n                docs.write_text(\\'This type has one constructor available.\\')\\n            else:\\n                docs.write_text(\\'This type has %d constructors available.\\' %\\n                                len(cs))\\n\\n            docs.begin_table(2)\\n            for constructor in cs:\\n                # Constructor full name\\n                link = create_path_for(constructor)\\n                docs.add_row(constructor.class_name, link=link)\\n            docs.end_table()\\n\\n            # List all the methods which return this type\\n            docs.write_title(\\'Methods returning this type\\', level=3)\\n            functions = type_to_functions.get(t, [])\\n            if not functions:\\n                docs.write_text(\\'No method returns this type.\\')\\n            elif len(functions) == 1:\\n                docs.write_text(\\'Only the following method returns this type.\\')\\n            else:\\n                docs.write_text(\\n                    \\'The following %d methods return this type as a result.\\' %\\n                    len(functions)\\n                )\\n\\n            docs.begin_table(2)\\n            for func in functions:\\n                link = create_path_for(func)\\n                docs.add_row(func.class_name, link=link)\\n            docs.end_table()\\n\\n            # List all the methods which take this type as input\\n            docs.write_title(\\'Methods accepting this type as input\\', level=3)\\n            other_methods = sorted(\\n                (u for u in tlobjects\\n                 if any(a.type == t for a in u.args) and u.is_function),\\n                key=lambda u: u.name\\n            )\\n            if not other_methods:\\n                docs.write_text(\\n                    \\'No methods accept this type as an input parameter.\\')\\n            elif len(other_methods) == 1:\\n                docs.write_text(\\n                    \\'Only this method has a parameter with this type.\\')\\n            else:\\n                docs.write_text(\\n                    \\'The following %d methods accept this type as an input \\'\\n                    \\'parameter.\\' % len(other_methods))\\n\\n            docs.begin_table(2)\\n            for ot in other_methods:\\n                link = create_path_for(ot)\\n                docs.add_row(ot.class_name, link=link)\\n            docs.end_table()\\n\\n            # List every other type which has this type as a member\\n            docs.write_title(\\'Other types containing this type\\', level=3)\\n            other_types = sorted(\\n                (u for u in tlobjects\\n                 if any(a.type == t for a in u.args) and not u.is_function),\\n                key=lambda u: u.name\\n            )\\n\\n            if not other_types:\\n                docs.write_text(\\n                    \\'No other types have a member of this type.\\')\\n            elif len(other_types) == 1:\\n                docs.write_text(\\n                    \\'You can find this type as a member of this other type.\\')\\n            else:\\n                docs.write_text(\\n                    \\'You can find this type as a member of any of \\'\\n                    \\'the following %d types.\\' % len(other_types))\\n\\n            docs.begin_table(2)\\n            for ot in other_types:\\n                link = create_path_for(ot)\\n                docs.add_row(ot.class_name, link=link)\\n            docs.end_table()\\n            docs.end_body()\\n\\n    # After everything\\'s been written, generate an index.html per folder.\\n    # This will be done automatically and not taking into account any extra\\n    # information that we have available, simply a file listing all the others\\n    # accessible by clicking on their title\\n    for folder in [\\'types\\', \\'methods\\', \\'constructors\\']:\\n        _generate_index(root, root / folder, paths)\\n\\n    _generate_index(root, root / \\'methods\\', paths, True,\\n                    bot_docs_paths)\\n\\n    # Write the final core index, the main index for the rest of files\\n    types = set()\\n    methods = []\\n    cs = []\\n    for tlobject in tlobjects:\\n        if tlobject.is_function:\\n            methods.append(tlobject)\\n        else:\\n            cs.append(tlobject)\\n\\n        if not tlobject.result.lower() in CORE_TYPES:\\n            if re.search(\\'^vector<\\', tlobject.result, re.IGNORECASE):\\n                types.add(tlobject.result.split(\\'<\\')[1].strip(\\'>\\'))\\n            else:\\n                types.add(tlobject.result)\\n\\n    types = sorted(types)\\n    methods = sorted(methods, key=lambda m: m.name)\\n    cs = sorted(cs, key=lambda c: c.name)\\n\\n    shutil.copy(str(input_res / \\'404.html\\'), str(paths[\\'404\\']))\\n    _copy_replace(input_res / \\'core.html\\', paths[\\'index_all\\'], {\\n        \\'{type_count}\\': len(types),\\n        \\'{method_count}\\': len(methods),\\n        \\'{constructor_count}\\': len(tlobjects) - len(methods),\\n        \\'{layer}\\': layer,\\n    })\\n\\n    def fmt(xs):\\n        zs = {}  # create a dict to hold those which have duplicated keys\\n        for x in xs:\\n            zs[x.class_name] = x.class_name in zs\\n        return \\', \\'.join(\\n            \\'\"{}.{}\"\\'.format(x.namespace, x.class_name)\\n            if zs[x.class_name] and x.namespace\\n            else \\'\"{}\"\\'.format(x.class_name) for x in xs\\n        )\\n\\n    request_names = fmt(methods)\\n    constructor_names = fmt(cs)\\n\\n    def fmt(xs, formatter):\\n        return \\', \\'.join(\\'\"{}\"\\'.format(\\n            formatter(x)).replace(os.path.sep, \\'/\\') for x in xs)\\n\\n    type_names = fmt(types, formatter=lambda x: x)\\n\\n    # Local URLs shouldn\\'t rely on the output\\'s root, so set empty root\\n    get_path_for = functools.partial(_get_path_for, Path())\\n\\n    request_urls = fmt(methods, get_path_for)\\n    type_urls = fmt(types, _get_path_for_type)\\n    constructor_urls = fmt(cs, get_path_for)\\n\\n    paths[\\'search.js\\'].parent.mkdir(parents=True, exist_ok=True)\\n    _copy_replace(input_res / \\'js/search.js\\', paths[\\'search.js\\'], {\\n        \\'{request_names}\\': request_names,\\n        \\'{type_names}\\': type_names,\\n        \\'{constructor_names}\\': constructor_names,\\n        \\'{request_urls}\\': request_urls,\\n        \\'{type_urls}\\': type_urls,\\n        \\'{constructor_urls}\\': constructor_urls\\n    })', 'def _create_structure(tlobjects, output_dir):\\n    \"\"\"\\n    Pre-create the required directory structure\\n    in `output_dir` for the input objects.\\n    \"\"\"\\n    types_ns = set()\\n    method_ns = set()\\n    for obj in tlobjects:\\n        if obj.namespace:\\n            if obj.is_function:\\n                method_ns.add(obj.namespace)\\n            else:\\n                types_ns.add(obj.namespace)\\n\\n    output_dir.mkdir(exist_ok=True)\\n\\n    type_dir = output_dir / \\'types\\'\\n    type_dir.mkdir(exist_ok=True)\\n\\n    cons_dir = output_dir / \\'constructors\\'\\n    cons_dir.mkdir(exist_ok=True)\\n    for ns in types_ns:\\n        (type_dir / ns).mkdir(exist_ok=True)\\n        (cons_dir / ns).mkdir(exist_ok=True)\\n\\n    meth_dir = output_dir / \\'methods\\'\\n    meth_dir.mkdir(exist_ok=True)\\n    for ns in types_ns:\\n        (meth_dir / ns).mkdir(exist_ok=True)', 'def create_connection(self, *args, **kwargs):\\n        \"\"\"This method is trying to establish connection with one of the zookeeper nodes.\\n           Somehow strategy \"fail earlier and retry more often\" works way better comparing to\\n           the original strategy \"try to connect with specified timeout\".\\n           Since we want to try connect to zookeeper more often (with the smaller connect_timeout),\\n           he have to override `create_connection` method in the `SequentialThreadingHandler`\\n           class (which is used by `kazoo.Client`).\\n\\n        :param args: always contains `tuple(host, port)` as the first element and could contain\\n                     `connect_timeout` (negotiated session timeout) as the second element.\"\"\"\\n\\n        args = list(args)\\n        if len(args) == 0:  # kazoo 2.6.0 slightly changed the way how it calls create_connection method\\n            kwargs[\\'timeout\\'] = max(self._connect_timeout, kwargs.get(\\'timeout\\', self._connect_timeout*10)/10.0)\\n        elif len(args) == 1:\\n            args.append(self._connect_timeout)\\n        else:\\n            args[1] = max(self._connect_timeout, args[1]/10.0)\\n        return super(PatroniSequentialThreadingHandler, self).create_connection(*args, **kwargs)', 'def select(self, *args, **kwargs):\\n        \"\"\"Python3 raises `ValueError` if socket is closed, because fd == -1\"\"\"\\n        try:\\n            return super(PatroniSequentialThreadingHandler, self).select(*args, **kwargs)\\n        except ValueError as e:\\n            raise select.error(9, str(e))', 'def _kazoo_connect(self, host, port):\\n        \"\"\"Kazoo is using Ping\\'s to determine health of connection to zookeeper. If there is no\\n        response on Ping after Ping interval (1/2 from read_timeout) it will consider current\\n        connection dead and try to connect to another node. Without this \"magic\" it was taking\\n        up to 2/3 from session timeout (ttl) to figure out that connection was dead and we had\\n        only small time for reconnect and retry.\\n\\n        This method is needed to return different value of read_timeout, which is not calculated\\n        from negotiated session timeout but from value of `loop_wait`. And it is 2 sec smaller\\n        than loop_wait, because we can spend up to 2 seconds when calling `touch_member()` and\\n        `write_leader_optime()` methods, which also may hang...\"\"\"\\n\\n        ret = self._orig_kazoo_connect(host, port)\\n        return max(self.loop_wait - 2, 2)*1000, ret[1]', 'def set_ttl(self, ttl):\\n        \"\"\"It is not possible to change ttl (session_timeout) in zookeeper without\\n        destroying old session and creating the new one. This method returns `!True`\\n        if session_timeout has been changed (`restart()` has been called).\"\"\"\\n        if self._client._session_timeout != ttl:\\n            self._client._session_timeout = ttl\\n            self._client.restart()\\n            return True', 'def service_name_from_scope_name(scope_name):\\n    \"\"\"Translate scope name to service name which can be used in dns.\\n\\n    230 = 253 - len(\\'replica.\\') - len(\\'.service.consul\\')\\n    \"\"\"\\n\\n    def replace_char(match):\\n        c = match.group(0)\\n        return \\'-\\' if c in \\'. _\\' else \"u{:04d}\".format(ord(c))\\n\\n    service_name = re.sub(r\\'[^a-z0-9\\\\-]\\', replace_char, scope_name.lower())\\n    return service_name[0:230]', 'def _do_refresh_session(self):\\n        \"\"\":returns: `!True` if it had to create new session\"\"\"\\n        if self._session and self._last_session_refresh + self._loop_wait > time.time():\\n            return False\\n\\n        if self._session:\\n            try:\\n                self._client.session.renew(self._session)\\n            except NotFound:\\n                self._session = None\\n        ret = not self._session\\n        if ret:\\n            try:\\n                self._session = self._client.session.create(name=self._scope + \\'-\\' + self._name,\\n                                                            checks=self.__session_checks,\\n                                                            lock_delay=0.001, behavior=\\'delete\\')\\n            except InvalidSessionTTL:\\n                logger.exception(\\'session.create\\')\\n                self.adjust_ttl()\\n                raise\\n\\n        self._last_session_refresh = time.time()\\n        return ret', 'def _read_postmaster_pidfile(data_dir):\\n        \"\"\"Reads and parses postmaster.pid from the data directory\\n\\n        :returns dictionary of values if successful, empty dictionary otherwise\\n        \"\"\"\\n        pid_line_names = [\\'pid\\', \\'data_dir\\', \\'start_time\\', \\'port\\', \\'socket_dir\\', \\'listen_addr\\', \\'shmem_key\\']\\n        try:\\n            with open(os.path.join(data_dir, \\'postmaster.pid\\')) as f:\\n                return {name: line.rstrip(\\'\\\\n\\') for name, line in zip(pid_line_names, f)}\\n        except IOError:\\n            return {}', 'def signal_stop(self, mode):\\n        \"\"\"Signal postmaster process to stop\\n\\n        :returns None if signaled, True if process is already gone, False if error\\n        \"\"\"\\n        if self.is_single_user:\\n            logger.warning(\"Cannot stop server; single-user server is running (PID: {0})\".format(self.pid))\\n            return False\\n        try:\\n            self.send_signal(STOP_SIGNALS[mode])\\n        except psutil.NoSuchProcess:\\n            return True\\n        except psutil.AccessDenied as e:\\n            logger.warning(\"Could not send stop signal to PostgreSQL (error: {0})\".format(e))\\n            return False\\n\\n        return None', 'def repr_size(n_bytes):\\n    \"\"\"\\n    >>> repr_size(1000)\\n    \\'1000 Bytes\\'\\n    >>> repr_size(8257332324597)\\n    \\'7.5 TiB\\'\\n    \"\"\"\\n    if n_bytes < 1024:\\n        return \\'{0} Bytes\\'.format(n_bytes)\\n    i = -1\\n    while n_bytes > 1023:\\n        n_bytes /= 1024.0\\n        i += 1\\n    return \\'{0} {1}iB\\'.format(round(n_bytes, 1), si_prefixes[i])', 'def size_as_bytes(size_, prefix):\\n    \"\"\"\\n    >>> size_as_bytes(7.5, \\'T\\')\\n    8246337208320\\n    \"\"\"\\n    prefix = prefix.upper()\\n\\n    assert prefix in si_prefixes\\n\\n    exponent = si_prefixes.index(prefix) + 1\\n\\n    return int(size_ * (1024.0 ** exponent))', 'def run(self):\\n        \"\"\"\\n        Creates a new replica using WAL-E\\n\\n        Returns\\n        -------\\n        ExitCode\\n            0 = Success\\n            1 = Error, try again\\n            2 = Error, don\\'t try again\\n\\n        \"\"\"\\n        if self.init_error:\\n            logger.error(\\'init error: %r did not exist at initialization time\\',\\n                         self.wal_e.env_dir)\\n            return ExitCode.FAIL\\n\\n        try:\\n            should_use_s3 = self.should_use_s3_to_create_replica()\\n            if should_use_s3 is None:  # Need to retry\\n                return ExitCode.RETRY_LATER\\n            elif should_use_s3:\\n                return self.create_replica_with_s3()\\n            elif not should_use_s3:\\n                return ExitCode.FAIL\\n        except Exception:\\n            logger.exception(\"Unhandled exception when running WAL-E restore\")\\n        return ExitCode.FAIL', 'def should_use_s3_to_create_replica(self):\\n        \"\"\" determine whether it makes sense to use S3 and not pg_basebackup \"\"\"\\n\\n        threshold_megabytes = self.wal_e.threshold_mb\\n        threshold_percent = self.wal_e.threshold_pct\\n\\n        try:\\n            cmd = self.wal_e.cmd + [\\'backup-list\\', \\'--detail\\', \\'LATEST\\']\\n\\n            logger.debug(\\'calling %r\\', cmd)\\n            wale_output = subprocess.check_output(cmd)\\n\\n            reader = csv.DictReader(wale_output.decode(\\'utf-8\\').splitlines(),\\n                                    dialect=\\'excel-tab\\')\\n            rows = list(reader)\\n            if not len(rows):\\n                logger.warning(\\'wal-e did not find any backups\\')\\n                return False\\n\\n            # This check might not add much, it was performed in the previous\\n            # version of this code. since the old version rolled CSV parsing the\\n            # check may have been part of the CSV parsing.\\n            if len(rows) > 1:\\n                logger.warning(\\n                    \\'wal-e returned more than one row of backups: %r\\',\\n                    rows)\\n                return False\\n\\n            backup_info = rows[0]\\n        except subprocess.CalledProcessError:\\n            logger.exception(\"could not query wal-e latest backup\")\\n            return None\\n\\n        try:\\n            backup_size = int(backup_info[\\'expanded_size_bytes\\'])\\n            backup_start_segment = backup_info[\\'wal_segment_backup_start\\']\\n            backup_start_offset = backup_info[\\'wal_segment_offset_backup_start\\']\\n        except KeyError:\\n            logger.exception(\"unable to get some of WALE backup parameters\")\\n            return None\\n\\n        # WAL filename is XXXXXXXXYYYYYYYY000000ZZ, where X - timeline, Y - LSN logical log file,\\n        # ZZ - 2 high digits of LSN offset. The rest of the offset is the provided decimal offset,\\n        # that we have to convert to hex and \\'prepend\\' to the high offset digits.\\n\\n        lsn_segment = backup_start_segment[8:16]\\n        # first 2 characters of the result are 0x and the last one is L\\n        lsn_offset = hex((int(backup_start_segment[16:32], 16) << 24) + int(backup_start_offset))[2:-1]\\n\\n        # construct the LSN from the segment and offset\\n        backup_start_lsn = \\'{0}/{1}\\'.format(lsn_segment, lsn_offset)\\n\\n        diff_in_bytes = backup_size\\n        attempts_no = 0\\n        while True:\\n            if self.master_connection:\\n                try:\\n                    # get the difference in bytes between the current WAL location and the backup start offset\\n                    with psycopg2.connect(self.master_connection) as con:\\n                        if con.server_version >= 100000:\\n                            wal_name = \\'wal\\'\\n                            lsn_name = \\'lsn\\'\\n                        else:\\n                            wal_name = \\'xlog\\'\\n                            lsn_name = \\'location\\'\\n                        con.autocommit = True\\n                        with con.cursor() as cur:\\n                            cur.execute((\"SELECT CASE WHEN pg_catalog.pg_is_in_recovery()\"\\n                                         \" THEN GREATEST(pg_catalog.pg_{0}_{1}_diff(COALESCE(\"\\n                                         \"pg_last_{0}_receive_{1}(), \\'0/0\\'), %s)::bigint, \"\\n                                         \"pg_catalog.pg_{0}_{1}_diff(pg_catalog.pg_last_{0}_replay_{1}(), %s)::bigint)\"\\n                                         \" ELSE pg_catalog.pg_{0}_{1}_diff(pg_catalog.pg_current_{0}_{1}(), %s)::bigint\"\\n                                         \" END\").format(wal_name, lsn_name),\\n                                        (backup_start_lsn, backup_start_lsn, backup_start_lsn))\\n\\n                            diff_in_bytes = int(cur.fetchone()[0])\\n                except psycopg2.Error:\\n                    logger.exception(\\'could not determine difference with the master location\\')\\n                    if attempts_no < self.retries:  # retry in case of a temporarily connection issue\\n                        attempts_no = attempts_no + 1\\n                        time.sleep(RETRY_SLEEP_INTERVAL)\\n                        continue\\n                    else:\\n                        if not self.no_master:\\n                            return False  # do no more retries on the outer level\\n                        logger.info(\"continue with base backup from S3 since master is not available\")\\n                        diff_in_bytes = 0\\n                        break\\n            else:\\n                # always try to use WAL-E if master connection string is not available\\n                diff_in_bytes = 0\\n            break\\n\\n        # if the size of the accumulated WAL segments is more than a certan percentage of the backup size\\n        # or exceeds the pre-determined size - pg_basebackup is chosen instead.\\n        is_size_thresh_ok = diff_in_bytes < int(threshold_megabytes) * 1048576\\n        threshold_pct_bytes = backup_size * threshold_percent / 100.0\\n        is_percentage_thresh_ok = float(diff_in_bytes) < int(threshold_pct_bytes)\\n        are_thresholds_ok = is_size_thresh_ok and is_percentage_thresh_ok\\n\\n        class Size(object):\\n            def __init__(self, n_bytes, prefix=None):\\n                self.n_bytes = n_bytes\\n                self.prefix = prefix\\n\\n            def __repr__(self):\\n                if self.prefix is not None:\\n                    n_bytes = size_as_bytes(self.n_bytes, self.prefix)\\n                else:\\n                    n_bytes = self.n_bytes\\n                return repr_size(n_bytes)\\n\\n        class HumanContext(object):\\n            def __init__(self, items):\\n                self.items = items\\n\\n            def __repr__(self):\\n                return \\', \\'.join(\\'{}={!r}\\'.format(key, value)\\n                                 for key, value in self.items)\\n\\n        human_context = repr(HumanContext([\\n            (\\'threshold_size\\', Size(threshold_megabytes, \\'M\\')),\\n            (\\'threshold_percent\\', threshold_percent),\\n            (\\'threshold_percent_size\\', Size(threshold_pct_bytes)),\\n            (\\'backup_size\\', Size(backup_size)),\\n            (\\'backup_diff\\', Size(diff_in_bytes)),\\n            (\\'is_size_thresh_ok\\', is_size_thresh_ok),\\n            (\\'is_percentage_thresh_ok\\', is_percentage_thresh_ok),\\n        ]))\\n\\n        if not are_thresholds_ok:\\n            logger.info(\\'wal-e backup size diff is over threshold, falling back \\'\\n                        \\'to other means of restore: %s\\', human_context)\\n        else:\\n            logger.info(\\'Thresholds are OK, using wal-e basebackup: %s\\', human_context)\\n        return are_thresholds_ok', 'def watching(w, watch, max_count=None, clear=True):\\n    \"\"\"\\n    >>> len(list(watching(True, 1, 0)))\\n    1\\n    >>> len(list(watching(True, 1, 1)))\\n    2\\n    >>> len(list(watching(True, None, 0)))\\n    1\\n    \"\"\"\\n\\n    if w and not watch:\\n        watch = 2\\n    if watch and clear:\\n        click.clear()\\n    yield 0\\n\\n    if max_count is not None and max_count < 1:\\n        return\\n\\n    counter = 1\\n    while watch and counter <= (max_count or counter):\\n        time.sleep(watch)\\n        counter += 1\\n        if clear:\\n            click.clear()\\n        yield 0', 'def _do_failover_or_switchover(obj, action, cluster_name, master, candidate, force, scheduled=None):\\n    \"\"\"\\n        We want to trigger a failover or switchover for the specified cluster name.\\n\\n        We verify that the cluster name, master name and candidate name are correct.\\n        If so, we trigger an action and keep the client up to date.\\n    \"\"\"\\n\\n    dcs = get_dcs(obj, cluster_name)\\n    cluster = dcs.get_cluster()\\n\\n    if action == \\'switchover\\' and cluster.leader is None:\\n        raise PatroniCtlException(\\'This cluster has no master\\')\\n\\n    if master is None:\\n        if force or action == \\'failover\\':\\n            master = cluster.leader and cluster.leader.name\\n        else:\\n            master = click.prompt(\\'Master\\', type=str, default=cluster.leader.member.name)\\n\\n    if master is not None and cluster.leader and cluster.leader.member.name != master:\\n        raise PatroniCtlException(\\'Member {0} is not the leader of cluster {1}\\'.format(master, cluster_name))\\n\\n    # excluding members with nofailover tag\\n    candidate_names = [str(m.name) for m in cluster.members if m.name != master and not m.nofailover]\\n    # We sort the names for consistent output to the client\\n    candidate_names.sort()\\n\\n    if not candidate_names:\\n        raise PatroniCtlException(\\'No candidates found to {0} to\\'.format(action))\\n\\n    if candidate is None and not force:\\n        candidate = click.prompt(\\'Candidate \\' + str(candidate_names), type=str, default=\\'\\')\\n\\n    if action == \\'failover\\' and not candidate:\\n        raise PatroniCtlException(\\'Failover could be performed only to a specific candidate\\')\\n\\n    if candidate == master:\\n        raise PatroniCtlException(action.title() + \\' target and source are the same.\\')\\n\\n    if candidate and candidate not in candidate_names:\\n        raise PatroniCtlException(\\'Member {0} does not exist in cluster {1}\\'.format(candidate, cluster_name))\\n\\n    scheduled_at_str = None\\n    scheduled_at = None\\n\\n    if action == \\'switchover\\':\\n        if scheduled is None and not force:\\n            scheduled = click.prompt(\\'When should the switchover take place (e.g. 2015-10-01T14:30) \\',\\n                                     type=str, default=\\'now\\')\\n\\n        scheduled_at = parse_scheduled(scheduled)\\n        if scheduled_at:\\n            if cluster.is_paused():\\n                raise PatroniCtlException(\"Can\\'t schedule switchover in the paused state\")\\n            scheduled_at_str = scheduled_at.isoformat()\\n\\n    failover_value = {\\'leader\\': master, \\'candidate\\': candidate, \\'scheduled_at\\': scheduled_at_str}\\n\\n    logging.debug(failover_value)\\n\\n    # By now we have established that the leader exists and the candidate exists\\n    click.echo(\\'Current cluster topology\\')\\n    output_members(dcs.get_cluster(), cluster_name)\\n\\n    if not force:\\n        demote_msg = \\', demoting current master \\' + master if master else \\'\\'\\n\\n        if not click.confirm(\\'Are you sure you want to {0} cluster {1}{2}?\\'.format(action, cluster_name, demote_msg)):\\n            raise PatroniCtlException(\\'Aborting \\' + action)\\n\\n    r = None\\n    try:\\n        member = cluster.leader.member if cluster.leader else cluster.get_member(candidate, False)\\n\\n        r = request_patroni(member, \\'post\\', action, failover_value, auth_header(obj))\\n\\n        # probably old patroni, which doesn\\'t support switchover yet\\n        if r.status_code == 501 and action == \\'switchover\\' and \\'Server does not support this operation\\' in r.text:\\n            r = request_patroni(member, \\'post\\', \\'failover\\', failover_value, auth_header(obj))\\n\\n        if r.status_code in (200, 202):\\n            logging.debug(r)\\n            cluster = dcs.get_cluster()\\n            logging.debug(cluster)\\n            click.echo(\\'{0} {1}\\'.format(timestamp(), r.text))\\n        else:\\n            click.echo(\\'{0} failed, details: {1}, {2}\\'.format(action.title(), r.status_code, r.text))\\n            return\\n    except Exception:\\n        logging.exception(r)\\n        logging.warning(\\'Failing over to DCS\\')\\n        click.echo(\\'{0} Could not {1} using Patroni api, falling back to DCS\\'.format(timestamp(), action))\\n        dcs.manual_failover(master, candidate, scheduled_at=scheduled_at)\\n\\n    output_members(cluster, cluster_name)', \"def touch_member(config, dcs):\\n    ''' Rip-off of the ha.touch_member without inter-class dependencies '''\\n    p = Postgresql(config['postgresql'])\\n    p.set_state('running')\\n    p.set_role('master')\\n\\n    def restapi_connection_string(config):\\n        protocol = 'https' if config.get('certfile') else 'http'\\n        connect_address = config.get('connect_address')\\n        listen = config['listen']\\n        return '{0}://{1}/patroni'.format(protocol, connect_address or listen)\\n\\n    data = {\\n        'conn_url': p.connection_string,\\n        'api_url': restapi_connection_string(config['restapi']),\\n        'state': p.state,\\n        'role': p.role\\n    }\\n\\n    return dcs.touch_member(data, permanent=True)\", 'def set_defaults(config, cluster_name):\\n    \"\"\"fill-in some basic configuration parameters if config file is not set \"\"\"\\n    config[\\'postgresql\\'].setdefault(\\'name\\', cluster_name)\\n    config[\\'postgresql\\'].setdefault(\\'scope\\', cluster_name)\\n    config[\\'postgresql\\'].setdefault(\\'listen\\', \\'127.0.0.1\\')\\n    config[\\'postgresql\\'][\\'authentication\\'] = {\\'replication\\': None}\\n    config[\\'restapi\\'][\\'listen\\'] = \\':\\' in config[\\'restapi\\'][\\'listen\\'] and config[\\'restapi\\'][\\'listen\\'] or \\'127.0.0.1:8008\\'', 'def temporary_file(contents, suffix=\\'\\', prefix=\\'tmp\\'):\\n    \"\"\"Creates a temporary file with specified contents that persists for the context.\\n\\n    :param contents: binary string that will be written to the file.\\n    :param prefix: will be prefixed to the filename.\\n    :param suffix: will be appended to the filename.\\n    :returns path of the created file.\\n    \"\"\"\\n    tmp = tempfile.NamedTemporaryFile(suffix=suffix, prefix=prefix, delete=False)\\n    with tmp:\\n        tmp.write(contents)\\n\\n    try:\\n        yield tmp.name\\n    finally:\\n        os.unlink(tmp.name)', 'def show_diff(before_editing, after_editing):\\n    \"\"\"Shows a diff between two strings.\\n\\n    If the output is to a tty the diff will be colored. Inputs are expected to be unicode strings.\\n    \"\"\"\\n    def listify(string):\\n        return [l+\\'\\\\n\\' for l in string.rstrip(\\'\\\\n\\').split(\\'\\\\n\\')]\\n\\n    unified_diff = difflib.unified_diff(listify(before_editing), listify(after_editing))\\n\\n    if sys.stdout.isatty():\\n        buf = io.StringIO()\\n        for line in unified_diff:\\n            # Force cast to unicode as difflib on Python 2.7 returns a mix of unicode and str.\\n            buf.write(text_type(line))\\n        buf.seek(0)\\n\\n        class opts:\\n            side_by_side = False\\n            width = 80\\n            tab_width = 8\\n        cdiff.markup_to_pager(cdiff.PatchStream(buf), opts)\\n    else:\\n        for line in unified_diff:\\n            click.echo(line.rstrip(\\'\\\\n\\'))', 'def format_config_for_editing(data):\\n    \"\"\"Formats configuration as YAML for human consumption.\\n\\n    :param data: configuration as nested dictionaries\\n    :returns unicode YAML of the configuration\"\"\"\\n    return yaml.safe_dump(data, default_flow_style=False, encoding=None, allow_unicode=True)', 'def apply_config_changes(before_editing, data, kvpairs):\\n    \"\"\"Applies config changes specified as a list of key-value pairs.\\n\\n    Keys are interpreted as dotted paths into the configuration data structure. Except for paths beginning with\\n    `postgresql.parameters` where rest of the path is used directly to allow for PostgreSQL GUCs containing dots.\\n    Values are interpreted as YAML values.\\n\\n    :param before_editing: human representation before editing\\n    :param data: configuration datastructure\\n    :param kvpairs: list of strings containing key value pairs separated by =\\n    :returns tuple of human readable and parsed datastructure after changes\\n    \"\"\"\\n    changed_data = copy.deepcopy(data)\\n\\n    def set_path_value(config, path, value, prefix=()):\\n        # Postgresql GUCs can\\'t be nested, but can contain dots so we re-flatten the structure for this case\\n        if prefix == (\\'postgresql\\', \\'parameters\\'):\\n            path = [\\'.\\'.join(path)]\\n\\n        key = path[0]\\n        if len(path) == 1:\\n            if value is None:\\n                config.pop(key, None)\\n            else:\\n                config[key] = value\\n        else:\\n            if not isinstance(config.get(key), dict):\\n                config[key] = {}\\n            set_path_value(config[key], path[1:], value, prefix + (key,))\\n            if config[key] == {}:\\n                del config[key]\\n\\n    for pair in kvpairs:\\n        if not pair or \"=\" not in pair:\\n            raise PatroniCtlException(\"Invalid parameter setting {0}\".format(pair))\\n        key_path, value = pair.split(\"=\", 1)\\n        set_path_value(changed_data, key_path.strip().split(\".\"), yaml.safe_load(value))\\n\\n    return format_config_for_editing(changed_data), changed_data', 'def apply_yaml_file(data, filename):\\n    \"\"\"Applies changes from a YAML file to configuration\\n\\n    :param data: configuration datastructure\\n    :param filename: name of the YAML file, - is taken to mean standard input\\n    :returns tuple of human readable and parsed datastructure after changes\\n    \"\"\"\\n    changed_data = copy.deepcopy(data)\\n\\n    if filename == \\'-\\':\\n        new_options = yaml.safe_load(sys.stdin)\\n    else:\\n        with open(filename) as fd:\\n            new_options = yaml.safe_load(fd)\\n\\n    patch_config(changed_data, new_options)\\n\\n    return format_config_for_editing(changed_data), changed_data', 'def invoke_editor(before_editing, cluster_name):\\n    \"\"\"Starts editor command to edit configuration in human readable format\\n\\n    :param before_editing: human representation before editing\\n    :returns tuple of human readable and parsed datastructure after changes\\n    \"\"\"\\n    editor_cmd = os.environ.get(\\'EDITOR\\')\\n    if not editor_cmd:\\n        raise PatroniCtlException(\\'EDITOR environment variable is not set\\')\\n\\n    with temporary_file(contents=before_editing.encode(\\'utf-8\\'),\\n                        suffix=\\'.yaml\\',\\n                        prefix=\\'{0}-config-\\'.format(cluster_name)) as tmpfile:\\n        ret = subprocess.call([editor_cmd, tmpfile])\\n        if ret:\\n            raise PatroniCtlException(\"Editor exited with return code {0}\".format(ret))\\n\\n        with codecs.open(tmpfile, encoding=\\'utf-8\\') as fd:\\n            after_editing = fd.read()\\n\\n        return after_editing, yaml.safe_load(after_editing)', 'def _tag_ebs(self, conn, role):\\n        \"\"\" set tags, carrying the cluster name, instance role and instance id for the EBS storage \"\"\"\\n        tags = {\\'Name\\': \\'spilo_\\' + self.cluster_name, \\'Role\\': role, \\'Instance\\': self.instance_id}\\n        volumes = conn.get_all_volumes(filters={\\'attachment.instance-id\\': self.instance_id})\\n        conn.create_tags([v.id for v in volumes], tags)', 'def _tag_ec2(self, conn, role):\\n        \"\"\" tag the current EC2 instance with a cluster role \"\"\"\\n        tags = {\\'Role\\': role}\\n        conn.create_tags([self.instance_id], tags)', 'def failover_limitation(self):\\n        \"\"\"Returns reason why this node can\\'t promote or None if everything is ok.\"\"\"\\n        if not self.reachable:\\n            return \\'not reachable\\'\\n        if self.tags.get(\\'nofailover\\', False):\\n            return \\'not allowed to promote\\'\\n        if self.watchdog_failed:\\n            return \\'not watchdog capable\\'\\n        return None', 'def get_effective_tags(self):\\n        \"\"\"Return configuration tags merged with dynamically applied tags.\"\"\"\\n        tags = self.patroni.tags.copy()\\n        # _disable_sync could be modified concurrently, but we don\\'t care as attribute get and set are atomic.\\n        if self._disable_sync > 0:\\n            tags[\\'nosync\\'] = True\\n        return tags', 'def bootstrap_standby_leader(self):\\n        \"\"\" If we found \\'standby\\' key in the configuration, we need to bootstrap\\n            not a real master, but a \\'standby leader\\', that will take base backup\\n            from a remote master and start follow it.\\n        \"\"\"\\n        clone_source = self.get_remote_master()\\n        msg = \\'clone from remote master {0}\\'.format(clone_source.conn_url)\\n        result = self.clone(clone_source, msg)\\n        self._post_bootstrap_task.complete(result)\\n        if result:\\n            self.state_handler.set_role(\\'standby_leader\\')\\n\\n        return result', 'def process_sync_replication(self):\\n        \"\"\"Process synchronous standby beahvior.\\n\\n        Synchronous standbys are registered in two places postgresql.conf and DCS. The order of updating them must\\n        be right. The invariant that should be kept is that if a node is master and sync_standby is set in DCS,\\n        then that node must have synchronous_standby set to that value. Or more simple, first set in postgresql.conf\\n        and then in DCS. When removing, first remove in DCS, then in postgresql.conf. This is so we only consider\\n        promoting standbys that were guaranteed to be replicating synchronously.\\n        \"\"\"\\n        if self.is_synchronous_mode():\\n            current = self.cluster.sync.leader and self.cluster.sync.sync_standby\\n            picked, allow_promote = self.state_handler.pick_synchronous_standby(self.cluster)\\n            if picked != current:\\n                # We need to revoke privilege from current before replacing it in the config\\n                if current:\\n                    logger.info(\"Removing synchronous privilege from %s\", current)\\n                    if not self.dcs.write_sync_state(self.state_handler.name, None, index=self.cluster.sync.index):\\n                        logger.info(\\'Synchronous replication key updated by someone else.\\')\\n                        return\\n\\n                if self.is_synchronous_mode_strict() and picked is None:\\n                    picked = \\'*\\'\\n                    logger.warning(\"No standbys available!\")\\n\\n                logger.info(\"Assigning synchronous standby status to %s\", picked)\\n                self.state_handler.set_synchronous_standby(picked)\\n\\n                if picked and picked != \\'*\\' and not allow_promote:\\n                    # Wait for PostgreSQL to enable synchronous mode and see if we can immediately set sync_standby\\n                    time.sleep(2)\\n                    picked, allow_promote = self.state_handler.pick_synchronous_standby(self.cluster)\\n                if allow_promote:\\n                    try:\\n                        cluster = self.dcs.get_cluster()\\n                    except DCSError:\\n                        return logger.warning(\"Could not get cluster state from DCS during process_sync_replication()\")\\n                    if cluster.sync.leader and cluster.sync.leader != self.state_handler.name:\\n                        logger.info(\"Synchronous replication key updated by someone else\")\\n                        return\\n                    if not self.dcs.write_sync_state(self.state_handler.name, picked, index=cluster.sync.index):\\n                        logger.info(\"Synchronous replication key updated by someone else\")\\n                        return\\n                    logger.info(\"Synchronous standby status assigned to %s\", picked)\\n        else:\\n            if self.cluster.sync.leader and self.dcs.delete_sync_state(index=self.cluster.sync.index):\\n                logger.info(\"Disabled synchronous replication\")\\n            self.state_handler.set_synchronous_standby(None)', 'def while_not_sync_standby(self, func):\\n        \"\"\"Runs specified action while trying to make sure that the node is not assigned synchronous standby status.\\n\\n        Tags us as not allowed to be a sync standby as we are going to go away, if we currently are wait for\\n        leader to notice and pick an alternative one or if the leader changes or goes away we are also free.\\n\\n        If the connection to DCS fails we run the action anyway, as this is only a hint.\\n\\n        There is a small race window where this function runs between a master picking us the sync standby and\\n        publishing it to the DCS. As the window is rather tiny consequences are holding up commits for one cycle\\n        period we don\\'t worry about it here.\"\"\"\\n\\n        if not self.is_synchronous_mode() or self.patroni.nosync:\\n            return func()\\n\\n        with self._member_state_lock:\\n            self._disable_sync += 1\\n        try:\\n            if self.touch_member():\\n                # Master should notice the updated value during the next cycle. We will wait double that, if master\\n                # hasn\\'t noticed the value by then not disabling sync replication is not likely to matter.\\n                for _ in polling_loop(timeout=self.dcs.loop_wait*2, interval=2):\\n                    try:\\n                        if not self.is_sync_standby(self.dcs.get_cluster()):\\n                            break\\n                    except DCSError:\\n                        logger.warning(\"Could not get cluster state, skipping synchronous standby disable\")\\n                        break\\n                    logger.info(\"Waiting for master to release us from synchronous standby\")\\n            else:\\n                logger.warning(\"Updating member state failed, skipping synchronous standby disable\")\\n\\n            return func()\\n        finally:\\n            with self._member_state_lock:\\n                self._disable_sync -= 1', 'def fetch_node_status(member):\\n        \"\"\"This function perform http get request on member.api_url and fetches its status\\n        :returns: `_MemberStatus` object\\n        \"\"\"\\n\\n        try:\\n            response = requests.get(member.api_url, timeout=2, verify=False)\\n            logger.info(\\'Got response from %s %s: %s\\', member.name, member.api_url, response.content)\\n            return _MemberStatus.from_api_response(member, response.json())\\n        except Exception as e:\\n            logger.warning(\"Request failed to %s: GET %s (%s)\", member.name, member.api_url, e)\\n        return _MemberStatus.unknown(member)', 'def is_lagging(self, wal_position):\\n        \"\"\"Returns if instance with an wal should consider itself unhealthy to be promoted due to replication lag.\\n\\n        :param wal_position: Current wal position.\\n        :returns True when node is lagging\\n        \"\"\"\\n        lag = (self.cluster.last_leader_operation or 0) - wal_position\\n        return lag > self.patroni.config.get(\\'maximum_lag_on_failover\\', 0)', 'def _is_healthiest_node(self, members, check_replication_lag=True):\\n        \"\"\"This method tries to determine whether I am healthy enough to became a new leader candidate or not.\"\"\"\\n\\n        _, my_wal_position = self.state_handler.timeline_wal_position()\\n        if check_replication_lag and self.is_lagging(my_wal_position):\\n            logger.info(\\'My wal position exceeds maximum replication lag\\')\\n            return False  # Too far behind last reported wal position on master\\n\\n        if not self.is_standby_cluster() and self.check_timeline():\\n            cluster_timeline = self.cluster.timeline\\n            my_timeline = self.state_handler.replica_cached_timeline(cluster_timeline)\\n            if my_timeline < cluster_timeline:\\n                logger.info(\\'My timeline %s is behind last known cluster timeline %s\\', my_timeline, cluster_timeline)\\n                return False\\n\\n        # Prepare list of nodes to run check against\\n        members = [m for m in members if m.name != self.state_handler.name and not m.nofailover and m.api_url]\\n\\n        if members:\\n            for st in self.fetch_nodes_statuses(members):\\n                if st.failover_limitation() is None:\\n                    if not st.in_recovery:\\n                        logger.warning(\\'Master (%s) is still alive\\', st.member.name)\\n                        return False\\n                    if my_wal_position < st.wal_position:\\n                        logger.info(\\'Wal position of %s is ahead of my wal position\\', st.member.name)\\n                        return False\\n        return True', 'def demote(self, mode):\\n        \"\"\"Demote PostgreSQL running as master.\\n\\n        :param mode: One of offline, graceful or immediate.\\n            offline is used when connection to DCS is not available.\\n            graceful is used when failing over to another node due to user request. May only be called running async.\\n            immediate is used when we determine that we are not suitable for master and want to failover quickly\\n                without regard for data durability. May only be called synchronously.\\n            immediate-nolock is used when find out that we have lost the lock to be master. Need to bring down\\n                PostgreSQL as quickly as possible without regard for data durability. May only be called synchronously.\\n        \"\"\"\\n        mode_control = {\\n            \\'offline\\':          dict(stop=\\'fast\\', checkpoint=False, release=False, offline=True, async_req=False),\\n            \\'graceful\\':         dict(stop=\\'fast\\', checkpoint=True, release=True, offline=False, async_req=False),\\n            \\'immediate\\':        dict(stop=\\'immediate\\', checkpoint=False, release=True, offline=False, async_req=True),\\n            \\'immediate-nolock\\': dict(stop=\\'immediate\\', checkpoint=False, release=False, offline=False, async_req=True),\\n        }[mode]\\n\\n        self.state_handler.trigger_check_diverged_lsn()\\n        self.state_handler.stop(mode_control[\\'stop\\'], checkpoint=mode_control[\\'checkpoint\\'],\\n                                on_safepoint=self.watchdog.disable if self.watchdog.is_running else None)\\n        self.state_handler.set_role(\\'demoted\\')\\n        self.set_is_leader(False)\\n\\n        if mode_control[\\'release\\']:\\n            with self._async_executor:\\n                self.release_leader_key_voluntarily()\\n            time.sleep(2)  # Give a time to somebody to take the leader lock\\n        if mode_control[\\'offline\\']:\\n            node_to_follow, leader = None, None\\n        else:\\n            cluster = self.dcs.get_cluster()\\n            node_to_follow, leader = self._get_node_to_follow(cluster), cluster.leader\\n\\n        # FIXME: with mode offline called from DCS exception handler and handle_long_action_in_progress\\n        # there could be an async action already running, calling follow from here will lead\\n        # to racy state handler state updates.\\n        if mode_control[\\'async_req\\']:\\n            self._async_executor.schedule(\\'starting after demotion\\')\\n            self._async_executor.run_async(self.state_handler.follow, (node_to_follow,))\\n        else:\\n            if self.is_synchronous_mode():\\n                self.state_handler.set_synchronous_standby(None)\\n            if self.state_handler.rewind_or_reinitialize_needed_and_possible(leader):\\n                return False  # do not start postgres, but run pg_rewind on the next iteration\\n            self.state_handler.follow(node_to_follow)', 'def process_manual_failover_from_leader(self):\\n        \"\"\"Checks if manual failover is requested and takes action if appropriate.\\n\\n        Cleans up failover key if failover conditions are not matched.\\n\\n        :returns: action message if demote was initiated, None if no action was taken\"\"\"\\n        failover = self.cluster.failover\\n        if not failover or (self.is_paused() and not self.state_handler.is_leader()):\\n            return\\n\\n        if (failover.scheduled_at and not\\n            self.should_run_scheduled_action(\"failover\", failover.scheduled_at, lambda:\\n                                             self.dcs.manual_failover(\\'\\', \\'\\', index=failover.index))):\\n            return\\n\\n        if not failover.leader or failover.leader == self.state_handler.name:\\n            if not failover.candidate or failover.candidate != self.state_handler.name:\\n                if not failover.candidate and self.is_paused():\\n                    logger.warning(\\'Failover is possible only to a specific candidate in a paused state\\')\\n                else:\\n                    if self.is_synchronous_mode():\\n                        if failover.candidate and not self.cluster.sync.matches(failover.candidate):\\n                            logger.warning(\\'Failover candidate=%s does not match with sync_standby=%s\\',\\n                                           failover.candidate, self.cluster.sync.sync_standby)\\n                            members = []\\n                        else:\\n                            members = [m for m in self.cluster.members if self.cluster.sync.matches(m.name)]\\n                    else:\\n                        members = [m for m in self.cluster.members\\n                                   if not failover.candidate or m.name == failover.candidate]\\n                    if self.is_failover_possible(members):  # check that there are healthy members\\n                        self._async_executor.schedule(\\'manual failover: demote\\')\\n                        self._async_executor.run_async(self.demote, (\\'graceful\\',))\\n                        return \\'manual failover: demoting myself\\'\\n                    else:\\n                        logger.warning(\\'manual failover: no healthy members found, failover is not possible\\')\\n            else:\\n                logger.warning(\\'manual failover: I am already the leader, no need to failover\\')\\n        else:\\n            logger.warning(\\'manual failover: leader name does not match: %s != %s\\',\\n                           failover.leader, self.state_handler.name)\\n\\n        logger.info(\\'Cleaning up failover key\\')\\n        self.dcs.manual_failover(\\'\\', \\'\\', index=failover.index)', 'def process_unhealthy_cluster(self):\\n        \"\"\"Cluster has no leader key\"\"\"\\n\\n        if self.is_healthiest_node():\\n            if self.acquire_lock():\\n                failover = self.cluster.failover\\n                if failover:\\n                    if self.is_paused() and failover.leader and failover.candidate:\\n                        logger.info(\\'Updating failover key after acquiring leader lock...\\')\\n                        self.dcs.manual_failover(\\'\\', failover.candidate, failover.scheduled_at, failover.index)\\n                    else:\\n                        logger.info(\\'Cleaning up failover key after acquiring leader lock...\\')\\n                        self.dcs.manual_failover(\\'\\', \\'\\')\\n                self.load_cluster_from_dcs()\\n\\n                if self.is_standby_cluster():\\n                    # standby leader disappeared, and this is a healthiest\\n                    # replica, so it should become a new standby leader.\\n                    # This imply that we need to start following a remote master\\n                    msg = \\'promoted self to a standby leader by acquiring session lock\\'\\n                    return self.enforce_follow_remote_master(msg)\\n                else:\\n                    return self.enforce_master_role(\\n                        \\'acquired session lock as a leader\\',\\n                        \\'promoted self to leader by acquiring session lock\\'\\n                    )\\n            else:\\n                return self.follow(\\'demoted self after trying and failing to obtain lock\\',\\n                                   \\'following new leader after trying and failing to obtain lock\\')\\n        else:\\n            # when we are doing manual failover there is no guaranty that new leader is ahead of any other node\\n            # node tagged as nofailover can be ahead of the new leader either, but it is always excluded from elections\\n            if bool(self.cluster.failover) or self.patroni.nofailover:\\n                self.state_handler.trigger_check_diverged_lsn()\\n                time.sleep(2)  # Give a time to somebody to take the leader lock\\n\\n            if self.patroni.nofailover:\\n                return self.follow(\\'demoting self because I am not allowed to become master\\',\\n                                   \\'following a different leader because I am not allowed to promote\\')\\n            return self.follow(\\'demoting self because i am not the healthiest node\\',\\n                               \\'following a different leader because i am not the healthiest node\\')', 'def restart(self, restart_data, run_async=False):\\n        \"\"\" conditional and unconditional restart \"\"\"\\n        assert isinstance(restart_data, dict)\\n\\n        if (not self.restart_matches(restart_data.get(\\'role\\'),\\n                                     restart_data.get(\\'postgres_version\\'),\\n                                     (\\'restart_pending\\' in restart_data))):\\n            return (False, \"restart conditions are not satisfied\")\\n\\n        with self._async_executor:\\n            prev = self._async_executor.schedule(\\'restart\\')\\n            if prev is not None:\\n                return (False, prev + \\' already in progress\\')\\n\\n            # Make the main loop to think that we were recovering dead postgres. If we fail\\n            # to start postgres after a specified timeout (see below), we need to remove\\n            # leader key (if it belong to us) rather than trying to start postgres once again.\\n            self.recovering = True\\n\\n        # Now that restart is scheduled we can set timeout for startup, it will get reset\\n        # once async executor runs and main loop notices PostgreSQL as up.\\n        timeout = restart_data.get(\\'timeout\\', self.patroni.config[\\'master_start_timeout\\'])\\n        self.set_start_timeout(timeout)\\n\\n        # For non async cases we want to wait for restart to complete or timeout before returning.\\n        do_restart = functools.partial(self.state_handler.restart, timeout, self._async_executor.critical_task)\\n        if self.is_synchronous_mode() and not self.has_lock():\\n            do_restart = functools.partial(self.while_not_sync_standby, do_restart)\\n\\n        if run_async:\\n            self._async_executor.run_async(do_restart)\\n            return (True, \\'restart initiated\\')\\n        else:\\n            res = self._async_executor.run(do_restart)\\n            if res:\\n                return (True, \\'restarted successfully\\')\\n            elif res is None:\\n                return (False, \\'postgres is still starting\\')\\n            else:\\n                return (False, \\'restart failed\\')', 'def handle_starting_instance(self):\\n        \"\"\"Starting up PostgreSQL may take a long time. In case we are the leader we may want to\\n        fail over to.\"\"\"\\n\\n        # Check if we are in startup, when paused defer to main loop for manual failovers.\\n        if not self.state_handler.check_for_startup() or self.is_paused():\\n            self.set_start_timeout(None)\\n            if self.is_paused():\\n                self.state_handler.set_state(self.state_handler.is_running() and \\'running\\' or \\'stopped\\')\\n            return None\\n\\n        # state_handler.state == \\'starting\\' here\\n        if self.has_lock():\\n            if not self.update_lock():\\n                logger.info(\"Lost lock while starting up. Demoting self.\")\\n                self.demote(\\'immediate-nolock\\')\\n                return \\'stopped PostgreSQL while starting up because leader key was lost\\'\\n\\n            timeout = self._start_timeout or self.patroni.config[\\'master_start_timeout\\']\\n            time_left = timeout - self.state_handler.time_in_state()\\n\\n            if time_left <= 0:\\n                if self.is_failover_possible(self.cluster.members):\\n                    logger.info(\"Demoting self because master startup is taking too long\")\\n                    self.demote(\\'immediate\\')\\n                    return \\'stopped PostgreSQL because of startup timeout\\'\\n                else:\\n                    return \\'master start has timed out, but continuing to wait because failover is not possible\\'\\n            else:\\n                msg = self.process_manual_failover_from_leader()\\n                if msg is not None:\\n                    return msg\\n\\n                return \\'PostgreSQL is still starting up, {0:.0f} seconds until timeout\\'.format(time_left)\\n        else:\\n            # Use normal processing for standbys\\n            logger.info(\"Still starting up as a standby.\")\\n            return None', 'def get_remote_member(self, member=None):\\n        \"\"\" In case of standby cluster this will tel us from which remote\\n            master to stream. Config can be both patroni config or\\n            cluster.config.data\\n        \"\"\"\\n        cluster_params = self.get_standby_cluster_config()\\n\\n        if cluster_params:\\n            name = member.name if member else \\'remote_master:{}\\'.format(uuid.uuid1())\\n\\n            data = {k: v for k, v in cluster_params.items() if k in RemoteMember.allowed_keys()}\\n            data[\\'no_replication_slot\\'] = \\'primary_slot_name\\' not in cluster_params\\n            conn_kwargs = member.conn_kwargs() if member else \\\\\\n                {k: cluster_params[k] for k in (\\'host\\', \\'port\\') if k in cluster_params}\\n            if conn_kwargs:\\n                data[\\'conn_kwargs\\'] = conn_kwargs\\n\\n            return RemoteMember(name, data)', 'def machines(self):\\n        \"\"\"Original `machines` method(property) of `etcd.Client` class raise exception\\n        when it failed to get list of etcd cluster members. This method is being called\\n        only when request failed on one of the etcd members during `api_execute` call.\\n        For us it\\'s more important to execute original request rather then get new topology\\n        of etcd cluster. So we will catch this exception and return empty list of machines.\\n        Later, during next `api_execute` call we will forcefully update machines_cache.\\n\\n        Also this method implements the same timeout-retry logic as `api_execute`, because\\n        the original method was retrying 2 times with the `read_timeout` on each node.\"\"\"\\n\\n        kwargs = self._build_request_parameters()\\n\\n        while True:\\n            try:\\n                response = self.http.request(self._MGET, self._base_uri + self.version_prefix + \\'/machines\\', **kwargs)\\n                data = self._handle_server_response(response).data.decode(\\'utf-8\\')\\n                machines = [m.strip() for m in data.split(\\',\\') if m.strip()]\\n                logger.debug(\"Retrieved list of machines: %s\", machines)\\n                if not machines:\\n                    raise etcd.EtcdException\\n                random.shuffle(machines)\\n                for url in machines:\\n                    r = urlparse(url)\\n                    port = r.port or (443 if r.scheme == \\'https\\' else 80)\\n                    self._dns_resolver.resolve_async(r.hostname, port)\\n                return machines\\n            except Exception as e:\\n                # We can\\'t get the list of machines, if one server is in the\\n                # machines cache, try on it\\n                logger.error(\"Failed to get list of machines from %s%s: %r\", self._base_uri, self.version_prefix, e)\\n                if self._machines_cache:\\n                    self._base_uri = self._machines_cache.pop(0)\\n                    logger.info(\"Retrying on %s\", self._base_uri)\\n                elif self._update_machines_cache:\\n                    raise etcd.EtcdException(\"Could not get the list of servers, \"\\n                                             \"maybe you provided the wrong \"\\n                                             \"host(s) to connect to?\")\\n                else:\\n                    return []', 'def _get_machines_cache_from_srv(self, srv):\\n        \"\"\"Fetch list of etcd-cluster member by resolving _etcd-server._tcp. SRV record.\\n        This record should contain list of host and peer ports which could be used to run\\n        \\'GET http://{host}:{port}/members\\' request (peer protocol)\"\"\"\\n\\n        ret = []\\n        for r in [\\'-client-ssl\\', \\'-client\\', \\'-ssl\\', \\'\\', \\'-server-ssl\\', \\'-server\\']:\\n            protocol = \\'https\\' if \\'-ssl\\' in r else \\'http\\'\\n            endpoint = \\'/members\\' if \\'-server\\' in r else \\'\\'\\n            for host, port in self.get_srv_record(\\'_etcd{0}._tcp.{1}\\'.format(r, srv)):\\n                url = uri(protocol, host, port, endpoint)\\n                if endpoint:\\n                    try:\\n                        response = requests.get(url, timeout=self.read_timeout, verify=False)\\n                        if response.ok:\\n                            for member in response.json():\\n                                ret.extend(member[\\'clientURLs\\'])\\n                            break\\n                    except RequestException:\\n                        logger.exception(\\'GET %s\\', url)\\n                else:\\n                    ret.append(url)\\n            if ret:\\n                self._protocol = protocol\\n                break\\n        else:\\n            logger.warning(\\'Can not resolve SRV for %s\\', srv)\\n        return list(set(ret))', 'def _get_machines_cache_from_dns(self, host, port):\\n        \"\"\"One host might be resolved into multiple ip addresses. We will make list out of it\"\"\"\\n        if self.protocol == \\'http\\':\\n            ret = []\\n            for af, _, _, _, sa in self._dns_resolver.resolve(host, port):\\n                host, port = sa[:2]\\n                if af == socket.AF_INET6:\\n                    host = \\'[{0}]\\'.format(host)\\n                ret.append(uri(self.protocol, host, port))\\n            if ret:\\n                return list(set(ret))\\n        return [uri(self.protocol, host, port)]', 'def _load_machines_cache(self):\\n        \"\"\"This method should fill up `_machines_cache` from scratch.\\n        It could happen only in two cases:\\n        1. During class initialization\\n        2. When all etcd members failed\"\"\"\\n\\n        self._update_machines_cache = True\\n\\n        if \\'srv\\' not in self._config and \\'host\\' not in self._config and \\'hosts\\' not in self._config:\\n            raise Exception(\\'Neither srv, hosts, host nor url are defined in etcd section of config\\')\\n\\n        self._machines_cache = self._get_machines_cache_from_config()\\n\\n        # Can not bootstrap list of etcd-cluster members, giving up\\n        if not self._machines_cache:\\n            raise etcd.EtcdException\\n\\n        # After filling up initial list of machines_cache we should ask etcd-cluster about actual list\\n        self._base_uri = self._next_server()\\n        self._refresh_machines_cache()\\n\\n        self._update_machines_cache = False\\n        self._machines_cache_updated = time.time()', 'def pg_ctl(self, cmd, *args, **kwargs):\\n        \"\"\"Builds and executes pg_ctl command\\n\\n        :returns: `!True` when return_code == 0, otherwise `!False`\"\"\"\\n\\n        pg_ctl = [self._pgcommand(\\'pg_ctl\\'), cmd]\\n        return subprocess.call(pg_ctl + [\\'-D\\', self._data_dir] + list(args), **kwargs) == 0', 'def pg_isready(self):\\n        \"\"\"Runs pg_isready to see if PostgreSQL is accepting connections.\\n\\n        :returns: \\'ok\\' if PostgreSQL is up, \\'reject\\' if starting up, \\'no_resopnse\\' if not up.\"\"\"\\n\\n        cmd = [self._pgcommand(\\'pg_isready\\'), \\'-p\\', self._local_address[\\'port\\'], \\'-d\\', self._database]\\n\\n        # Host is not set if we are connecting via default unix socket\\n        if \\'host\\' in self._local_address:\\n            cmd.extend([\\'-h\\', self._local_address[\\'host\\']])\\n\\n        # We only need the username because pg_isready does not try to authenticate\\n        if \\'username\\' in self._superuser:\\n            cmd.extend([\\'-U\\', self._superuser[\\'username\\']])\\n\\n        ret = subprocess.call(cmd)\\n        return_codes = {0: STATE_RUNNING,\\n                        1: STATE_REJECT,\\n                        2: STATE_NO_RESPONSE,\\n                        3: STATE_UNKNOWN}\\n        return return_codes.get(ret, STATE_UNKNOWN)', 'def can_rewind(self):\\n        \"\"\" check if pg_rewind executable is there and that pg_controldata indicates\\n            we have either wal_log_hints or checksums turned on\\n        \"\"\"\\n        # low-hanging fruit: check if pg_rewind configuration is there\\n        if not self.config.get(\\'use_pg_rewind\\'):\\n            return False\\n\\n        cmd = [self._pgcommand(\\'pg_rewind\\'), \\'--help\\']\\n        try:\\n            ret = subprocess.call(cmd, stdout=open(os.devnull, \\'w\\'), stderr=subprocess.STDOUT)\\n            if ret != 0:  # pg_rewind is not there, close up the shop and go home\\n                return False\\n        except OSError:\\n            return False\\n        return self.configuration_allows_rewind(self.controldata())', 'def _query(self, sql, *params):\\n        \"\"\"We are always using the same cursor, therefore this method is not thread-safe!!!\\n        You can call it from different threads only if you are holding explicit `AsyncExecutor` lock,\\n        because the main thread is always holding this lock when running HA cycle.\"\"\"\\n        cursor = None\\n        try:\\n            cursor = self._cursor()\\n            cursor.execute(sql, params)\\n            return cursor\\n        except psycopg2.Error as e:\\n            if cursor and cursor.connection.closed == 0:\\n                # When connected via unix socket, psycopg2 can\\'t recoginze \\'connection lost\\'\\n                # and leaves `_cursor_holder.connection.closed == 0`, but psycopg2.OperationalError\\n                # is still raised (what is correct). It doesn\\'t make sense to continiue with existing\\n                # connection and we will close it, to avoid its reuse by the `_cursor` method.\\n                if isinstance(e, psycopg2.OperationalError):\\n                    self.close_connection()\\n                else:\\n                    raise e\\n            if self.state == \\'restarting\\':\\n                raise RetryFailedError(\\'cluster is being restarted\\')\\n            raise PostgresConnectionException(\\'connection problems\\')', 'def run_bootstrap_post_init(self, config):\\n        \"\"\"\\n        runs a script after initdb or custom bootstrap script is called and waits until completion.\\n        \"\"\"\\n        cmd = config.get(\\'post_bootstrap\\') or config.get(\\'post_init\\')\\n        if cmd:\\n            r = self._local_connect_kwargs\\n\\n            if \\'host\\' in r:\\n                # \\'/tmp\\' => \\'%2Ftmp\\' for unix socket path\\n                host = quote_plus(r[\\'host\\']) if r[\\'host\\'].startswith(\\'/\\') else r[\\'host\\']\\n            else:\\n                host = \\'\\'\\n\\n                # https://www.postgresql.org/docs/current/static/libpq-pgpass.html\\n                # A host name of localhost matches both TCP (host name localhost) and Unix domain socket\\n                # (pghost empty or the default socket directory) connections coming from the local machine.\\n                r[\\'host\\'] = \\'localhost\\'  # set it to localhost to write into pgpass\\n\\n            if \\'user\\' in r:\\n                user = r[\\'user\\'] + \\'@\\'\\n            else:\\n                user = \\'\\'\\n                if \\'password\\' in r:\\n                    import getpass\\n                    r.setdefault(\\'user\\', os.environ.get(\\'PGUSER\\', getpass.getuser()))\\n\\n            connstring = \\'postgres://{0}{1}:{2}/{3}\\'.format(user, host, r[\\'port\\'], r[\\'database\\'])\\n            env = self.write_pgpass(r) if \\'password\\' in r else None\\n\\n            try:\\n                ret = self.cancellable_subprocess_call(shlex.split(cmd) + [connstring], env=env)\\n            except OSError:\\n                logger.error(\\'post_init script %s failed\\', cmd)\\n                return False\\n            if ret != 0:\\n                logger.error(\\'post_init script %s returned non-zero code %d\\', cmd, ret)\\n                return False\\n        return True', 'def can_create_replica_without_replication_connection(self):\\n        \"\"\" go through the replication methods to see if there are ones\\n            that does not require a working replication connection.\\n        \"\"\"\\n        replica_methods = self._create_replica_methods\\n        return any(self.replica_method_can_work_without_replication_connection(method) for method in replica_methods)', 'def create_replica(self, clone_member):\\n        \"\"\"\\n            create the replica according to the replica_method\\n            defined by the user.  this is a list, so we need to\\n            loop through all methods the user supplies\\n        \"\"\"\\n\\n        self.set_state(\\'creating replica\\')\\n        self._sysid = None\\n\\n        is_remote_master = isinstance(clone_member, RemoteMember)\\n        create_replica_methods = is_remote_master and clone_member.create_replica_methods\\n\\n        # get list of replica methods either from clone member or from\\n        # the config. If there is no configuration key, or no value is\\n        # specified, use basebackup\\n        replica_methods = (\\n            create_replica_methods\\n            or self._create_replica_methods\\n            or [\\'basebackup\\']\\n        )\\n\\n        if clone_member and clone_member.conn_url:\\n            r = clone_member.conn_kwargs(self._replication)\\n            connstring = \\'postgres://{user}@{host}:{port}/{database}\\'.format(**r)\\n            # add the credentials to connect to the replica origin to pgpass.\\n            env = self.write_pgpass(r)\\n        else:\\n            connstring = \\'\\'\\n            env = os.environ.copy()\\n            # if we don\\'t have any source, leave only replica methods that work without it\\n            replica_methods = \\\\\\n                [r for r in replica_methods if self.replica_method_can_work_without_replication_connection(r)]\\n\\n        # go through them in priority order\\n        ret = 1\\n        for replica_method in replica_methods:\\n            with self._cancellable_lock:\\n                if self._is_cancelled:\\n                    break\\n            # if the method is basebackup, then use the built-in\\n            if replica_method == \"basebackup\":\\n                ret = self.basebackup(connstring, env, self.config.get(replica_method, {}))\\n                if ret == 0:\\n                    logger.info(\"replica has been created using basebackup\")\\n                    # if basebackup succeeds, exit with success\\n                    break\\n            else:\\n                if not self.data_directory_empty():\\n                    if self.config.get(replica_method, {}).get(\\'keep_data\\', False):\\n                        logger.info(\\'Leaving data directory uncleaned\\')\\n                    else:\\n                        self.remove_data_directory()\\n\\n                cmd = replica_method\\n                method_config = {}\\n                # user-defined method; check for configuration\\n                # not required, actually\\n                if self.config.get(replica_method, {}):\\n                    method_config = self.config[replica_method].copy()\\n                    # look to see if the user has supplied a full command path\\n                    # if not, use the method name as the command\\n                    cmd = method_config.pop(\\'command\\', cmd)\\n\\n                # add the default parameters\\n                if not method_config.get(\\'no_params\\', False):\\n                    method_config.update({\"scope\": self.scope,\\n                                          \"role\": \"replica\",\\n                                          \"datadir\": self._data_dir,\\n                                          \"connstring\": connstring})\\n                else:\\n                    for param in (\\'no_params\\', \\'no_master\\', \\'keep_data\\'):\\n                        method_config.pop(param, None)\\n                params = [\"--{0}={1}\".format(arg, val) for arg, val in method_config.items()]\\n                try:\\n                    # call script with the full set of parameters\\n                    ret = self.cancellable_subprocess_call(shlex.split(cmd) + params, env=env)\\n                    # if we succeeded, stop\\n                    if ret == 0:\\n                        logger.info(\\'replica has been created using %s\\', replica_method)\\n                        break\\n                    else:\\n                        logger.error(\\'Error creating replica using method %s: %s exited with code=%s\\',\\n                                     replica_method, cmd, ret)\\n                except Exception:\\n                    logger.exception(\\'Error creating replica using method %s\\', replica_method)\\n                    ret = 1\\n\\n        self.set_state(\\'stopped\\')\\n        return ret', 'def is_running(self):\\n        \"\"\"Returns PostmasterProcess if one is running on the data directory or None. If most recently seen process\\n        is running updates the cached process based on pid file.\"\"\"\\n        if self._postmaster_proc:\\n            if self._postmaster_proc.is_running():\\n                return self._postmaster_proc\\n            self._postmaster_proc = None\\n\\n        # we noticed that postgres was restarted, force syncing of replication\\n        self._schedule_load_slots = self.use_slots\\n\\n        self._postmaster_proc = PostmasterProcess.from_pidfile(self._data_dir)\\n        return self._postmaster_proc', 'def call_nowait(self, cb_name):\\n        \"\"\" pick a callback command and call it without waiting for it to finish \"\"\"\\n        if self.bootstrapping:\\n            return\\n        if cb_name in (ACTION_ON_START, ACTION_ON_STOP, ACTION_ON_RESTART, ACTION_ON_ROLE_CHANGE):\\n            self.__cb_called = True\\n\\n        if self.callback and cb_name in self.callback:\\n            cmd = self.callback[cb_name]\\n            try:\\n                cmd = shlex.split(self.callback[cb_name]) + [cb_name, self.role, self.scope]\\n                self._callback_executor.call(cmd)\\n            except Exception:\\n                logger.exception(\\'callback %s %s %s %s failed\\', cmd, cb_name, self.role, self.scope)', 'def wait_for_port_open(self, postmaster, timeout):\\n        \"\"\"Waits until PostgreSQL opens ports.\"\"\"\\n        for _ in polling_loop(timeout):\\n            with self._cancellable_lock:\\n                if self._is_cancelled:\\n                    return False\\n\\n            if not postmaster.is_running():\\n                logger.error(\\'postmaster is not running\\')\\n                self.set_state(\\'start failed\\')\\n                return False\\n\\n            isready = self.pg_isready()\\n            if isready != STATE_NO_RESPONSE:\\n                if isready not in [STATE_REJECT, STATE_RUNNING]:\\n                    logger.warning(\"Can\\'t determine PostgreSQL startup status, assuming running\")\\n                return True\\n\\n        logger.warning(\"Timed out waiting for PostgreSQL to start\")\\n        return False', 'def _build_effective_configuration(self):\\n        \"\"\"It might happen that the current value of one (or more) below parameters stored in\\n        the controldata is higher than the value stored in the global cluster configuration.\\n\\n        Example: max_connections in global configuration is 100, but in controldata\\n        `Current max_connections setting: 200`. If we try to start postgres with\\n        max_connections=100, it will immediately exit.\\n        As a workaround we will start it with the values from controldata and set `pending_restart`\\n        to true as an indicator that current values of parameters are not matching expectations.\"\"\"\\n\\n        OPTIONS_MAPPING = {\\n            \\'max_connections\\': \\'max_connections setting\\',\\n            \\'max_prepared_transactions\\': \\'max_prepared_xacts setting\\',\\n            \\'max_locks_per_transaction\\': \\'max_locks_per_xact setting\\'\\n        }\\n\\n        if self._major_version >= 90400:\\n            OPTIONS_MAPPING[\\'max_worker_processes\\'] = \\'max_worker_processes setting\\'\\n\\n        data = self.controldata()\\n        effective_configuration = self._server_parameters.copy()\\n\\n        for name, cname in OPTIONS_MAPPING.items():\\n            value = parse_int(effective_configuration[name])\\n            cvalue = parse_int(data[cname])\\n            if cvalue > value:\\n                effective_configuration[name] = cvalue\\n                self._pending_restart = True\\n        return effective_configuration', 'def start(self, timeout=None, task=None, block_callbacks=False, role=None):\\n        \"\"\"Start PostgreSQL\\n\\n        Waits for postmaster to open ports or terminate so pg_isready can be used to check startup completion\\n        or failure.\\n\\n        :returns: True if start was initiated and postmaster ports are open, False if start failed\"\"\"\\n        # make sure we close all connections established against\\n        # the former node, otherwise, we might get a stalled one\\n        # after kill -9, which would report incorrect data to\\n        # patroni.\\n        self.close_connection()\\n\\n        if self.is_running():\\n            logger.error(\\'Cannot start PostgreSQL because one is already running.\\')\\n            self.set_state(\\'starting\\')\\n            return True\\n\\n        if not block_callbacks:\\n            self.__cb_pending = ACTION_ON_START\\n\\n        self.set_role(role or self.get_postgres_role_from_data_directory())\\n\\n        self.set_state(\\'starting\\')\\n        self._pending_restart = False\\n\\n        configuration = self._server_parameters if self.role == \\'master\\' else self._build_effective_configuration()\\n        self._write_postgresql_conf(configuration)\\n        self.resolve_connection_addresses()\\n        self._replace_pg_hba()\\n        self._replace_pg_ident()\\n\\n        options = [\\'--{0}={1}\\'.format(p, configuration[p]) for p in self.CMDLINE_OPTIONS\\n                   if p in configuration and p != \\'wal_keep_segments\\']\\n\\n        with self._cancellable_lock:\\n            if self._is_cancelled:\\n                return False\\n\\n        with task or null_context():\\n            if task and task.is_cancelled:\\n                logger.info(\"PostgreSQL start cancelled.\")\\n                return False\\n\\n            self._postmaster_proc = PostmasterProcess.start(self._pgcommand(\\'postgres\\'),\\n                                                            self._data_dir,\\n                                                            self._postgresql_conf,\\n                                                            options)\\n\\n            if task:\\n                task.complete(self._postmaster_proc)\\n\\n        start_timeout = timeout\\n        if not start_timeout:\\n            try:\\n                start_timeout = float(self.config.get(\\'pg_ctl_timeout\\', 60))\\n            except ValueError:\\n                start_timeout = 60\\n\\n        # We want postmaster to open ports before we continue\\n        if not self._postmaster_proc or not self.wait_for_port_open(self._postmaster_proc, start_timeout):\\n            return False\\n\\n        ret = self.wait_for_startup(start_timeout)\\n        if ret is not None:\\n            return ret\\n        elif timeout is not None:\\n            return False\\n        else:\\n            return None', 'def stop(self, mode=\\'fast\\', block_callbacks=False, checkpoint=None, on_safepoint=None):\\n        \"\"\"Stop PostgreSQL\\n\\n        Supports a callback when a safepoint is reached. A safepoint is when no user backend can return a successful\\n        commit to users. Currently this means we wait for user backends to close. But in the future alternate mechanisms\\n        could be added.\\n\\n        :param on_safepoint: This callback is called when no user backends are running.\\n        \"\"\"\\n        if checkpoint is None:\\n            checkpoint = False if mode == \\'immediate\\' else True\\n\\n        success, pg_signaled = self._do_stop(mode, block_callbacks, checkpoint, on_safepoint)\\n        if success:\\n            # block_callbacks is used during restart to avoid\\n            # running start/stop callbacks in addition to restart ones\\n            if not block_callbacks:\\n                self.set_state(\\'stopped\\')\\n                if pg_signaled:\\n                    self.call_nowait(ACTION_ON_STOP)\\n        else:\\n            logger.warning(\\'pg_ctl stop failed\\')\\n            self.set_state(\\'stop failed\\')\\n        return success', 'def check_startup_state_changed(self):\\n        \"\"\"Checks if PostgreSQL has completed starting up or failed or still starting.\\n\\n        Should only be called when state == \\'starting\\'\\n\\n        :returns: True if state was changed from \\'starting\\'\\n        \"\"\"\\n        ready = self.pg_isready()\\n\\n        if ready == STATE_REJECT:\\n            return False\\n        elif ready == STATE_NO_RESPONSE:\\n            self.set_state(\\'start failed\\')\\n            self._schedule_load_slots = False  # TODO: can remove this?\\n            if not self._running_custom_bootstrap:\\n                self.save_configuration_files()  # TODO: maybe remove this?\\n            return True\\n        else:\\n            if ready != STATE_RUNNING:\\n                # Bad configuration or unexpected OS error. No idea of PostgreSQL status.\\n                # Let the main loop of run cycle clean up the mess.\\n                logger.warning(\"%s status returned from pg_isready\",\\n                               \"Unknown\" if ready == STATE_UNKNOWN else \"Invalid\")\\n            self.set_state(\\'running\\')\\n            self._schedule_load_slots = self.use_slots\\n            if not self._running_custom_bootstrap:\\n                self.save_configuration_files()\\n            # TODO: __cb_pending can be None here after PostgreSQL restarts on its own. Do we want to call the callback?\\n            # Previously we didn\\'t even notice.\\n            action = self.__cb_pending or ACTION_ON_START\\n            self.call_nowait(action)\\n            self.__cb_pending = None\\n\\n            return True', 'def wait_for_startup(self, timeout=None):\\n        \"\"\"Waits for PostgreSQL startup to complete or fail.\\n\\n        :returns: True if start was successful, False otherwise\"\"\"\\n        if not self.is_starting():\\n            # Should not happen\\n            logger.warning(\"wait_for_startup() called when not in starting state\")\\n\\n        while not self.check_startup_state_changed():\\n            with self._cancellable_lock:\\n                if self._is_cancelled:\\n                    return None\\n            if timeout and self.time_in_state() > timeout:\\n                return None\\n            time.sleep(1)\\n\\n        return self.state == \\'running\\'', 'def restart(self, timeout=None, task=None, block_callbacks=False, role=None):\\n        \"\"\"Restarts PostgreSQL.\\n\\n        When timeout parameter is set the call will block either until PostgreSQL has started, failed to start or\\n        timeout arrives.\\n\\n        :returns: True when restart was successful and timeout did not expire when waiting.\\n        \"\"\"\\n        self.set_state(\\'restarting\\')\\n        if not block_callbacks:\\n            self.__cb_pending = ACTION_ON_RESTART\\n        ret = self.stop(block_callbacks=True) and self.start(timeout, task, True, role)\\n        if not ret and not self.is_starting():\\n            self.set_state(\\'restart failed ({0})\\'.format(self.state))\\n        return ret', 'def _replace_pg_hba(self):\\n        \"\"\"\\n        Replace pg_hba.conf content in the PGDATA if hba_file is not defined in the\\n        `postgresql.parameters` and pg_hba is defined in `postgresql` configuration section.\\n\\n        :returns: True if pg_hba.conf was rewritten.\\n        \"\"\"\\n\\n        # when we are doing custom bootstrap we assume that we don\\'t know superuser password\\n        # and in order to be able to change it, we are opening trust access from a certain address\\n        if self._running_custom_bootstrap:\\n            addresses = {\\'\\': \\'local\\'}\\n            if \\'host\\' in self._local_address and not self._local_address[\\'host\\'].startswith(\\'/\\'):\\n                for _, _, _, _, sa in socket.getaddrinfo(self._local_address[\\'host\\'], self._local_address[\\'port\\'],\\n                                                         0, socket.SOCK_STREAM, socket.IPPROTO_TCP):\\n                    addresses[sa[0] + \\'/32\\'] = \\'host\\'\\n\\n            with open(self._pg_hba_conf, \\'w\\') as f:\\n                f.write(self._CONFIG_WARNING_HEADER)\\n                for address, t in addresses.items():\\n                    f.write((\\n                        \\'{0}\\\\treplication\\\\t{1}\\\\t{3}\\\\ttrust\\\\n\\'\\n                        \\'{0}\\\\tall\\\\t{2}\\\\t{3}\\\\ttrust\\\\n\\'\\n                    ).format(t, self._replication[\\'username\\'], self._superuser.get(\\'username\\') or \\'all\\', address))\\n        elif not self._server_parameters.get(\\'hba_file\\') and self.config.get(\\'pg_hba\\'):\\n            with open(self._pg_hba_conf, \\'w\\') as f:\\n                f.write(self._CONFIG_WARNING_HEADER)\\n                for line in self.config[\\'pg_hba\\']:\\n                    f.write(\\'{0}\\\\n\\'.format(line))\\n            return True', 'def _replace_pg_ident(self):\\n        \"\"\"\\n        Replace pg_ident.conf content in the PGDATA if ident_file is not defined in the\\n        `postgresql.parameters` and pg_ident is defined in the `postgresql` section.\\n\\n        :returns: True if pg_ident.conf was rewritten.\\n        \"\"\"\\n\\n        if not self._server_parameters.get(\\'ident_file\\') and self.config.get(\\'pg_ident\\'):\\n            with open(self._pg_ident_conf, \\'w\\') as f:\\n                f.write(self._CONFIG_WARNING_HEADER)\\n                for line in self.config[\\'pg_ident\\']:\\n                    f.write(\\'{0}\\\\n\\'.format(line))\\n            return True', 'def controldata(self):\\n        \"\"\" return the contents of pg_controldata, or non-True value if pg_controldata call failed \"\"\"\\n        result = {}\\n        # Don\\'t try to call pg_controldata during backup restore\\n        if self._version_file_exists() and self.state != \\'creating replica\\':\\n            try:\\n                env = {\\'LANG\\': \\'C\\', \\'LC_ALL\\': \\'C\\', \\'PATH\\': os.getenv(\\'PATH\\')}\\n                if os.getenv(\\'SYSTEMROOT\\') is not None:\\n                    env[\\'SYSTEMROOT\\'] = os.getenv(\\'SYSTEMROOT\\')\\n                data = subprocess.check_output([self._pgcommand(\\'pg_controldata\\'), self._data_dir], env=env)\\n                if data:\\n                    data = data.decode(\\'utf-8\\').splitlines()\\n                    # pg_controldata output depends on major verion. Some of parameters are prefixed by \\'Current \\'\\n                    result = {l.split(\\':\\')[0].replace(\\'Current \\', \\'\\', 1): l.split(\\':\\', 1)[1].strip() for l in data\\n                              if l and \\':\\' in l}\\n            except subprocess.CalledProcessError:\\n                logger.exception(\"Error when calling pg_controldata\")\\n        return result', 'def save_configuration_files(self):\\n        \"\"\"\\n            copy postgresql.conf to postgresql.conf.backup to be able to retrive configuration files\\n            - originally stored as symlinks, those are normally skipped by pg_basebackup\\n            - in case of WAL-E basebackup (see http://comments.gmane.org/gmane.comp.db.postgresql.wal-e/239)\\n        \"\"\"\\n        try:\\n            for f in self._configuration_to_save:\\n                config_file = os.path.join(self._config_dir, f)\\n                backup_file = os.path.join(self._data_dir, f + \\'.backup\\')\\n                if os.path.isfile(config_file):\\n                    shutil.copy(config_file, backup_file)\\n        except IOError:\\n            logger.exception(\\'unable to create backup copies of configuration files\\')\\n        return True', 'def restore_configuration_files(self):\\n        \"\"\" restore a previously saved postgresql.conf \"\"\"\\n        try:\\n            for f in self._configuration_to_save:\\n                config_file = os.path.join(self._config_dir, f)\\n                backup_file = os.path.join(self._data_dir, f + \\'.backup\\')\\n                if not os.path.isfile(config_file):\\n                    if os.path.isfile(backup_file):\\n                        shutil.copy(backup_file, config_file)\\n                    # Previously we didn\\'t backup pg_ident.conf, if file is missing just create empty\\n                    elif f == \\'pg_ident.conf\\':\\n                        open(config_file, \\'w\\').close()\\n        except IOError:\\n            logger.exception(\\'unable to restore configuration files from backup\\')', 'def clone(self, clone_member):\\n        \"\"\"\\n             - initialize the replica from an existing member (master or replica)\\n             - initialize the replica using the replica creation method that\\n               works without the replication connection (i.e. restore from on-disk\\n               base backup)\\n        \"\"\"\\n\\n        self._rewind_state = REWIND_STATUS.INITIAL\\n        ret = self.create_replica(clone_member) == 0\\n        if ret:\\n            self._post_restore()\\n            self._configure_server_parameters()\\n        return ret', 'def bootstrap(self, config):\\n        \"\"\" Initialize a new node from scratch and start it. \"\"\"\\n        pg_hba = config.get(\\'pg_hba\\', [])\\n        method = config.get(\\'method\\') or \\'initdb\\'\\n        self._running_custom_bootstrap = method != \\'initdb\\' and method in config and \\'command\\' in config[method]\\n        if self._running_custom_bootstrap:\\n            do_initialize = self._custom_bootstrap\\n            config = config[method]\\n        else:\\n            do_initialize = self._initdb\\n        return do_initialize(config) and self.append_pg_hba(pg_hba) and self.save_configuration_files() \\\\\\n            and self._configure_server_parameters() and self.start()', 'def pick_synchronous_standby(self, cluster):\\n        \"\"\"Finds the best candidate to be the synchronous standby.\\n\\n        Current synchronous standby is always preferred, unless it has disconnected or does not want to be a\\n        synchronous standby any longer.\\n\\n        :returns tuple of candidate name or None, and bool showing if the member is the active synchronous standby.\\n        \"\"\"\\n        current = cluster.sync.sync_standby\\n        current = current.lower() if current else current\\n        members = {m.name.lower(): m for m in cluster.members}\\n        candidates = []\\n        # Pick candidates based on who has flushed WAL farthest.\\n        # TODO: for synchronous_commit = remote_write we actually want to order on write_location\\n        for app_name, state, sync_state in self.query(\\n                \"SELECT pg_catalog.lower(application_name), state, sync_state\"\\n                \" FROM pg_catalog.pg_stat_replication\"\\n                \" ORDER BY flush_{0} DESC\".format(self.lsn_name)):\\n            member = members.get(app_name)\\n            if state != \\'streaming\\' or not member or member.tags.get(\\'nosync\\', False):\\n                continue\\n            if sync_state == \\'sync\\':\\n                return app_name, True\\n            if sync_state == \\'potential\\' and app_name == current:\\n                # Prefer current even if not the best one any more to avoid indecisivness and spurious swaps.\\n                return current, False\\n            if sync_state in (\\'async\\', \\'potential\\'):\\n                candidates.append(app_name)\\n\\n        if candidates:\\n            return candidates[0], False\\n        return None, False', 'def set_synchronous_standby(self, name):\\n        \"\"\"Sets a node to be synchronous standby and if changed does a reload for PostgreSQL.\"\"\"\\n        if name and name != \\'*\\':\\n            name = quote_ident(name)\\n        if name != self._synchronous_standby_names:\\n            if name is None:\\n                self._server_parameters.pop(\\'synchronous_standby_names\\', None)\\n            else:\\n                self._server_parameters[\\'synchronous_standby_names\\'] = name\\n            self._synchronous_standby_names = name\\n            if self.state == \\'running\\':\\n                self._write_postgresql_conf()\\n                self.reload()', 'def postgres_version_to_int(pg_version):\\n        \"\"\"Convert the server_version to integer\\n\\n        >>> Postgresql.postgres_version_to_int(\\'9.5.3\\')\\n        90503\\n        >>> Postgresql.postgres_version_to_int(\\'9.3.13\\')\\n        90313\\n        >>> Postgresql.postgres_version_to_int(\\'10.1\\')\\n        100001\\n        >>> Postgresql.postgres_version_to_int(\\'10\\')  # doctest: +IGNORE_EXCEPTION_DETAIL\\n        Traceback (most recent call last):\\n            ...\\n        PostgresException: \\'Invalid PostgreSQL version format: X.Y or X.Y.Z is accepted: 10\\'\\n        >>> Postgresql.postgres_version_to_int(\\'9.6\\')  # doctest: +IGNORE_EXCEPTION_DETAIL\\n        Traceback (most recent call last):\\n            ...\\n        PostgresException: \\'Invalid PostgreSQL version format: X.Y or X.Y.Z is accepted: 9.6\\'\\n        >>> Postgresql.postgres_version_to_int(\\'a.b.c\\')  # doctest: +IGNORE_EXCEPTION_DETAIL\\n        Traceback (most recent call last):\\n            ...\\n        PostgresException: \\'Invalid PostgreSQL version: a.b.c\\'\\n        \"\"\"\\n\\n        try:\\n            components = list(map(int, pg_version.split(\\'.\\')))\\n        except ValueError:\\n            raise PostgresException(\\'Invalid PostgreSQL version: {0}\\'.format(pg_version))\\n\\n        if len(components) < 2 or len(components) == 2 and components[0] < 10 or len(components) > 3:\\n            raise PostgresException(\\'Invalid PostgreSQL version format: X.Y or X.Y.Z is accepted: {0}\\'\\n                                    .format(pg_version))\\n\\n        if len(components) == 2:\\n            # new style verion numbers, i.e. 10.1 becomes 100001\\n            components.insert(1, 0)\\n\\n        return int(\\'\\'.join(\\'{0:02d}\\'.format(c) for c in components))', 'def read_postmaster_opts(self):\\n        \"\"\"returns the list of option names/values from postgres.opts, Empty dict if read failed or no file\"\"\"\\n        result = {}\\n        try:\\n            with open(os.path.join(self._data_dir, \\'postmaster.opts\\')) as f:\\n                data = f.read()\\n                for opt in data.split(\\'\" \"\\'):\\n                    if \\'=\\' in opt and opt.startswith(\\'--\\'):\\n                        name, val = opt.split(\\'=\\', 1)\\n                        result[name.strip(\\'-\\')] = val.rstrip(\\'\"\\\\n\\')\\n        except IOError:\\n            logger.exception(\\'Error when reading postmaster.opts\\')\\n        return result', 'def single_user_mode(self, command=None, options=None):\\n        \"\"\"run a given command in a single-user mode. If the command is empty - then just start and stop\"\"\"\\n        cmd = [self._pgcommand(\\'postgres\\'), \\'--single\\', \\'-D\\', self._data_dir]\\n        for opt, val in sorted((options or {}).items()):\\n            cmd.extend([\\'-c\\', \\'{0}={1}\\'.format(opt, val)])\\n        # need a database name to connect\\n        cmd.append(self._database)\\n        return self.cancellable_subprocess_call(cmd, communicate_input=command)', 'def deep_compare(obj1, obj2):\\n    \"\"\"\\n    >>> deep_compare({\\'1\\': None}, {})\\n    False\\n    >>> deep_compare({\\'1\\': {}}, {\\'1\\': None})\\n    False\\n    >>> deep_compare({\\'1\\': [1]}, {\\'1\\': [2]})\\n    False\\n    >>> deep_compare({\\'1\\': 2}, {\\'1\\': \\'2\\'})\\n    True\\n    >>> deep_compare({\\'1\\': {\\'2\\': [3, 4]}}, {\\'1\\': {\\'2\\': [3, 4]}})\\n    True\\n    \"\"\"\\n\\n    if set(list(obj1.keys())) != set(list(obj2.keys())):  # Objects have different sets of keys\\n        return False\\n\\n    for key, value in obj1.items():\\n        if isinstance(value, dict):\\n            if not (isinstance(obj2[key], dict) and deep_compare(value, obj2[key])):\\n                return False\\n        elif str(value) != str(obj2[key]):\\n            return False\\n    return True', 'def patch_config(config, data):\\n    \"\"\"recursively \\'patch\\' `config` with `data`\\n    :returns: `!True` if the `config` was changed\"\"\"\\n    is_changed = False\\n    for name, value in data.items():\\n        if value is None:\\n            if config.pop(name, None) is not None:\\n                is_changed = True\\n        elif name in config:\\n            if isinstance(value, dict):\\n                if isinstance(config[name], dict):\\n                    if patch_config(config[name], value):\\n                        is_changed = True\\n                else:\\n                    config[name] = value\\n                    is_changed = True\\n            elif str(config[name]) != str(value):\\n                config[name] = value\\n                is_changed = True\\n        else:\\n            config[name] = value\\n            is_changed = True\\n    return is_changed', 'def parse_bool(value):\\n    \"\"\"\\n    >>> parse_bool(1)\\n    True\\n    >>> parse_bool(\\'off\\')\\n    False\\n    >>> parse_bool(\\'foo\\')\\n    \"\"\"\\n    value = str(value).lower()\\n    if value in (\\'on\\', \\'true\\', \\'yes\\', \\'1\\'):\\n        return True\\n    if value in (\\'off\\', \\'false\\', \\'no\\', \\'0\\'):\\n        return False', 'def strtol(value, strict=True):\\n    \"\"\"As most as possible close equivalent of strtol(3) function (with base=0),\\n       used by postgres to parse parameter values.\\n    >>> strtol(0) == (0, \\'\\')\\n    True\\n    >>> strtol(1) == (1, \\'\\')\\n    True\\n    >>> strtol(9) == (9, \\'\\')\\n    True\\n    >>> strtol(\\' +0x400MB\\') == (1024, \\'MB\\')\\n    True\\n    >>> strtol(\\' -070d\\') == (-56, \\'d\\')\\n    True\\n    >>> strtol(\\' d \\') == (None, \\'d\\')\\n    True\\n    >>> strtol(\\'9s\\', False) == (9, \\'s\\')\\n    True\\n    >>> strtol(\\' s \\', False) == (1, \\'s\\')\\n    True\\n    \"\"\"\\n    value = str(value).strip()\\n    ln = len(value)\\n    i = 0\\n    # skip sign:\\n    if i < ln and value[i] in (\\'-\\', \\'+\\'):\\n        i += 1\\n\\n    # we always expect to get digit in the beginning\\n    if i < ln and value[i].isdigit():\\n        if value[i] == \\'0\\':\\n            i += 1\\n            if i < ln and value[i] in (\\'x\\', \\'X\\'):  # \\'0\\' followed by \\'x\\': HEX\\n                base = 16\\n                i += 1\\n            else:  # just starts with \\'0\\': OCT\\n                base = 8\\n        else:  # any other digit: DEC\\n            base = 10\\n\\n        ret = None\\n        while i <= ln:\\n            try:  # try to find maximally long number\\n                i += 1  # by giving to `int` longer and longer strings\\n                ret = int(value[:i], base)\\n            except ValueError:  # until we will not get an exception or end of the string\\n                i -= 1\\n                break\\n        if ret is not None:  # yay! there is a number in the beginning of the string\\n            return ret, value[i:].strip()  # return the number and the \"rest\"\\n\\n    return (None if strict else 1), value.strip()', 'def parse_int(value, base_unit=None):\\n    \"\"\"\\n    >>> parse_int(\\'1\\') == 1\\n    True\\n    >>> parse_int(\\' 0x400 MB \\', \\'16384kB\\') == 64\\n    True\\n    >>> parse_int(\\'1MB\\', \\'kB\\') == 1024\\n    True\\n    >>> parse_int(\\'1000 ms\\', \\'s\\') == 1\\n    True\\n    >>> parse_int(\\'1GB\\', \\'MB\\') is None\\n    True\\n    >>> parse_int(0) == 0\\n    True\\n    \"\"\"\\n\\n    convert = {\\n        \\'kB\\': {\\'kB\\': 1, \\'MB\\': 1024, \\'GB\\': 1024 * 1024, \\'TB\\': 1024 * 1024 * 1024},\\n        \\'ms\\': {\\'ms\\': 1, \\'s\\': 1000, \\'min\\': 1000 * 60, \\'h\\': 1000 * 60 * 60, \\'d\\': 1000 * 60 * 60 * 24},\\n        \\'s\\': {\\'ms\\': -1000, \\'s\\': 1, \\'min\\': 60, \\'h\\': 60 * 60, \\'d\\': 60 * 60 * 24},\\n        \\'min\\': {\\'ms\\': -1000 * 60, \\'s\\': -60, \\'min\\': 1, \\'h\\': 60, \\'d\\': 60 * 24}\\n    }\\n\\n    value, unit = strtol(value)\\n    if value is not None:\\n        if not unit:\\n            return value\\n\\n        if base_unit and base_unit not in convert:\\n            base_value, base_unit = strtol(base_unit, False)\\n        else:\\n            base_value = 1\\n        if base_unit in convert and unit in convert[base_unit]:\\n            multiplier = convert[base_unit][unit]\\n            if multiplier < 0:\\n                value /= -multiplier\\n            else:\\n                value *= multiplier\\n            return int(value/base_value)', 'def compare_values(vartype, unit, old_value, new_value):\\n    \"\"\"\\n    >>> compare_values(\\'enum\\', None, \\'remote_write\\', \\'REMOTE_WRITE\\')\\n    True\\n    >>> compare_values(\\'real\\', None, \\'1.23\\', 1.23)\\n    True\\n    \"\"\"\\n\\n    # if the integer or bool new_value is not correct this function will return False\\n    if vartype == \\'bool\\':\\n        old_value = parse_bool(old_value)\\n        new_value = parse_bool(new_value)\\n    elif vartype == \\'integer\\':\\n        old_value = parse_int(old_value)\\n        new_value = parse_int(new_value, unit)\\n    elif vartype == \\'enum\\':\\n        return str(old_value).lower() == str(new_value).lower()\\n    else:  # (\\'string\\', \\'real\\')\\n        return str(old_value) == str(new_value)\\n    return old_value is not None and new_value is not None and old_value == new_value', 'def polling_loop(timeout, interval=1):\\n    \"\"\"Returns an iterator that returns values until timeout has passed. Timeout is measured from start of iteration.\"\"\"\\n    start_time = time.time()\\n    iteration = 0\\n    end_time = start_time + timeout\\n    while time.time() < end_time:\\n        yield iteration\\n        iteration += 1\\n        time.sleep(interval)', 'def reset(self):\\n        \"\"\"Reset the attempt counter\"\"\"\\n        self._attempts = 0\\n        self._cur_delay = self.delay\\n        self._cur_stoptime = None', 'def copy(self):\\n        \"\"\"Return a clone of this retry manager\"\"\"\\n        return Retry(max_tries=self.max_tries, delay=self.delay, backoff=self.backoff,\\n                     max_jitter=self.max_jitter / 100.0, max_delay=self.max_delay, sleep_func=self.sleep_func,\\n                     deadline=self.deadline, retry_exceptions=self.retry_exceptions)', 'def check_auth(func):\\n    \"\"\"Decorator function to check authorization header.\\n\\n    Usage example:\\n    @check_auth\\n    def do_PUT_foo():\\n        pass\\n    \"\"\"\\n    def wrapper(handler, *args, **kwargs):\\n        if handler.check_auth_header():\\n            return func(handler, *args, **kwargs)\\n    return wrapper', 'def do_GET(self, write_status_code_only=False):\\n        \"\"\"Default method for processing all GET requests which can not be routed to other methods\"\"\"\\n\\n        path = \\'/master\\' if self.path == \\'/\\' else self.path\\n        response = self.get_postgresql_status()\\n\\n        patroni = self.server.patroni\\n        cluster = patroni.dcs.cluster\\n\\n        if not cluster and patroni.ha.is_paused():\\n            primary_status_code = 200 if response[\\'role\\'] == \\'master\\' else 503\\n        else:\\n            primary_status_code = 200 if patroni.ha.is_leader() else 503\\n\\n        replica_status_code = 200 if not patroni.noloadbalance and response.get(\\'role\\') == \\'replica\\' else 503\\n        status_code = 503\\n\\n        if patroni.ha.is_standby_cluster() and (\\'standby_leader\\' in path or \\'standby-leader\\' in path):\\n            status_code = 200 if patroni.ha.is_leader() else 503\\n        elif \\'master\\' in path or \\'leader\\' in path or \\'primary\\' in path or \\'read-write\\' in path:\\n            status_code = primary_status_code\\n        elif \\'replica\\' in path:\\n            status_code = replica_status_code\\n        elif \\'read-only\\' in path:\\n            status_code = 200 if primary_status_code == 200 else replica_status_code\\n        elif cluster:  # dcs is available\\n            is_synchronous = cluster.is_synchronous_mode() and cluster.sync \\\\\\n                    and cluster.sync.sync_standby == patroni.postgresql.name\\n            if path in (\\'/sync\\', \\'/synchronous\\') and is_synchronous:\\n                status_code = replica_status_code\\n            elif path in (\\'/async\\', \\'/asynchronous\\') and not is_synchronous:\\n                status_code = replica_status_code\\n\\n        if write_status_code_only:  # when haproxy sends OPTIONS request it reads only status code and nothing more\\n            message = self.responses[status_code][0]\\n            self.wfile.write(\\'{0} {1} {2}\\\\r\\\\n\\'.format(self.protocol_version, status_code, message).encode(\\'utf-8\\'))\\n        else:\\n            self._write_status_response(status_code, response)', 'def parse_schedule(schedule, action):\\n        \"\"\" parses the given schedule and validates at \"\"\"\\n        error = None\\n        scheduled_at = None\\n        try:\\n            scheduled_at = dateutil.parser.parse(schedule)\\n            if scheduled_at.tzinfo is None:\\n                error = \\'Timezone information is mandatory for the scheduled {0}\\'.format(action)\\n                status_code = 400\\n            elif scheduled_at < datetime.datetime.now(tzutc):\\n                error = \\'Cannot schedule {0} in the past\\'.format(action)\\n                status_code = 422\\n            else:\\n                status_code = None\\n        except (ValueError, TypeError):\\n            logger.exception(\\'Invalid scheduled %s time: %s\\', action, schedule)\\n            error = \\'Unable to parse scheduled timestamp. It should be in an unambiguous format, e.g. ISO 8601\\'\\n            status_code = 422\\n        return (status_code, error, scheduled_at)', 'def parse_request(self):\\n        \"\"\"Override parse_request method to enrich basic functionality of `BaseHTTPRequestHandler` class\\n\\n        Original class can only invoke do_GET, do_POST, do_PUT, etc method implementations if they are defined.\\n        But we would like to have at least some simple routing mechanism, i.e.:\\n        GET /uri1/part2 request should invoke `do_GET_uri1()`\\n        POST /other should invoke `do_POST_other()`\\n\\n        If the `do_<REQUEST_METHOD>_<first_part_url>` method does not exists we\\'ll fallback to original behavior.\"\"\"\\n\\n        ret = BaseHTTPRequestHandler.parse_request(self)\\n        if ret:\\n            mname = self.path.lstrip(\\'/\\').split(\\'/\\')[0]\\n            mname = self.command + (\\'_\\' + mname if mname else \\'\\')\\n            if hasattr(self, \\'do_\\' + mname):\\n                self.command = mname\\n        return ret', 'def _load_config_file(self):\\n        \"\"\"Loads config.yaml from filesystem and applies some values which were set via ENV\"\"\"\\n        with open(self._config_file) as f:\\n            config = yaml.safe_load(f)\\n            patch_config(config, self.__environment_configuration)\\n            return config', 'def slot_name_from_member_name(member_name):\\n    \"\"\"Translate member name to valid PostgreSQL slot name.\\n\\n    PostgreSQL replication slot names must be valid PostgreSQL names. This function maps the wider space of\\n    member names to valid PostgreSQL names. Names are lowercased, dashes and periods common in hostnames\\n    are replaced with underscores, other characters are encoded as their unicode codepoint. Name is truncated\\n    to 64 characters. Multiple different member names may map to a single slot name.\"\"\"\\n\\n    def replace_char(match):\\n        c = match.group(0)\\n        return \\'_\\' if c in \\'-.\\' else \"u{:04d}\".format(ord(c))\\n\\n    slot_name = re.sub(\\'[^a-z0-9_]\\', replace_char, member_name.lower())\\n    return slot_name[0:63]', 'def parse_connection_string(value):\\n    \"\"\"Original Governor stores connection strings for each cluster members if a following format:\\n        postgres://{username}:{password}@{connect_address}/postgres\\n    Since each of our patroni instances provides own REST API endpoint it\\'s good to store this information\\n    in DCS among with postgresql connection string. In order to not introduce new keys and be compatible with\\n    original Governor we decided to extend original connection string in a following way:\\n        postgres://{username}:{password}@{connect_address}/postgres?application_name={api_url}\\n    This way original Governor could use such connection string as it is, because of feature of `libpq` library.\\n\\n    This method is able to split connection string stored in DCS into two parts, `conn_url` and `api_url`\"\"\"\\n\\n    scheme, netloc, path, params, query, fragment = urlparse(value)\\n    conn_url = urlunparse((scheme, netloc, path, params, \\'\\', fragment))\\n    api_url = ([v for n, v in parse_qsl(query) if n == \\'application_name\\'] or [None])[0]\\n    return conn_url, api_url', 'def dcs_modules():\\n    \"\"\"Get names of DCS modules, depending on execution environment. If being packaged with PyInstaller,\\n    modules aren\\'t discoverable dynamically by scanning source directory because `FrozenImporter` doesn\\'t\\n    implement `iter_modules` method. But it is still possible to find all potential DCS modules by\\n    iterating through `toc`, which contains list of all \"frozen\" resources.\"\"\"\\n\\n    dcs_dirname = os.path.dirname(__file__)\\n    module_prefix = __package__ + \\'.\\'\\n\\n    if getattr(sys, \\'frozen\\', False):\\n        importer = pkgutil.get_importer(dcs_dirname)\\n        return [module for module in list(importer.toc) if module.startswith(module_prefix) and module.count(\\'.\\') == 2]\\n    else:\\n        return [module_prefix + name for _, name, is_pkg in pkgutil.iter_modules([dcs_dirname]) if not is_pkg]', 'def from_node(index, name, session, data):\\n        \"\"\"\\n        >>> Member.from_node(-1, \\'\\', \\'\\', \\'{\"conn_url\": \"postgres://foo@bar/postgres\"}\\') is not None\\n        True\\n        >>> Member.from_node(-1, \\'\\', \\'\\', \\'{\\')\\n        Member(index=-1, name=\\'\\', session=\\'\\', data={})\\n        \"\"\"\\n        if data.startswith(\\'postgres\\'):\\n            conn_url, api_url = parse_connection_string(data)\\n            data = {\\'conn_url\\': conn_url, \\'api_url\\': api_url}\\n        else:\\n            try:\\n                data = json.loads(data)\\n            except (TypeError, ValueError):\\n                data = {}\\n        return Member(index, name, session, data)', 'def from_node(index, data, modify_index=None):\\n        \"\"\"\\n        >>> ClusterConfig.from_node(1, \\'{\\') is None\\n        False\\n        \"\"\"\\n\\n        try:\\n            data = json.loads(data)\\n        except (TypeError, ValueError):\\n            data = None\\n            modify_index = 0\\n        if not isinstance(data, dict):\\n            data = {}\\n        return ClusterConfig(index, data, index if modify_index is None else modify_index)', 'def from_node(index, value):\\n        \"\"\"\\n        >>> SyncState.from_node(1, None).leader is None\\n        True\\n        >>> SyncState.from_node(1, \\'{}\\').leader is None\\n        True\\n        >>> SyncState.from_node(1, \\'{\\').leader is None\\n        True\\n        >>> SyncState.from_node(1, \\'[]\\').leader is None\\n        True\\n        >>> SyncState.from_node(1, \\'{\"leader\": \"leader\"}\\').leader == \"leader\"\\n        True\\n        >>> SyncState.from_node(1, {\"leader\": \"leader\"}).leader == \"leader\"\\n        True\\n        \"\"\"\\n        if isinstance(value, dict):\\n            data = value\\n        elif value:\\n            try:\\n                data = json.loads(value)\\n                if not isinstance(data, dict):\\n                    data = {}\\n            except (TypeError, ValueError):\\n                data = {}\\n        else:\\n            data = {}\\n        return SyncState(index, data.get(\\'leader\\'), data.get(\\'sync_standby\\'))', 'def from_node(index, value):\\n        \"\"\"\\n        >>> h = TimelineHistory.from_node(1, 2)\\n        >>> h.lines\\n        []\\n        \"\"\"\\n        try:\\n            lines = json.loads(value)\\n        except (TypeError, ValueError):\\n            lines = None\\n        if not isinstance(lines, list):\\n            lines = []\\n        return TimelineHistory(index, value, lines)', 'def timeline(self):\\n        \"\"\"\\n        >>> Cluster(0, 0, 0, 0, 0, 0, 0, 0).timeline\\n        0\\n        >>> Cluster(0, 0, 0, 0, 0, 0, 0, TimelineHistory.from_node(1, \\'[]\\')).timeline\\n        1\\n        >>> Cluster(0, 0, 0, 0, 0, 0, 0, TimelineHistory.from_node(1, \\'[[\"a\"]]\\')).timeline\\n        0\\n        \"\"\"\\n        if self.history:\\n            if self.history.lines:\\n                try:\\n                    return int(self.history.lines[-1][0]) + 1\\n                except Exception:\\n                    logger.error(\\'Failed to parse cluster history from DCS: %s\\', self.history.lines)\\n            elif self.history.value == \\'[]\\':\\n                return 1\\n        return 0', 'def subsets_changed(last_observed_subsets, subsets):\\n        \"\"\"\\n        >>> Kubernetes.subsets_changed([], [])\\n        False\\n        >>> Kubernetes.subsets_changed([], [k8s_client.V1EndpointSubset()])\\n        True\\n        >>> s1 = [k8s_client.V1EndpointSubset(addresses=[k8s_client.V1EndpointAddress(ip=\\'1.2.3.4\\')])]\\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=[k8s_client.V1EndpointAddress(ip=\\'1.2.3.5\\')])]\\n        >>> Kubernetes.subsets_changed(s1, s2)\\n        True\\n        >>> a = [k8s_client.V1EndpointAddress(ip=\\'1.2.3.4\\')]\\n        >>> s1 = [k8s_client.V1EndpointSubset(addresses=a, ports=[k8s_client.V1EndpointPort(protocol=\\'TCP\\', port=1)])]\\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=a, ports=[k8s_client.V1EndpointPort(port=5432)])]\\n        >>> Kubernetes.subsets_changed(s1, s2)\\n        True\\n        >>> p1 = k8s_client.V1EndpointPort(name=\\'port1\\', port=1)\\n        >>> p2 = k8s_client.V1EndpointPort(name=\\'port2\\', port=2)\\n        >>> p3 = k8s_client.V1EndpointPort(name=\\'port3\\', port=3)\\n        >>> s1 = [k8s_client.V1EndpointSubset(addresses=a, ports=[p1, p2])]\\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=a, ports=[p2, p3])]\\n        >>> Kubernetes.subsets_changed(s1, s2)\\n        True\\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=a, ports=[p2, p1])]\\n        >>> Kubernetes.subsets_changed(s1, s2)\\n        False\\n        \"\"\"\\n        if len(last_observed_subsets) != len(subsets):\\n            return True\\n        if subsets == []:\\n            return False\\n        if len(last_observed_subsets[0].addresses or []) != 1 or \\\\\\n                last_observed_subsets[0].addresses[0].ip != subsets[0].addresses[0].ip or \\\\\\n                len(last_observed_subsets[0].ports) != len(subsets[0].ports):\\n            return True\\n        if len(subsets[0].ports) == 1:\\n            return not Kubernetes.compare_ports(last_observed_subsets[0].ports[0], subsets[0].ports[0])\\n        observed_ports = {p.name: p for p in last_observed_subsets[0].ports}\\n        for p in subsets[0].ports:\\n            if p.name not in observed_ports or not Kubernetes.compare_ports(p, observed_ports.pop(p.name)):\\n                return True\\n        return False', 'def after_feature(context, feature):\\n    \"\"\" stop all Patronis, remove their data directory and cleanup the keys in etcd \"\"\"\\n    context.pctl.stop_all()\\n    shutil.rmtree(os.path.join(context.pctl.patroni_path, \\'data\\'))\\n    context.dcs_ctl.cleanup_service_tree()\\n    if feature.status == \\'failed\\':\\n        shutil.copytree(context.pctl.output_dir, context.pctl.output_dir + \\'_failed\\')', 'def stop(self, kill=False, timeout=15):\\n        \"\"\" terminate process and wipe out the temp work directory, but only if we actually started it\"\"\"\\n        super(AbstractDcsController, self).stop(kill=kill, timeout=timeout)\\n        if self._work_directory:\\n            shutil.rmtree(self._work_directory)', 'def _ioctl(self, func, arg):\\n        \"\"\"Runs the specified ioctl on the underlying fd.\\n\\n        Raises WatchdogError if the device is closed.\\n        Raises OSError or IOError (Python 2) when the ioctl fails.\"\"\"\\n        if self._fd is None:\\n            raise WatchdogError(\"Watchdog device is closed\")\\n        if os.name != \\'nt\\':\\n            import fcntl\\n            fcntl.ioctl(self._fd, func, arg, True)', 'def json_safe(string, content_type=\\'application/octet-stream\\'):\\n    \"\"\"Returns JSON-safe version of `string`.\\n\\n    If `string` is a Unicode string or a valid UTF-8, it is returned unmodified,\\n    as it can safely be encoded to JSON string.\\n\\n    If `string` contains raw/binary data, it is Base64-encoded, formatted and\\n    returned according to \"data\" URL scheme (RFC2397). Since JSON is not\\n    suitable for binary data, some additional encoding was necessary; \"data\"\\n    URL scheme was chosen for its simplicity.\\n    \"\"\"\\n    try:\\n        string = string.decode(\\'utf-8\\')\\n        json.dumps(string)\\n        return string\\n    except (ValueError, TypeError):\\n        return b\\'\\'.join([\\n            b\\'data:\\',\\n            content_type.encode(\\'utf-8\\'),\\n            b\\';base64,\\',\\n            base64.b64encode(string)\\n        ]).decode(\\'utf-8\\')', 'def get_files():\\n    \"\"\"Returns files dict from request context.\"\"\"\\n\\n    files = dict()\\n\\n    for k, v in request.files.items():\\n        content_type = request.files[k].content_type or \\'application/octet-stream\\'\\n        val = json_safe(v.read(), content_type)\\n        if files.get(k):\\n            if not isinstance(files[k], list):\\n                files[k] = [files[k]]\\n            files[k].append(val)\\n        else:\\n            files[k] = val\\n\\n    return files', 'def get_headers(hide_env=True):\\n    \"\"\"Returns headers dict from request context.\"\"\"\\n\\n    headers = dict(request.headers.items())\\n\\n    if hide_env and (\\'show_env\\' not in request.args):\\n        for key in ENV_HEADERS:\\n            try:\\n                del headers[key]\\n            except KeyError:\\n                pass\\n\\n    return CaseInsensitiveDict(headers.items())', 'def semiflatten(multi):\\n    \"\"\"Convert a MutiDict into a regular dict. If there are more than one value\\n    for a key, the result will have a list of values for the key. Otherwise it\\n    will have the plain value.\"\"\"\\n    if multi:\\n        result = multi.to_dict(flat=False)\\n        for k, v in result.items():\\n            if len(v) == 1:\\n                result[k] = v[0]\\n        return result\\n    else:\\n        return multi', 'def get_url(request):\\n    \"\"\"\\n    Since we might be hosted behind a proxy, we need to check the\\n    X-Forwarded-Proto, X-Forwarded-Protocol, or X-Forwarded-SSL headers\\n    to find out what protocol was used to access us.\\n    \"\"\"\\n    protocol = request.headers.get(\\'X-Forwarded-Proto\\') or request.headers.get(\\'X-Forwarded-Protocol\\')\\n    if protocol is None and request.headers.get(\\'X-Forwarded-Ssl\\') == \\'on\\':\\n        protocol = \\'https\\'\\n    if protocol is None:\\n        return request.url\\n    url = list(urlparse(request.url))\\n    url[0] = protocol\\n    return urlunparse(url)', 'def get_dict(*keys, **extras):\\n    \"\"\"Returns request dict of given keys.\"\"\"\\n\\n    _keys = (\\'url\\', \\'args\\', \\'form\\', \\'data\\', \\'origin\\', \\'headers\\', \\'files\\', \\'json\\', \\'method\\')\\n\\n    assert all(map(_keys.__contains__, keys))\\n    data = request.data\\n    form = semiflatten(request.form)\\n\\n    try:\\n        _json = json.loads(data.decode(\\'utf-8\\'))\\n    except (ValueError, TypeError):\\n        _json = None\\n\\n    d = dict(\\n        url=get_url(request),\\n        args=semiflatten(request.args),\\n        form=form,\\n        data=json_safe(data),\\n        origin=request.headers.get(\\'X-Forwarded-For\\', request.remote_addr),\\n        headers=get_headers(),\\n        files=get_files(),\\n        json=_json,\\n        method=request.method,\\n    )\\n\\n    out_d = dict()\\n\\n    for key in keys:\\n        out_d[key] = d.get(key)\\n\\n    out_d.update(extras)\\n\\n    return out_d', 'def status_code(code):\\n    \"\"\"Returns response object of given status code.\"\"\"\\n\\n    redirect = dict(headers=dict(location=REDIRECT_LOCATION))\\n\\n    code_map = {\\n        301: redirect,\\n        302: redirect,\\n        303: redirect,\\n        304: dict(data=\\'\\'),\\n        305: redirect,\\n        307: redirect,\\n        401: dict(headers={\\'WWW-Authenticate\\': \\'Basic realm=\"Fake Realm\"\\'}),\\n        402: dict(\\n            data=\\'Fuck you, pay me!\\',\\n            headers={\\n                \\'x-more-info\\': \\'http://vimeo.com/22053820\\'\\n            }\\n        ),\\n        406: dict(data=json.dumps({\\n                \\'message\\': \\'Client did not request a supported media type.\\',\\n                \\'accept\\': ACCEPTED_MEDIA_TYPES\\n            }),\\n            headers={\\n                \\'Content-Type\\': \\'application/json\\'\\n            }),\\n        407: dict(headers={\\'Proxy-Authenticate\\': \\'Basic realm=\"Fake Realm\"\\'}),\\n        418: dict(  # I\\'m a teapot!\\n            data=ASCII_ART,\\n            headers={\\n                \\'x-more-info\\': \\'http://tools.ietf.org/html/rfc2324\\'\\n            }\\n        ),\\n\\n    }\\n\\n    r = make_response()\\n    r.status_code = code\\n\\n    if code in code_map:\\n\\n        m = code_map[code]\\n\\n        if \\'data\\' in m:\\n            r.data = m[\\'data\\']\\n        if \\'headers\\' in m:\\n            r.headers = m[\\'headers\\']\\n\\n    return r', 'def check_basic_auth(user, passwd):\\n    \"\"\"Checks user authentication using HTTP Basic Auth.\"\"\"\\n\\n    auth = request.authorization\\n    return auth and auth.username == user and auth.password == passwd', 'def HA1(realm, username, password, algorithm):\\n    \"\"\"Create HA1 hash by realm, username, password\\n\\n    HA1 = md5(A1) = MD5(username:realm:password)\\n    \"\"\"\\n    if not realm:\\n        realm = u\\'\\'\\n    return H(b\":\".join([username.encode(\\'utf-8\\'),\\n                           realm.encode(\\'utf-8\\'),\\n                           password.encode(\\'utf-8\\')]), algorithm)', 'def HA2(credentials, request, algorithm):\\n    \"\"\"Create HA2 md5 hash\\n\\n    If the qop directive\\'s value is \"auth\" or is unspecified, then HA2:\\n        HA2 = md5(A2) = MD5(method:digestURI)\\n    If the qop directive\\'s value is \"auth-int\" , then HA2 is\\n        HA2 = md5(A2) = MD5(method:digestURI:MD5(entityBody))\\n    \"\"\"\\n    if credentials.get(\"qop\") == \"auth\" or credentials.get(\\'qop\\') is None:\\n        return H(b\":\".join([request[\\'method\\'].encode(\\'utf-8\\'), request[\\'uri\\'].encode(\\'utf-8\\')]), algorithm)\\n    elif credentials.get(\"qop\") == \"auth-int\":\\n        for k in \\'method\\', \\'uri\\', \\'body\\':\\n            if k not in request:\\n                raise ValueError(\"%s required\" % k)\\n        A2 = b\":\".join([request[\\'method\\'].encode(\\'utf-8\\'),\\n                        request[\\'uri\\'].encode(\\'utf-8\\'),\\n                        H(request[\\'body\\'], algorithm).encode(\\'utf-8\\')])\\n        return H(A2, algorithm)\\n    raise ValueError']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "0b1740bbab024b718cacf9a195dcd9e8",
            "aa22e48e39044584a08ef9d8182609e5",
            "60052a02267e4f48a947a07c4ef0f049",
            "557f1c0a6fca4696b19ebaf28e54db96",
            "f5e4712a8c9646e8bdfe29af87232cd5",
            "17d169b13aa64e60a1affa8b8f8813a1",
            "4f84d6353cdd46f48123efb032c889bb",
            "00e89577491644d88e5cc442a6188a61",
            "b181fcd791764e7a823e0678d7121e51",
            "41785660ae16498cbbe758274413a4a5",
            "f310603ab9de4d06834bf852fac70aac",
            "8dc91e7890194b708eb37b27ef086366",
            "a5e2dda2cb934a65a747656fc67cb932",
            "5cdc416b5f78452095848a934d81d480",
            "f2d0c05e89384ee89a64bb57cc8c2e18",
            "d59f806edc264c83a15b0a20e1fc8d64",
            "953186ea08e544e888a306614beef57e",
            "77d0ec0f721743f3989efd24beaa3387",
            "1bf43bfd7ad040829b88ff667da051ad",
            "29052b2eae1e475db1660ebe3e216583",
            "242c0dc30fe94aed9b83d60c5f406ba6",
            "b325ddf694284236b881cd296f82b5e1",
            "35ff95da9cb043659899a0c09caef6a1",
            "f8382e4dc1534c039bdc1f794f2c771e",
            "f50827fd80324a3eaf6a3239acd3bd51",
            "9f011410971c47fdac1bb947ede59d92",
            "53c3979c27ee4d908cb85c211227b965",
            "953d15d7a7ed4ab78ce8530ff850e6ce",
            "1a54daae914a42afa9faaa3553228809",
            "bc07c176512c4cc4ba73558d496d6dc4",
            "6c9d28764e9d4835ac21a671393ad9ec",
            "4208a81c11894825833e54d924c5e977",
            "ea8a138743134f24840a78c35bed3e7a",
            "6713958c31534656a83292c87b0f08e4",
            "64b84be1349a42d38af972a8a7e05dc5",
            "e5e62130741d48f09e727ffba57226df",
            "12e565bd71694aa0873c8bb11b28f4ab",
            "39e9d1e31e384fa2b75438a917cb808c",
            "25c721e3dcfb4055a71a92b170081b1c",
            "fa3d8c86ebf44a4fa0a0b184c9ae7c12",
            "8692e7465e144c6db89aa23b7409ace3",
            "3ec9bdb247a14a6f814e09b5a28b6540",
            "1aa3bdd9846143b5baae3ee9624ca5b3",
            "23dd1946423a4eba90e4402bb0a964a2"
          ]
        },
        "id": "EzHR-kk2CbAK",
        "outputId": "5f81df05-e04e-4b5a-b07c-31ed80164076"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b1740bbab024b718cacf9a195dcd9e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dc91e7890194b708eb37b27ef086366"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35ff95da9cb043659899a0c09caef6a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6713958c31534656a83292c87b0f08e4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = '''def add_numbers(a, b):\n",
        "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
        "    return a + b'''\n",
        "\n",
        "old_tokens = old_tokenizer.tokenize(example)\n",
        "old_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3x2XIaCrA6",
        "outputId": "e0dd900a-c648-48eb-9095-a2b2a7deedd1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['def',\n",
              " 'Ġadd',\n",
              " '_',\n",
              " 'n',\n",
              " 'umbers',\n",
              " '(',\n",
              " 'a',\n",
              " ',',\n",
              " 'Ġb',\n",
              " '):',\n",
              " 'Ċ',\n",
              " 'Ġ',\n",
              " 'Ġ',\n",
              " 'Ġ',\n",
              " 'Ġ\"\"\"',\n",
              " 'Add',\n",
              " 'Ġthe',\n",
              " 'Ġtwo',\n",
              " 'Ġnumbers',\n",
              " 'Ġ`',\n",
              " 'a',\n",
              " '`',\n",
              " 'Ġand',\n",
              " 'Ġ`',\n",
              " 'b',\n",
              " '`',\n",
              " '.\"',\n",
              " '\"\"',\n",
              " 'Ċ',\n",
              " 'Ġ',\n",
              " 'Ġ',\n",
              " 'Ġ',\n",
              " 'Ġreturn',\n",
              " 'Ġa',\n",
              " 'Ġ+',\n",
              " 'Ġb']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
      ],
      "metadata": {
        "id": "T_nedDVFAjkh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_tokens = tokenizer.tokenize(example)\n",
        "new_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnw4GDe6FdX_",
        "outputId": "29f6e8b7-d307-40d4-c80e-ef220e5a181f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['def',\n",
              " 'Ġadd',\n",
              " '_',\n",
              " 'numbers',\n",
              " '(',\n",
              " 'a',\n",
              " ',',\n",
              " 'Ġb',\n",
              " '):',\n",
              " 'ĊĠĠĠ',\n",
              " 'Ġ\"\"\"',\n",
              " 'Add',\n",
              " 'Ġthe',\n",
              " 'Ġtwo',\n",
              " 'Ġnumbers',\n",
              " 'Ġ`',\n",
              " 'a',\n",
              " '`',\n",
              " 'Ġand',\n",
              " 'Ġ`',\n",
              " 'b',\n",
              " '`.\"\"\"',\n",
              " 'ĊĠĠĠ',\n",
              " 'Ġreturn',\n",
              " 'Ġa',\n",
              " 'Ġ+',\n",
              " 'Ġb']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(old_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1j_mCM8FkH3",
        "outputId": "fbb43a82-a024-41b5-8380-f646da5070b1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(new_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25kaBuDWGgcq",
        "outputId": "a0089d90-0a96-40d4-c65d-4a99fe6e92a0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"\"\"class LinearLayer():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weight = torch.randn(input_size, output_size)\n",
        "        self.bias = torch.zeros(output_size)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x @ self.weights + self.bias\n",
        "    \"\"\"\n",
        "print(tokenizer.tokenize(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4h2noQVGiXT",
        "outputId": "8608b2ad-90ec-4483-e052-64a04cf1f54c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',', 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G03VQ3UtHCIm",
        "outputId": "4a300222-2a9d-40c5-972d-23e5440c0bc2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('code-search-net-tokenizer/tokenizer_config.json',\n",
              " 'code-search-net-tokenizer/special_tokens_map.json',\n",
              " 'code-search-net-tokenizer/vocab.json',\n",
              " 'code-search-net-tokenizer/merges.txt',\n",
              " 'code-search-net-tokenizer/added_tokens.json',\n",
              " 'code-search-net-tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "hf_token = \"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\"\n",
        "huggingface_hub.login(token=hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "eMYxm8djHxeK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"code-search-net-tokenizer\", organization=\"mitra-et-al\", use_auth_token=hf_token, private = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y49vVJg1H0KS",
        "outputId": "cb024122-a213-47e6-b1cf-f8f6f3afeacc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:652: UserWarning: The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/mitra-et-al/code-search-net-tokenizer/commit/020a3d0f5f539e8bc8d320c909ba83b1b8764af0', commit_message='Upload tokenizer', commit_description='', oid='020a3d0f5f539e8bc8d320c909ba83b1b8764af0', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"huggingface-course\" below with your actual namespace to use your own tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mitra-et-al/code-search-net-tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "9c5924e0ab2347e0adc1adb207b64c65",
            "478f9ca8526c48268fdabb1964308249",
            "e24239df5a9d4cb4b5a2957846b59e5f",
            "3c80dd3a1d4e4c45a34e4e3fab3b1c00",
            "04f7a51e5d40468fb114e06c02bd817d",
            "f64b6c884d5d41de9b165b204861ab11",
            "dda1777b75d041bd9ff70acf107df218",
            "7c4d62dd41ba4510bf6e2b18086bbce6",
            "1b4681d6d0de4ee1a8b84cd2edf7c714",
            "e999ca429472470bb412ef616502d5a9",
            "ddadad3b6b234d8abf10e68d30ac1e11",
            "395ba15e7fbe4e6289250605b6d45bb1",
            "759f0ce6e1864124b9fb9dc420adb4d0",
            "30967782aec54e70b47d6a03cd2626a0",
            "64739ff5991f4170a1c7fa646af3920d",
            "4668fa7d2e6e4b538e273bdbf1e206b0",
            "5339dc30359e4b3f89cde766f12879e4",
            "7a10b0ecf45b4e6bb40f96f04537963f",
            "fc23fee0122748288ae79b210c262a85",
            "37a6b3e3cfea49feac889c6446d7c51f",
            "ac540d6aa41c43b997be0d31406accb8",
            "7dd3b8cfc5844f6c996c11ccab5d2d78",
            "3e8a360ad74f476f9e7cab3e7131eb8b",
            "9db7d56c4b7d48888d65b52fa3915182",
            "e5ee6fa5f6704b23b7eedef4ff1daa2e",
            "6c913ec3585f4581a922647df4a5fbcf",
            "f3e38243ff07499c8412c33791fa6d95",
            "e55448193daa45ad868ea9bc3e155926",
            "7c09301b856647ea98eb2cd0e16b9ccd",
            "5b41852d5edb456b87d63124520fcaa2",
            "5a6d1adb4dd44eb48e281bb194b8cdbd",
            "2f645e70514c4a61be8fe4972a5ed9ed",
            "0b3c0c9381c34c2fbd39a35a0deda32d",
            "c28222fa9cfa4b13bd06344fc5b38972",
            "0d2472b80cd0445c846e0e70eaa3a497",
            "1396431d25cd40788adc58c3b6e22840",
            "f8e5057dc1e9463eaee1100a4edc5d46",
            "bdbc78a9cc334e0c9bd8e558da2b3151",
            "9d1cc36db4d34daa916d7d0f6184121d",
            "a804139fbd7a4b23b3e32636d17110c1",
            "b2f020f0b69243cfba8c36dc6dec42a6",
            "c62e5b18190c40899d0513f8ea05fabe",
            "1acf3667e1d042239b96a8c46ea8751b",
            "0b9466e761164eb9a38d19b23ceed9bb",
            "7f7c911fdf12463683ef6d5efa2a2198",
            "ec4d3ea73f224d7b8eb20fdb624e3e7a",
            "14e4e824690942818126998f44acb9d6",
            "7c6dbec1477f49e8bd32ca0c93498e8c",
            "c3673866b33a4b74b71ec139391bf403",
            "b222a85b807e451c9daeb73701903ebe",
            "cb406d7ea393418c9252259cf0271778",
            "92cf911c1e1a4e2eaeac420f71dd9235",
            "1f227c9879bb450593e6e81b552b588e",
            "5d40858103e84f7e904cf6cbaad561df",
            "979b236c720d445d991c046e4a9faf24"
          ]
        },
        "id": "wfHqywEZIV90",
        "outputId": "a5661b7f-7846-41ec-8f1d-3be6bf72758d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c5924e0ab2347e0adc1adb207b64c65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/822k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "395ba15e7fbe4e6289250605b6d45bb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/467k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e8a360ad74f476f9e7cab3e7131eb8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.17M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c28222fa9cfa4b13bd06344fc5b38972"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f7c911fdf12463683ef6d5efa2a2198"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}